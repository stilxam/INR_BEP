{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to get multiple INRs to train in parallel on a single GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:22.968569596Z",
     "start_time": "2025-01-06T14:55:22.925237450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-30 19:24:53.411474: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "# import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "# wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:23.553170506Z",
     "start_time": "2025-01-06T14:55:23.447043302Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\n",
    "\n",
    "To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \"CombinedINR\"` instead of `config[\"model_type\"] = \"CombinedINR\"`). You can also just use a dictionary instead.\n",
    "\n",
    "Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\n",
    "\n",
    "Doing this in a config instead of hard coded might seem like extra work, but consider this:\n",
    "1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \n",
    "   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\n",
    "2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\n",
    "   and then run an experiment specified by that config on any machine you want, e.g. on Snellius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:24.410408058Z",
     "start_time": "2025-01-06T14:55:24.393999841Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 3\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 1, #3,\n",
    "        'activation_kwargs': {'w0':12.},#{'inverse_scale': 5.},\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        'initialization_scheme_kwargs': {'w0': 12.},\n",
    "        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "config.trainer_type = 'training.train_inr_scan'\n",
    "config.loss_evaluator = 'losses.PointWiseLossEvaluator'\n",
    "config.target_function = 'images.ContinuousImage'\n",
    "config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation',\n",
    "    'data_index': None,\n",
    "}\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "config.steps = 40000 #changed from 40000\n",
    "# config.use_wandb = True\n",
    "\n",
    "# # now we want some extra things, like logging, to happen during training\n",
    "# # the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# # The callbacks we want to use can be found in inr_utils.callbacks\n",
    "# config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "# config.after_step_callback_config = {\n",
    "#     'callbacks':[\n",
    "#         ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "#         'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "#         ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "#              {'metric_collector':'metrics.MetricCollector'}\n",
    "#         ),\n",
    "#         'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "#     ],\n",
    "#     'show_logs': False\n",
    "# }\n",
    "\n",
    "# config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\n",
    "\n",
    "# config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "#     'metrics':[\n",
    "#         ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "#         # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "#         ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "#         # ^ compute the MSE with the actual image pixels\n",
    "#     ],\n",
    "#     'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "#     'epoch_frequency': 1  # not actually used\n",
    "# }\n",
    "\n",
    "# #config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "# config.optimizer_state = None  # we're starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T13:11:48.290828912Z",
     "start_time": "2024-11-27T13:11:48.207796154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      layers=(\n",
       "        CountingIdentity(\n",
       "          _embedding_matrix=f32[3],\n",
       "          state_index=StateIndex(\n",
       "            marker=<object object at 0x7fdc605aae20>,\n",
       "            init=i32[]\n",
       "          )\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,2],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "      )\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it works properly\n",
    "try:\n",
    "    inr(jnp.zeros(2))\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostponedInitialization(cls=train_inr_scan, kwargs={'steps': 40000, 'loss_evaluator': PostponedInitialization(cls=PointWiseLossEvaluator, kwargs={'target_function': PostponedInitialization(cls=ContinuousImage, kwargs={'image': './example_data/parrot.png', 'scale_to_01': True, 'interpolation_method': <function make_piece_wise_constant_interpolation at 0x7fdc28121bd0>, 'data_index': None}, missing_args=[]), 'loss_function': <function scaled_mse_loss at 0x7fdbf0d34ca0>, 'state_update_function': <function counter_updater at 0x7fdbd8765ab0>}, missing_args=[]), 'sampler': PostponedInitialization(cls=GridSubsetSampler, kwargs={'size': [2040, 1356], 'batch_size': 2000, 'allow_duplicates': False, 'min': 0.0, 'max': 1.0, 'num_dimensions': None, 'indexing': 'ij'}, missing_args=[]), 'optimizer': PostponedInitialization(cls=adam, kwargs={'learning_rate': 0.00015, 'b1': 0.9, 'b2': 0.999, 'eps': 1e-08, 'eps_root': 0.0, 'mu_dtype': None, 'nesterov': False}, missing_args=[]), 'state_initialization_function': <function initialize_state at 0x7fdbf0d6e170>, 'inr': PostponedInitialization(cls=CombinedINR, kwargs={'terms': [PostponedInitialization(cls=from_config, kwargs={'hidden_size': 256, 'num_layers': 5, 'layer_type': <class 'model_components.inr_layers.SirenLayer'>, 'activation_kwargs': {'w0': 12.0}, 'initialization_scheme': <function siren_scheme at 0x7fdc4ffb2950>, 'initialization_scheme_kwargs': {'w0': 12.0}, 'positional_encoding_layer': PostponedInitialization(cls=CountingIdentity, kwargs={}, missing_args=[]), 'num_splits': 1, 'post_processor': None, 'in_size': 2, 'out_size': 3, 'key': Array([4177750840, 1613599438], dtype=uint32)}, missing_args=[])], 'post_processor': <function real_part at 0x7fdc54efe560>}, missing_args=[]), 'key': Array([ 793826064, 3381256178], dtype=uint32)}, missing_args=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0130 19:25:01.106727 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.106762 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.106769 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.106776 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.106780 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.106784 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.106792 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.106800 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.106807 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.106816 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.106819: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.107547 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.107561 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.107567 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.107574 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.107578 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.107582 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.107592 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.107600 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.107608 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.107616 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.107620: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.108364 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0130 19:25:01.108381 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490271, expected -0.328079\n",
      "E0130 19:25:01.108387 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861816, expected -0.251582\n",
      "E0130 19:25:01.108394 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21686, expected 0.980453\n",
      "E0130 19:25:01.108399 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598793, expected 0.847711\n",
      "E0130 19:25:01.108403 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00623322, expected -0.148094\n",
      "E0130 19:25:01.108410 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391197, expected 0.24692\n",
      "E0130 19:25:01.108419 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0130 19:25:01.108426 2669192 buffer_comparator.cc:157] Difference at 20861: 0.0473442, expected -0.0853729\n",
      "E0130 19:25:01.108435 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671219, expected 0.481705\n",
      "2025-01-30 19:25:01.108439: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.109276 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0130 19:25:01.109294 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490253, expected -0.328079\n",
      "E0130 19:25:01.109299 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861359, expected -0.251582\n",
      "E0130 19:25:01.109307 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21662, expected 0.980453\n",
      "E0130 19:25:01.109311 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598774, expected 0.847711\n",
      "E0130 19:25:01.109315 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00616455, expected -0.148094\n",
      "E0130 19:25:01.109323 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391201, expected 0.24692\n",
      "E0130 19:25:01.109332 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0633163, expected 0.211475\n",
      "E0130 19:25:01.109340 2669192 buffer_comparator.cc:157] Difference at 20861: 0.0471668, expected -0.0853729\n",
      "E0130 19:25:01.109348 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671066, expected 0.481705\n",
      "2025-01-30 19:25:01.109351: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.110365 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.110384 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.110390 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.110399 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.110405 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.110409 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.110416 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.110425 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.110432 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.110442 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.110445: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.111076 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0130 19:25:01.111093 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490271, expected -0.328079\n",
      "E0130 19:25:01.111099 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861816, expected -0.251582\n",
      "E0130 19:25:01.111107 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21686, expected 0.980453\n",
      "E0130 19:25:01.111113 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598793, expected 0.847711\n",
      "E0130 19:25:01.111117 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00623322, expected -0.148094\n",
      "E0130 19:25:01.111124 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391197, expected 0.24692\n",
      "E0130 19:25:01.111133 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0130 19:25:01.111140 2669192 buffer_comparator.cc:157] Difference at 20861: 0.0473442, expected -0.0853729\n",
      "E0130 19:25:01.111149 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671219, expected 0.481705\n",
      "2025-01-30 19:25:01.111152: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.111853 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.111869 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.111875 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.111882 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.111886 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.111890 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.111898 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.111907 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.111915 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.111924 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.111927: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.112663 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.112681 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.112687 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.112702 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.112707 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.112710 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.112718 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.112727 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.112734 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.112743 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.112746: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.113660 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.113677 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.113683 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.113690 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.113699 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.113703 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.113711 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.113719 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.113727 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.113735 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.113739: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.114843 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.114860 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.114866 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.114874 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.114878 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.114882 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.114890 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.114898 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.114905 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.114915 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.114918: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.115744 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.115764 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.115770 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.115779 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.115784 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.115789 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.115796 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.115805 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.115813 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.115823 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.115826: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.116775 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.116792 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.116798 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.116806 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.116812 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.116816 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.116823 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.116832 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.116839 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.116848 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.116851: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.117749 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854904, expected 0.651917\n",
      "E0130 19:25:01.117766 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490173, expected -0.328079\n",
      "E0130 19:25:01.117772 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861664, expected -0.251582\n",
      "E0130 19:25:01.117780 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21642, expected 0.980453\n",
      "E0130 19:25:01.117786 2669192 buffer_comparator.cc:157] Difference at 11249: 0.59843, expected 0.847711\n",
      "E0130 19:25:01.117790 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00585938, expected -0.148094\n",
      "E0130 19:25:01.117798 2669192 buffer_comparator.cc:157] Difference at 15248: 0.39116, expected 0.24692\n",
      "E0130 19:25:01.117806 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0130 19:25:01.117814 2669192 buffer_comparator.cc:157] Difference at 20861: 0.0469704, expected -0.0853729\n",
      "E0130 19:25:01.117824 2669192 buffer_comparator.cc:157] Difference at 24193: 0.670959, expected 0.481705\n",
      "2025-01-30 19:25:01.117827: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0130 19:25:01.119287 2669192 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0130 19:25:01.119306 2669192 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0130 19:25:01.119312 2669192 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0130 19:25:01.119321 2669192 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0130 19:25:01.119326 2669192 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0130 19:25:01.119331 2669192 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0130 19:25:01.119338 2669192 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0130 19:25:01.119347 2669192 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0130 19:25:01.119354 2669192 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0130 19:25:01.119363 2669192 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-30 19:25:01.119366: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=<function real_part>\n",
      "), (ScaleByAdamState(count=Array(40000, dtype=int32), mu=CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': None}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': None}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': None}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=None\n",
      "), nu=CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': None}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': None}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': None}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=None\n",
      ")), EmptyState()), State(0x7fdc862e80d0=i32[]), Array([ 7.288151  , 13.96123   ,  3.5011785 , ...,  0.10888974,\n",
      "        0.09800542,  0.08990859], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# run it\n",
    "try:\n",
    "    results = experiment.initialize()\n",
    "    print(results)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print()\n",
    "    traceback.print_exc()\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence, Mapping\n",
    "from common_dl_utils.config_realization import PostponedInitialization\n",
    "def complete_postponed_initialization(postponed_init:PostponedInitialization, completion: dict):\n",
    "    postponed_init.resolve_missing_args(completion)\n",
    "    for value in postponed_init.kwargs.values():\n",
    "        if isinstance(value, PostponedInitialization):\n",
    "            complete_postponed_initialization(value, completion)\n",
    "        elif isinstance(value, Sequence):\n",
    "            for v in value:\n",
    "                if isinstance(v, PostponedInitialization):\n",
    "                    complete_postponed_initialization(v, completion)\n",
    "        elif isinstance(value, Mapping):\n",
    "            for v in value.values():\n",
    "                if isinstance(v, PostponedInitialization):\n",
    "                    complete_postponed_initialization(v, completion)\n",
    "\n",
    "\n",
    "def run_experiment(missing_kwargs: dict, config:dict, key:jax.Array):\n",
    "    experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "        prng_key=key,\n",
    "        config=config,\n",
    "        model_kwarg_in_trainer='inr',\n",
    "        model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "        trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "        additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "        add_model_module_to_architecture_default_module=False,\n",
    "        initialize=False  # don't run the experiment yet, we want to add the missing kwargs\n",
    "    )\n",
    "    complete_postponed_initialization(experiment, missing_kwargs)\n",
    "    return experiment.initialize()\n",
    "    #return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "incomplete_config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "incomplete_config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "incomplete_config.model_config = Config()\n",
    "incomplete_config.model_config.in_size = 2\n",
    "incomplete_config.model_config.out_size = 3\n",
    "incomplete_config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 3,\n",
    "        #'activation_kwargs': {'w0':12.}, #                        <-------------------------------------------------------------- this is the missin one\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        #'initialization_scheme_kwargs': {'w0': 12.},\n",
    "        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "incomplete_config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "incomplete_config.trainer_type = 'training.train_inr_scan'\n",
    "incomplete_config.loss_evaluator = 'losses.PointWiseLossEvaluator'\n",
    "incomplete_config.target_function = 'images.ContinuousImage'\n",
    "incomplete_config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation',\n",
    "    'data_index': None\n",
    "}\n",
    "incomplete_config.loss_function = 'losses.scaled_mse_loss'\n",
    "incomplete_config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "incomplete_config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "incomplete_config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "incomplete_config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "incomplete_config.steps = 40000 #changed from 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CombinedINR(\n",
       "   terms=(\n",
       "     MLPINR(\n",
       "       layers=(\n",
       "         CountingIdentity(\n",
       "           _embedding_matrix=f32[3],\n",
       "           state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[256,2],\n",
       "           biases=f32[256],\n",
       "           activation_kwargs={'w0': 12.0}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[256,256],\n",
       "           biases=f32[256],\n",
       "           activation_kwargs={'w0': 12.0}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[256,256],\n",
       "           biases=f32[256],\n",
       "           activation_kwargs={'w0': 12.0}\n",
       "         ),\n",
       "         Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "       )\n",
       "     ),\n",
       "   ),\n",
       "   post_processor=<function real_part>\n",
       " ),\n",
       " (ScaleByAdamState(count=Array(40000, dtype=int32), mu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,2],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': None}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': None}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': None}\n",
       "          ),\n",
       "          Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  ), nu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,2],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': None}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': None}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': None}\n",
       "          ),\n",
       "          Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  )),\n",
       "  EmptyState()),\n",
       " State(0x7fdc862e80d0=i32[]),\n",
       " Array([11.163851  ,  9.481189  ,  6.821996  , ...,  0.09610099,\n",
       "         0.09478693,  0.08648074], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(\n",
    "    missing_kwargs={\"activation_kwargs\": {\"w0\": 12.}},\n",
    "    config=incomplete_config,\n",
    "    key=next(key_gen)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equinox as eqx\n",
    "num_parallel = 10\n",
    "\n",
    "def v_mappable_runner(w0, key):\n",
    "    return_value = run_experiment(\n",
    "        missing_kwargs={\"activation_kwargs\": {\"w0\": w0}},\n",
    "        config=incomplete_config,\n",
    "        key=key\n",
    "    )\n",
    "    #return eqx.filter(return_value, eqx.is_array_like)\n",
    "    return return_value\n",
    "\n",
    "keys = jax.random.split(next(key_gen), num_parallel)\n",
    "w0s = jnp.linspace(10., 30., num=num_parallel)\n",
    "\n",
    "#results = jax.vmap(v_mappable_runner)(w0s, keys)  # no idea why this results in a user warning while the single one doesn't... but it seems to work\n",
    "results = eqx.filter_vmap(v_mappable_runner)(w0s, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CombinedINR(\n",
       "   terms=(\n",
       "     MLPINR(\n",
       "       layers=(\n",
       "         CountingIdentity(\n",
       "           _embedding_matrix=f32[10,3],\n",
       "           state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[10,256,2],\n",
       "           biases=f32[10,256],\n",
       "           activation_kwargs={'w0': f32[10]}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[10,256,256],\n",
       "           biases=f32[10,256],\n",
       "           activation_kwargs={'w0': f32[10]}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[10,256,256],\n",
       "           biases=f32[10,256],\n",
       "           activation_kwargs={'w0': f32[10]}\n",
       "         ),\n",
       "         Linear(weights=f32[10,3,256], biases=f32[10,3], activation_kwargs={})\n",
       "       )\n",
       "     ),\n",
       "   ),\n",
       "   post_processor=<function real_part>\n",
       " ),\n",
       " (ScaleByAdamState(count=Array([40000, 40000, 40000, 40000, 40000, 40000, 40000, 40000, 40000,\n",
       "         40000], dtype=int32), mu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[10,3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,2],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[10]}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,256],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[10]}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,256],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[10]}\n",
       "          ),\n",
       "          Linear(weights=f32[10,3,256], biases=f32[10,3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  ), nu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[10,3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,2],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[10]}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,256],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[10]}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,256],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[10]}\n",
       "          ),\n",
       "          Linear(weights=f32[10,3,256], biases=f32[10,3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  )),\n",
       "  EmptyState()),\n",
       " State(0x7fdc862e80d0=i32[10]),\n",
       " Array([[25.23208   ,  2.5933719 ,  6.378536  , ...,  0.10845421,\n",
       "          0.10386176,  0.07614031],\n",
       "        [ 2.0645764 , 25.206396  ,  2.5043325 , ...,  0.0929113 ,\n",
       "          0.09098022,  0.08648579],\n",
       "        [22.242195  ,  5.4414573 ,  3.4137573 , ...,  0.09876344,\n",
       "          0.08590956,  0.09317067],\n",
       "        ...,\n",
       "        [12.791691  , 28.487856  , 10.455021  , ...,  0.0916782 ,\n",
       "          0.08028591,  0.08070946],\n",
       "        [ 3.9212358 , 36.44636   , 16.845152  , ...,  0.08156741,\n",
       "          0.08129244,  0.07670492],\n",
       "        [13.683941  , 30.009329  ,  8.69969   , ...,  0.08125313,\n",
       "          0.08749905,  0.08224121]], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqx.filter_vmap(run_experiment, in_axes=(0, None, 0))({\"activation_kwargs\": {\"w0\": w0s}}, incomplete_config, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 40000 in final state after training.\n"
     ]
    }
   ],
   "source": [
    "from state_test_objects import after_training_callback, CountingIdentity\n",
    "from inr_utils.parallel_training import tree_unstack\n",
    "\n",
    "inr, optimizer_state, state, losses = results\n",
    "\n",
    "for _inr, _optimizer_state, _state, _losses in tree_unstack(results):\n",
    "    after_training_callback(_losses, _inr, _state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_unstack(tree, axis=0):\n",
    "    leaves, tree_def = jax.tree.flatten(tree)\n",
    "    array_leaf = next(filter(eqx.is_array, leaves))\n",
    "    num_out = array_leaf.shape[axis]\n",
    "    def _safe_unstack(maybe_array):\n",
    "        if eqx.is_array(maybe_array):\n",
    "            return jnp.unstack(maybe_array, axis=axis)\n",
    "        else:\n",
    "            return num_out*[maybe_array]\n",
    "    unstacked_leaves = [_safe_unstack(leaf) for leaf in leaves]\n",
    "    del leaves\n",
    "    return [tree_def.unflatten(leaves) for leaves in zip(*unstacked_leaves)]\n",
    "\n",
    "try:\n",
    "    tree_unstack(inr)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    traceback.print_exc()\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
