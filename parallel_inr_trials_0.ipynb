{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to get multiple INRs to train in parallel on a single GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:22.968569596Z",
     "start_time": "2025-01-06T14:55:22.925237450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 02:23:32.307523: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "# import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "# wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:23.553170506Z",
     "start_time": "2025-01-06T14:55:23.447043302Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\n",
    "\n",
    "To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \"CombinedINR\"` instead of `config[\"model_type\"] = \"CombinedINR\"`). You can also just use a dictionary instead.\n",
    "\n",
    "Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\n",
    "\n",
    "Doing this in a config instead of hard coded might seem like extra work, but consider this:\n",
    "1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \n",
    "   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\n",
    "2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\n",
    "   and then run an experiment specified by that config on any machine you want, e.g. on Snellius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:24.410408058Z",
     "start_time": "2025-01-06T14:55:24.393999841Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 3\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 3,\n",
    "        'activation_kwargs': {'w0':12.},#{'inverse_scale': 5.},\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        'initialization_scheme_kwargs': {'w0': 12.},\n",
    "        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "config.trainer_type = 'training.train_inr_scan'\n",
    "config.loss_evaluator = 'losses.PointWiseLossEvaluator'\n",
    "config.target_function = 'images.ContinuousImage'\n",
    "config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation'\n",
    "}\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "config.steps = 40000 #changed from 40000\n",
    "# config.use_wandb = True\n",
    "\n",
    "# # now we want some extra things, like logging, to happen during training\n",
    "# # the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# # The callbacks we want to use can be found in inr_utils.callbacks\n",
    "# config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "# config.after_step_callback_config = {\n",
    "#     'callbacks':[\n",
    "#         ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "#         'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "#         ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "#              {'metric_collector':'metrics.MetricCollector'}\n",
    "#         ),\n",
    "#         'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "#     ],\n",
    "#     'show_logs': False\n",
    "# }\n",
    "\n",
    "# config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\n",
    "\n",
    "# config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "#     'metrics':[\n",
    "#         ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "#         # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "#         ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "#         # ^ compute the MSE with the actual image pixels\n",
    "#     ],\n",
    "#     'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "#     'epoch_frequency': 1  # not actually used\n",
    "# }\n",
    "\n",
    "# #config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "# config.optimizer_state = None  # we're starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T13:11:48.290828912Z",
     "start_time": "2024-11-27T13:11:48.207796154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      layers=(\n",
       "        CountingIdentity(\n",
       "          _embedding_matrix=f32[3],\n",
       "          state_index=StateIndex(\n",
       "            marker=<object object at 0x7f7bb8aa2da0>,\n",
       "            init=i32[]\n",
       "          )\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,2],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "      )\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it works properly\n",
    "try:\n",
    "    inr(jnp.zeros(2))\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostponedInitialization(cls=train_inr_scan, kwargs={'steps': 40000, 'loss_evaluator': PostponedInitialization(cls=PointWiseLossEvaluator, kwargs={'target_function': PostponedInitialization(cls=ContinuousImage, kwargs={'image': './example_data/parrot.png', 'scale_to_01': True, 'interpolation_method': <function make_piece_wise_constant_interpolation at 0x7f7b7ffe3e20>}, missing_args=[]), 'loss_function': <function scaled_mse_loss at 0x7f7b7fffe680>, 'state_update_function': <function counter_updater at 0x7f7b7f525990>}, missing_args=[]), 'sampler': PostponedInitialization(cls=GridSubsetSampler, kwargs={'size': [2040, 1356], 'batch_size': 2000, 'allow_duplicates': False, 'min': 0.0, 'max': 1.0, 'num_dimensions': None, 'indexing': 'ij'}, missing_args=[]), 'optimizer': PostponedInitialization(cls=adam, kwargs={'learning_rate': 0.00015, 'b1': 0.9, 'b2': 0.999, 'eps': 1e-08, 'eps_root': 0.0, 'mu_dtype': None, 'nesterov': False}, missing_args=[]), 'state_initialization_function': <function initialize_state at 0x7f7b7f5241f0>, 'inr': PostponedInitialization(cls=CombinedINR, kwargs={'terms': [PostponedInitialization(cls=from_config, kwargs={'hidden_size': 256, 'num_layers': 5, 'layer_type': <class 'model_components.inr_layers.SirenLayer'>, 'activation_kwargs': {'w0': 12.0}, 'initialization_scheme': <function siren_scheme at 0x7f7b9ccacf70>, 'initialization_scheme_kwargs': {'w0': 12.0}, 'positional_encoding_layer': PostponedInitialization(cls=CountingIdentity, kwargs={}, missing_args=[]), 'num_splits': 3, 'post_processor': None, 'in_size': 2, 'out_size': 3, 'key': Array([4177750840, 1613599438], dtype=uint32)}, missing_args=[])], 'post_processor': <function real_part at 0x7f7b9cc08ca0>}, missing_args=[]), 'key': Array([ 793826064, 3381256178], dtype=uint32)}, missing_args=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0126 02:23:40.019612 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.019644 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.019650 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.019657 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.019661 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.019665 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.019673 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.019681 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.019688 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.019701 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.019705: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.020433 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.020446 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.020452 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.020458 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.020463 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.020467 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.020475 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.020483 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.020491 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.020499 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.020502: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.021239 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0126 02:23:40.021255 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490271, expected -0.328079\n",
      "E0126 02:23:40.021261 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861816, expected -0.251582\n",
      "E0126 02:23:40.021268 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21686, expected 0.980453\n",
      "E0126 02:23:40.021272 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598793, expected 0.847711\n",
      "E0126 02:23:40.021276 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00623322, expected -0.148094\n",
      "E0126 02:23:40.021284 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391197, expected 0.24692\n",
      "E0126 02:23:40.021292 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0126 02:23:40.021299 2125336 buffer_comparator.cc:157] Difference at 20861: 0.0473442, expected -0.0853729\n",
      "E0126 02:23:40.021307 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671219, expected 0.481705\n",
      "2025-01-26 02:23:40.021310: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.022020 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0126 02:23:40.022037 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490253, expected -0.328079\n",
      "E0126 02:23:40.022042 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861359, expected -0.251582\n",
      "E0126 02:23:40.022049 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21662, expected 0.980453\n",
      "E0126 02:23:40.022054 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598774, expected 0.847711\n",
      "E0126 02:23:40.022058 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00616455, expected -0.148094\n",
      "E0126 02:23:40.022067 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391201, expected 0.24692\n",
      "E0126 02:23:40.022076 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0633163, expected 0.211475\n",
      "E0126 02:23:40.022083 2125336 buffer_comparator.cc:157] Difference at 20861: 0.0471668, expected -0.0853729\n",
      "E0126 02:23:40.022092 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671066, expected 0.481705\n",
      "2025-01-26 02:23:40.022094: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.023101 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.023118 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.023124 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.023132 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.023136 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.023140 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.023148 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.023157 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.023164 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.023173 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.023179: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.023833 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0126 02:23:40.023849 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490271, expected -0.328079\n",
      "E0126 02:23:40.023855 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861816, expected -0.251582\n",
      "E0126 02:23:40.023863 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21686, expected 0.980453\n",
      "E0126 02:23:40.023869 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598793, expected 0.847711\n",
      "E0126 02:23:40.023873 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00623322, expected -0.148094\n",
      "E0126 02:23:40.023881 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391197, expected 0.24692\n",
      "E0126 02:23:40.023888 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0126 02:23:40.023896 2125336 buffer_comparator.cc:157] Difference at 20861: 0.0473442, expected -0.0853729\n",
      "E0126 02:23:40.023904 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671219, expected 0.481705\n",
      "2025-01-26 02:23:40.023907: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.024587 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.024604 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.024610 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.024617 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.024622 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.024626 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.024633 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.024642 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.024649 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.024659 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.024662: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.025431 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.025449 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.025455 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.025462 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.025468 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.025472 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.025480 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.025488 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.025496 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.025506 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.025509: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.026400 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.026417 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.026422 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.026431 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.026436 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.026440 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.026448 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.026456 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.026464 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.026474 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.026477: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.027577 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.027593 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.027599 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.027606 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.027612 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.027616 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.027623 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.027632 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.027639 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.027648 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.027651: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.028464 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.028482 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.028487 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.028496 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.028501 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.028505 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.028513 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.028522 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.028529 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.028539 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.028542: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.029513 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.029529 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.029535 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.029542 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.029546 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.029550 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.029558 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.029566 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.029573 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.029582 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.029585: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.030514 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854904, expected 0.651917\n",
      "E0126 02:23:40.030530 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490173, expected -0.328079\n",
      "E0126 02:23:40.030536 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861664, expected -0.251582\n",
      "E0126 02:23:40.030545 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21642, expected 0.980453\n",
      "E0126 02:23:40.030549 2125336 buffer_comparator.cc:157] Difference at 11249: 0.59843, expected 0.847711\n",
      "E0126 02:23:40.030553 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00585938, expected -0.148094\n",
      "E0126 02:23:40.030560 2125336 buffer_comparator.cc:157] Difference at 15248: 0.39116, expected 0.24692\n",
      "E0126 02:23:40.030569 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0126 02:23:40.030576 2125336 buffer_comparator.cc:157] Difference at 20861: 0.0469704, expected -0.0853729\n",
      "E0126 02:23:40.030584 2125336 buffer_comparator.cc:157] Difference at 24193: 0.670959, expected 0.481705\n",
      "2025-01-26 02:23:40.030588: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 02:23:40.032036 2125336 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 02:23:40.032054 2125336 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 02:23:40.032060 2125336 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 02:23:40.032067 2125336 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 02:23:40.032071 2125336 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 02:23:40.032075 2125336 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 02:23:40.032083 2125336 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 02:23:40.032092 2125336 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 02:23:40.032101 2125336 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 02:23:40.032111 2125336 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 02:23:40.032114: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=<function real_part>\n",
      "), (ScaleByAdamState(count=Array(40000, dtype=int32), mu=CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=None\n",
      "), nu=CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=None\n",
      ")), EmptyState()), State(0x7f7be4e640d0=i32[]), Array([ 7.288151  , 13.96123   ,  3.5011785 , ...,  0.10888974,\n",
      "        0.09800542,  0.08990859], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# run it\n",
    "try:\n",
    "    results = experiment.initialize()\n",
    "    print(results)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print()\n",
    "    traceback.print_exc()\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence, Mapping\n",
    "from common_dl_utils.config_realization import PostponedInitialization\n",
    "def complete_postponed_initialization(postponed_init:PostponedInitialization, completion: dict):\n",
    "    postponed_init.resolve_missing_args(completion)\n",
    "    for value in postponed_init.kwargs.values():\n",
    "        if isinstance(value, PostponedInitialization):\n",
    "            complete_postponed_initialization(value, completion)\n",
    "        elif isinstance(value, Sequence):\n",
    "            for v in value:\n",
    "                if isinstance(v, PostponedInitialization):\n",
    "                    complete_postponed_initialization(v, completion)\n",
    "        elif isinstance(value, Mapping):\n",
    "            for v in value.values():\n",
    "                if isinstance(v, PostponedInitialization):\n",
    "                    complete_postponed_initialization(v, completion)\n",
    "\n",
    "\n",
    "def run_experiment(missing_kwargs: dict, config:dict, key:jax.Array):\n",
    "    experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "        prng_key=key,\n",
    "        config=config,\n",
    "        model_kwarg_in_trainer='inr',\n",
    "        model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "        trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "        additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "        add_model_module_to_architecture_default_module=False,\n",
    "        initialize=False  # don't run the experiment yet, we want to add the missing kwargs\n",
    "    )\n",
    "    complete_postponed_initialization(experiment, missing_kwargs)\n",
    "    return experiment.initialize()\n",
    "    #return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "incomplete_config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "incomplete_config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "incomplete_config.model_config = Config()\n",
    "incomplete_config.model_config.in_size = 2\n",
    "incomplete_config.model_config.out_size = 3\n",
    "incomplete_config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 3,\n",
    "        #'activation_kwargs': {'w0':12.}, #                        <-------------------------------------------------------------- this is the missin one\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        #'initialization_scheme_kwargs': {'w0': 12.},\n",
    "        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "incomplete_config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "incomplete_config.trainer_type = 'training.train_inr_scan'\n",
    "incomplete_config.loss_evaluator = 'losses.PointWiseLossEvaluator'\n",
    "incomplete_config.target_function = 'images.ContinuousImage'\n",
    "incomplete_config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation'\n",
    "}\n",
    "incomplete_config.loss_function = 'losses.scaled_mse_loss'\n",
    "incomplete_config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "incomplete_config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "incomplete_config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "incomplete_config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "incomplete_config.steps = 40000 #changed from 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CombinedINR(\n",
       "   terms=(\n",
       "     MLPINR(\n",
       "       layers=(\n",
       "         CountingIdentity(\n",
       "           _embedding_matrix=f32[3],\n",
       "           state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[256,2],\n",
       "           biases=f32[256],\n",
       "           activation_kwargs={'w0': 12.0}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[256,256],\n",
       "           biases=f32[256],\n",
       "           activation_kwargs={'w0': 12.0}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[256,256],\n",
       "           biases=f32[256],\n",
       "           activation_kwargs={'w0': 12.0}\n",
       "         ),\n",
       "         Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "       )\n",
       "     ),\n",
       "   ),\n",
       "   post_processor=<function real_part>\n",
       " ),\n",
       " (ScaleByAdamState(count=Array(40000, dtype=int32), mu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,2],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  ), nu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,2],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  )),\n",
       "  EmptyState()),\n",
       " State(0x7f7be4e640d0=i32[]),\n",
       " Array([11.163851  ,  9.481189  ,  6.821996  , ...,  0.09610099,\n",
       "         0.09478693,  0.08648074], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(\n",
    "    missing_kwargs={\"activation_kwargs\": {\"w0\": 12.}},\n",
    "    config=incomplete_config,\n",
    "    key=next(key_gen)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Documents/INR_BEP/model_components/initialization_schemes.py:66: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n",
      "  return cls(weight, bias, **activation_kwargs)\n"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "num_parallel = 10\n",
    "\n",
    "def v_mappable_runner(w0, key):\n",
    "    return_value = run_experiment(\n",
    "        missing_kwargs={\"activation_kwargs\": {\"w0\": w0}},\n",
    "        config=incomplete_config,\n",
    "        key=key\n",
    "    )\n",
    "    return eqx.filter(return_value, eqx.is_array_like)\n",
    "\n",
    "keys = jax.random.split(next(key_gen), num_parallel)\n",
    "w0s = jnp.linspace(10., 30., num=num_parallel)\n",
    "\n",
    "results = jax.vmap(v_mappable_runner)(w0s, keys)  # no idea why this results in a user warning while the single one doesn't... but it seems to work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
