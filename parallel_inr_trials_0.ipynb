{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to get multiple INRs to train in parallel on a single GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:22.968569596Z",
     "start_time": "2025-01-06T14:55:22.925237450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-27 18:46:55.897438: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "# import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "# wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:23.553170506Z",
     "start_time": "2025-01-06T14:55:23.447043302Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\n",
    "\n",
    "To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \"CombinedINR\"` instead of `config[\"model_type\"] = \"CombinedINR\"`). You can also just use a dictionary instead.\n",
    "\n",
    "Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\n",
    "\n",
    "Doing this in a config instead of hard coded might seem like extra work, but consider this:\n",
    "1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \n",
    "   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\n",
    "2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\n",
    "   and then run an experiment specified by that config on any machine you want, e.g. on Snellius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:24.410408058Z",
     "start_time": "2025-01-06T14:55:24.393999841Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 3\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 1, #3,\n",
    "        'activation_kwargs': {'w0':12.},#{'inverse_scale': 5.},\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        'initialization_scheme_kwargs': {'w0': 12.},\n",
    "        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "config.trainer_type = 'training.train_inr_scan'\n",
    "config.loss_evaluator = 'losses.PointWiseLossEvaluator'\n",
    "config.target_function = 'images.ContinuousImage'\n",
    "config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation'\n",
    "}\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "config.steps = 40000 #changed from 40000\n",
    "# config.use_wandb = True\n",
    "\n",
    "# # now we want some extra things, like logging, to happen during training\n",
    "# # the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# # The callbacks we want to use can be found in inr_utils.callbacks\n",
    "# config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "# config.after_step_callback_config = {\n",
    "#     'callbacks':[\n",
    "#         ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "#         'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "#         ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "#              {'metric_collector':'metrics.MetricCollector'}\n",
    "#         ),\n",
    "#         'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "#     ],\n",
    "#     'show_logs': False\n",
    "# }\n",
    "\n",
    "# config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\n",
    "\n",
    "# config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "#     'metrics':[\n",
    "#         ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "#         # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "#         ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "#         # ^ compute the MSE with the actual image pixels\n",
    "#     ],\n",
    "#     'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "#     'epoch_frequency': 1  # not actually used\n",
    "# }\n",
    "\n",
    "# #config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "# config.optimizer_state = None  # we're starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T13:11:48.290828912Z",
     "start_time": "2024-11-27T13:11:48.207796154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      layers=(\n",
       "        CountingIdentity(\n",
       "          _embedding_matrix=f32[3],\n",
       "          state_index=StateIndex(\n",
       "            marker=<object object at 0x7daeb859ade0>,\n",
       "            init=i32[]\n",
       "          )\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,2],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "      )\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it works properly\n",
    "try:\n",
    "    inr(jnp.zeros(2))\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostponedInitialization(cls=train_inr_scan, kwargs={'steps': 40000, 'loss_evaluator': PostponedInitialization(cls=PointWiseLossEvaluator, kwargs={'target_function': PostponedInitialization(cls=ContinuousImage, kwargs={'image': './example_data/parrot.png', 'scale_to_01': True, 'interpolation_method': <function make_piece_wise_constant_interpolation at 0x7dae7e9dfd00>}, missing_args=[]), 'loss_function': <function scaled_mse_loss at 0x7dae7e9fea70>, 'state_update_function': <function counter_updater at 0x7dae7df1e170>}, missing_args=[]), 'sampler': PostponedInitialization(cls=GridSubsetSampler, kwargs={'size': [2040, 1356], 'batch_size': 2000, 'allow_duplicates': False, 'min': 0.0, 'max': 1.0, 'num_dimensions': None, 'indexing': 'ij'}, missing_args=[]), 'optimizer': PostponedInitialization(cls=adam, kwargs={'learning_rate': 0.00015, 'b1': 0.9, 'b2': 0.999, 'eps': 1e-08, 'eps_root': 0.0, 'mu_dtype': None, 'nesterov': False}, missing_args=[]), 'state_initialization_function': <function initialize_state at 0x7dae7df1c5e0>, 'inr': PostponedInitialization(cls=CombinedINR, kwargs={'terms': [PostponedInitialization(cls=from_config, kwargs={'hidden_size': 256, 'num_layers': 5, 'layer_type': <class 'model_components.inr_layers.SirenLayer'>, 'activation_kwargs': {'w0': 12.0}, 'initialization_scheme': <function siren_scheme at 0x7daea7fa8f70>, 'initialization_scheme_kwargs': {'w0': 12.0}, 'positional_encoding_layer': PostponedInitialization(cls=CountingIdentity, kwargs={}, missing_args=[]), 'num_splits': 1, 'post_processor': None, 'in_size': 2, 'out_size': 3, 'key': Array([4177750840, 1613599438], dtype=uint32)}, missing_args=[])], 'post_processor': <function real_part at 0x7daea7f08ca0>}, missing_args=[]), 'key': Array([ 793826064, 3381256178], dtype=uint32)}, missing_args=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0127 18:47:03.555487 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.555518 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.555524 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.555531 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.555535 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.555539 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.555546 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.555554 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.555561 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.555569 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.555574: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.556290 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.556303 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.556309 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.556316 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.556320 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.556324 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.556332 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.556340 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.556348 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.556357 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.556360: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.557129 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0127 18:47:03.557181 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490271, expected -0.328079\n",
      "E0127 18:47:03.557189 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861816, expected -0.251582\n",
      "E0127 18:47:03.557196 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21686, expected 0.980453\n",
      "E0127 18:47:03.557200 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598793, expected 0.847711\n",
      "E0127 18:47:03.557204 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00623322, expected -0.148094\n",
      "E0127 18:47:03.557212 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391197, expected 0.24692\n",
      "E0127 18:47:03.557220 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0127 18:47:03.557230 3794949 buffer_comparator.cc:157] Difference at 20861: 0.0473442, expected -0.0853729\n",
      "E0127 18:47:03.557238 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671219, expected 0.481705\n",
      "2025-01-27 18:47:03.557243: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.558192 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0127 18:47:03.558212 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490253, expected -0.328079\n",
      "E0127 18:47:03.558217 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861359, expected -0.251582\n",
      "E0127 18:47:03.558224 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21662, expected 0.980453\n",
      "E0127 18:47:03.558229 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598774, expected 0.847711\n",
      "E0127 18:47:03.558233 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00616455, expected -0.148094\n",
      "E0127 18:47:03.558241 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391201, expected 0.24692\n",
      "E0127 18:47:03.558249 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0633163, expected 0.211475\n",
      "E0127 18:47:03.558256 3794949 buffer_comparator.cc:157] Difference at 20861: 0.0471668, expected -0.0853729\n",
      "E0127 18:47:03.558265 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671066, expected 0.481705\n",
      "2025-01-27 18:47:03.558269: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.559278 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.559294 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.559300 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.559307 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.559312 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.559316 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.559324 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.559333 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.559340 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.559349 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.559352: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.559963 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0127 18:47:03.559982 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490271, expected -0.328079\n",
      "E0127 18:47:03.559987 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861816, expected -0.251582\n",
      "E0127 18:47:03.559994 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21686, expected 0.980453\n",
      "E0127 18:47:03.559999 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598793, expected 0.847711\n",
      "E0127 18:47:03.560002 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00623322, expected -0.148094\n",
      "E0127 18:47:03.560010 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391197, expected 0.24692\n",
      "E0127 18:47:03.560020 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0127 18:47:03.560027 3794949 buffer_comparator.cc:157] Difference at 20861: 0.0473442, expected -0.0853729\n",
      "E0127 18:47:03.560035 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671219, expected 0.481705\n",
      "2025-01-27 18:47:03.560039: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.560702 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.560721 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.560727 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.560735 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.560740 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.560744 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.560752 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.560760 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.560767 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.560776 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.560781: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.561510 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.561526 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.561531 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.561540 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.561546 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.561550 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.561557 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.561565 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.561572 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.561581 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.561584: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.562563 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.562579 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.562585 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.562592 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.562596 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.562600 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.562608 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.562616 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.562624 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.562634 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.562637: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.563731 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.563747 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.563753 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.563760 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.563765 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.563769 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.563777 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.563785 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.563793 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.563803 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.563806: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.564615 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.564631 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.564636 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.564643 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.564648 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.564651 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.564659 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.564667 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.564674 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.564683 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.564686: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.565636 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.565653 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.565658 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.565666 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.565670 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.565674 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.565681 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.565690 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.565706 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.565715 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.565718: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.566606 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854904, expected 0.651917\n",
      "E0127 18:47:03.566621 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490173, expected -0.328079\n",
      "E0127 18:47:03.566627 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861664, expected -0.251582\n",
      "E0127 18:47:03.566634 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21642, expected 0.980453\n",
      "E0127 18:47:03.566639 3794949 buffer_comparator.cc:157] Difference at 11249: 0.59843, expected 0.847711\n",
      "E0127 18:47:03.566643 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00585938, expected -0.148094\n",
      "E0127 18:47:03.566650 3794949 buffer_comparator.cc:157] Difference at 15248: 0.39116, expected 0.24692\n",
      "E0127 18:47:03.566659 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0127 18:47:03.566667 3794949 buffer_comparator.cc:157] Difference at 20861: 0.0469704, expected -0.0853729\n",
      "E0127 18:47:03.566676 3794949 buffer_comparator.cc:157] Difference at 24193: 0.670959, expected 0.481705\n",
      "2025-01-27 18:47:03.566678: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0127 18:47:03.568128 3794949 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0127 18:47:03.568147 3794949 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0127 18:47:03.568153 3794949 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0127 18:47:03.568160 3794949 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0127 18:47:03.568164 3794949 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0127 18:47:03.568168 3794949 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0127 18:47:03.568176 3794949 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0127 18:47:03.568184 3794949 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0127 18:47:03.568193 3794949 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0127 18:47:03.568202 3794949 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-27 18:47:03.568205: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=<function real_part>\n",
      "), (ScaleByAdamState(count=Array(40000, dtype=int32), mu=CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=None\n",
      "), nu=CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=None\n",
      ")), EmptyState()), State(0x7daede24c0d0=i32[]), Array([ 7.288151  , 13.96123   ,  3.5011785 , ...,  0.10847425,\n",
      "        0.10036564,  0.09392834], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# run it\n",
    "try:\n",
    "    results = experiment.initialize()\n",
    "    print(results)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print()\n",
    "    traceback.print_exc()\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections.abc import Sequence, Mapping\n",
    "from common_dl_utils.config_realization import PostponedInitialization\n",
    "def complete_postponed_initialization(postponed_init:PostponedInitialization, completion: dict):\n",
    "    postponed_init.resolve_missing_args(completion)\n",
    "    for value in postponed_init.kwargs.values():\n",
    "        if isinstance(value, PostponedInitialization):\n",
    "            complete_postponed_initialization(value, completion)\n",
    "        elif isinstance(value, Sequence):\n",
    "            for v in value:\n",
    "                if isinstance(v, PostponedInitialization):\n",
    "                    complete_postponed_initialization(v, completion)\n",
    "        elif isinstance(value, Mapping):\n",
    "            for v in value.values():\n",
    "                if isinstance(v, PostponedInitialization):\n",
    "                    complete_postponed_initialization(v, completion)\n",
    "\n",
    "\n",
    "def run_experiment(missing_kwargs: dict, config:dict, key:jax.Array):\n",
    "    experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "        prng_key=key,\n",
    "        config=config,\n",
    "        model_kwarg_in_trainer='inr',\n",
    "        model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "        trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "        additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "        add_model_module_to_architecture_default_module=False,\n",
    "        initialize=False  # don't run the experiment yet, we want to add the missing kwargs\n",
    "    )\n",
    "    complete_postponed_initialization(experiment, missing_kwargs)\n",
    "    return experiment.initialize()\n",
    "    #return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "incomplete_config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "incomplete_config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "incomplete_config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "incomplete_config.model_config = Config()\n",
    "incomplete_config.model_config.in_size = 2\n",
    "incomplete_config.model_config.out_size = 3\n",
    "incomplete_config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 3,\n",
    "        #'activation_kwargs': {'w0':12.}, #                        <-------------------------------------------------------------- this is the missin one\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        #'initialization_scheme_kwargs': {'w0': 12.},\n",
    "        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "incomplete_config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "incomplete_config.trainer_type = 'training.train_inr_scan'\n",
    "incomplete_config.loss_evaluator = 'losses.PointWiseLossEvaluator'\n",
    "incomplete_config.target_function = 'images.ContinuousImage'\n",
    "incomplete_config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation'\n",
    "}\n",
    "incomplete_config.loss_function = 'losses.scaled_mse_loss'\n",
    "incomplete_config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "incomplete_config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "incomplete_config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "incomplete_config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "incomplete_config.steps = 40000 #changed from 40000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CombinedINR(\n",
       "   terms=(\n",
       "     MLPINR(\n",
       "       layers=(\n",
       "         CountingIdentity(\n",
       "           _embedding_matrix=f32[3],\n",
       "           state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[256,2],\n",
       "           biases=f32[256],\n",
       "           activation_kwargs={'w0': 12.0}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[256,256],\n",
       "           biases=f32[256],\n",
       "           activation_kwargs={'w0': 12.0}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[256,256],\n",
       "           biases=f32[256],\n",
       "           activation_kwargs={'w0': 12.0}\n",
       "         ),\n",
       "         Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "       )\n",
       "     ),\n",
       "   ),\n",
       "   post_processor=<function real_part>\n",
       " ),\n",
       " (ScaleByAdamState(count=Array(40000, dtype=int32), mu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,2],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  ), nu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,2],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[256,256],\n",
       "            biases=f32[256],\n",
       "            activation_kwargs={'w0': 12.0}\n",
       "          ),\n",
       "          Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  )),\n",
       "  EmptyState()),\n",
       " State(0x7daede24c0d0=i32[]),\n",
       " Array([11.163851  ,  9.481189  ,  6.821996  , ...,  0.09782805,\n",
       "         0.09176412,  0.08511054], dtype=float32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_experiment(\n",
    "    missing_kwargs={\"activation_kwargs\": {\"w0\": 12.}},\n",
    "    config=incomplete_config,\n",
    "    key=next(key_gen)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Documents/INR_BEP/model_components/initialization_schemes.py:66: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n",
      "  return cls(weight, bias, **activation_kwargs)\n"
     ]
    }
   ],
   "source": [
    "import equinox as eqx\n",
    "num_parallel = 10\n",
    "\n",
    "def v_mappable_runner(w0, key):\n",
    "    return_value = run_experiment(\n",
    "        missing_kwargs={\"activation_kwargs\": {\"w0\": w0}},\n",
    "        config=incomplete_config,\n",
    "        key=key\n",
    "    )\n",
    "    #return eqx.filter(return_value, eqx.is_array_like)\n",
    "    return return_value\n",
    "\n",
    "keys = jax.random.split(next(key_gen), num_parallel)\n",
    "w0s = jnp.linspace(10., 30., num=num_parallel)\n",
    "\n",
    "#results = jax.vmap(v_mappable_runner)(w0s, keys)  # no idea why this results in a user warning while the single one doesn't... but it seems to work\n",
    "results = eqx.filter_vmap(v_mappable_runner)(w0s, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/simon/Documents/INR_BEP/model_components/initialization_schemes.py:66: UserWarning: A JAX array is being set as static! This can result in unexpected behavior and is usually a mistake to do.\n",
      "  return cls(weight, bias, **activation_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(CombinedINR(\n",
       "   terms=(\n",
       "     MLPINR(\n",
       "       layers=(\n",
       "         CountingIdentity(\n",
       "           _embedding_matrix=f32[10,3],\n",
       "           state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[10,256,2],\n",
       "           biases=f32[10,256],\n",
       "           activation_kwargs={'w0': f32[]}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[10,256,256],\n",
       "           biases=f32[10,256],\n",
       "           activation_kwargs={'w0': f32[]}\n",
       "         ),\n",
       "         SirenLayer(\n",
       "           weights=f32[10,256,256],\n",
       "           biases=f32[10,256],\n",
       "           activation_kwargs={'w0': f32[]}\n",
       "         ),\n",
       "         Linear(weights=f32[10,3,256], biases=f32[10,3], activation_kwargs={})\n",
       "       )\n",
       "     ),\n",
       "   ),\n",
       "   post_processor=<function real_part>\n",
       " ),\n",
       " (ScaleByAdamState(count=Array([40000, 40000, 40000, 40000, 40000, 40000, 40000, 40000, 40000,\n",
       "         40000], dtype=int32), mu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[10,3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,2],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[]}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,256],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[]}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,256],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[]}\n",
       "          ),\n",
       "          Linear(weights=f32[10,3,256], biases=f32[10,3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  ), nu=CombinedINR(\n",
       "    terms=(\n",
       "      MLPINR(\n",
       "        layers=(\n",
       "          CountingIdentity(\n",
       "            _embedding_matrix=f32[10,3],\n",
       "            state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,2],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[]}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,256],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[]}\n",
       "          ),\n",
       "          SirenLayer(\n",
       "            weights=f32[10,256,256],\n",
       "            biases=f32[10,256],\n",
       "            activation_kwargs={'w0': f32[]}\n",
       "          ),\n",
       "          Linear(weights=f32[10,3,256], biases=f32[10,3], activation_kwargs={})\n",
       "        )\n",
       "      ),\n",
       "    ),\n",
       "    post_processor=None\n",
       "  )),\n",
       "  EmptyState()),\n",
       " State(0x7daede24c0d0=i32[10]),\n",
       " Array([[25.23208   ,  2.5933719 ,  6.378536  , ...,  0.10845421,\n",
       "          0.10386176,  0.07614031],\n",
       "        [ 2.0645764 , 25.206396  ,  2.5043325 , ...,  0.0929113 ,\n",
       "          0.09098022,  0.08648579],\n",
       "        [22.242195  ,  5.4414573 ,  3.4137573 , ...,  0.09876344,\n",
       "          0.08590956,  0.09317067],\n",
       "        ...,\n",
       "        [12.791691  , 28.487856  , 10.455021  , ...,  0.0916782 ,\n",
       "          0.08028591,  0.08070946],\n",
       "        [ 3.9212358 , 36.44636   , 16.845152  , ...,  0.08156741,\n",
       "          0.08129244,  0.07670492],\n",
       "        [13.683941  , 30.009329  ,  8.69969   , ...,  0.08125313,\n",
       "          0.08749905,  0.08224121]], dtype=float32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eqx.filter_vmap(run_experiment, in_axes=(0, None, 0))({\"activation_kwargs\": {\"w0\": w0s}}, incomplete_config, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
