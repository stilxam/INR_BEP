{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying to get multiple INRs to train in parallel on a single GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:22.968569596Z",
     "start_time": "2025-01-06T14:55:22.925237450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-26 00:40:49.192491: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "# import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "# wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:23.553170506Z",
     "start_time": "2025-01-06T14:55:23.447043302Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\n",
    "\n",
    "To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \"CombinedINR\"` instead of `config[\"model_type\"] = \"CombinedINR\"`). You can also just use a dictionary instead.\n",
    "\n",
    "Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\n",
    "\n",
    "Doing this in a config instead of hard coded might seem like extra work, but consider this:\n",
    "1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \n",
    "   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\n",
    "2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\n",
    "   and then run an experiment specified by that config on any machine you want, e.g. on Snellius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:24.410408058Z",
     "start_time": "2025-01-06T14:55:24.393999841Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 3\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 3,\n",
    "        'activation_kwargs': {'w0':12.},#{'inverse_scale': 5.},\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        'initialization_scheme_kwargs': {'w0': 12.},\n",
    "        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "config.trainer_type = 'training.train_inr_scan'\n",
    "config.loss_evaluator = 'losses.PointWiseLossEvaluator'\n",
    "config.target_function = 'images.ContinuousImage'\n",
    "config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation'\n",
    "}\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "config.steps = 40000 #changed from 40000\n",
    "# config.use_wandb = True\n",
    "\n",
    "# # now we want some extra things, like logging, to happen during training\n",
    "# # the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# # The callbacks we want to use can be found in inr_utils.callbacks\n",
    "# config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "# config.after_step_callback_config = {\n",
    "#     'callbacks':[\n",
    "#         ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "#         'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "#         ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "#              {'metric_collector':'metrics.MetricCollector'}\n",
    "#         ),\n",
    "#         'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "#     ],\n",
    "#     'show_logs': False\n",
    "# }\n",
    "\n",
    "# config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\n",
    "\n",
    "# config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "#     'metrics':[\n",
    "#         ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "#         # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "#         ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "#         # ^ compute the MSE with the actual image pixels\n",
    "#     ],\n",
    "#     'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "#     'epoch_frequency': 1  # not actually used\n",
    "# }\n",
    "\n",
    "# #config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "# config.optimizer_state = None  # we're starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T13:11:48.290828912Z",
     "start_time": "2024-11-27T13:11:48.207796154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      layers=(\n",
       "        CountingIdentity(\n",
       "          _embedding_matrix=f32[3],\n",
       "          state_index=StateIndex(\n",
       "            marker=<object object at 0x7b5f400a6dc0>,\n",
       "            init=i32[]\n",
       "          )\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,2],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "      )\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it works properly\n",
    "try:\n",
    "    inr(jnp.zeros(2))\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostponedInitialization(cls=train_inr_scan, kwargs={'steps': 40000, 'loss_evaluator': PostponedInitialization(cls=PointWiseLossEvaluator, kwargs={'target_function': PostponedInitialization(cls=ContinuousImage, kwargs={'image': './example_data/parrot.png', 'scale_to_01': True, 'interpolation_method': <function make_piece_wise_constant_interpolation at 0x7b5efd7ebe20>}, missing_args=[]), 'loss_function': <function scaled_mse_loss at 0x7b5efcd0a680>, 'state_update_function': <function counter_updater at 0x7b5efcd31990>}, missing_args=[]), 'sampler': PostponedInitialization(cls=GridSubsetSampler, kwargs={'size': [2040, 1356], 'batch_size': 2000, 'allow_duplicates': False, 'min': 0.0, 'max': 1.0, 'num_dimensions': None, 'indexing': 'ij'}, missing_args=[]), 'optimizer': PostponedInitialization(cls=adam, kwargs={'learning_rate': 0.00015, 'b1': 0.9, 'b2': 0.999, 'eps': 1e-08, 'eps_root': 0.0, 'mu_dtype': None, 'nesterov': False}, missing_args=[]), 'state_initialization_function': <function initialize_state at 0x7b5efcd301f0>, 'inr': PostponedInitialization(cls=CombinedINR, kwargs={'terms': [PostponedInitialization(cls=from_config, kwargs={'hidden_size': 256, 'num_layers': 5, 'layer_type': <class 'model_components.inr_layers.SirenLayer'>, 'activation_kwargs': {'w0': 12.0}, 'initialization_scheme': <function siren_scheme at 0x7b5f23fb0f70>, 'initialization_scheme_kwargs': {'w0': 12.0}, 'positional_encoding_layer': PostponedInitialization(cls=CountingIdentity, kwargs={}, missing_args=[]), 'num_splits': 3, 'post_processor': None, 'in_size': 2, 'out_size': 3, 'key': Array([4177750840, 1613599438], dtype=uint32)}, missing_args=[])], 'post_processor': <function real_part at 0x7b5f23f10ca0>}, missing_args=[]), 'key': Array([ 793826064, 3381256178], dtype=uint32)}, missing_args=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0126 00:40:57.044915 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.044945 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.044951 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.044958 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.044963 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.044966 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.044974 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.044982 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.044990 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.044998 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.045002: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.045738 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.045755 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.045761 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.045768 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.045772 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.045776 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.045784 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.045792 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.045800 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.045808 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.045812: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.046560 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0126 00:40:57.046577 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490271, expected -0.328079\n",
      "E0126 00:40:57.046583 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861816, expected -0.251582\n",
      "E0126 00:40:57.046590 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21686, expected 0.980453\n",
      "E0126 00:40:57.046594 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598793, expected 0.847711\n",
      "E0126 00:40:57.046598 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00623322, expected -0.148094\n",
      "E0126 00:40:57.046605 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391197, expected 0.24692\n",
      "E0126 00:40:57.046613 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0126 00:40:57.046621 2050018 buffer_comparator.cc:157] Difference at 20861: 0.0473442, expected -0.0853729\n",
      "E0126 00:40:57.046629 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671219, expected 0.481705\n",
      "2025-01-26 00:40:57.046632: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.047340 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0126 00:40:57.047357 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490253, expected -0.328079\n",
      "E0126 00:40:57.047362 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861359, expected -0.251582\n",
      "E0126 00:40:57.047370 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21662, expected 0.980453\n",
      "E0126 00:40:57.047374 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598774, expected 0.847711\n",
      "E0126 00:40:57.047378 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00616455, expected -0.148094\n",
      "E0126 00:40:57.047385 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391201, expected 0.24692\n",
      "E0126 00:40:57.047393 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0633163, expected 0.211475\n",
      "E0126 00:40:57.047401 2050018 buffer_comparator.cc:157] Difference at 20861: 0.0471668, expected -0.0853729\n",
      "E0126 00:40:57.047409 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671066, expected 0.481705\n",
      "2025-01-26 00:40:57.047413: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.048437 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.048453 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.048459 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.048466 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.048470 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.048473 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.048481 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.048489 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.048496 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.048505 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.048507: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.049247 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854897, expected 0.651917\n",
      "E0126 00:40:57.049268 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490271, expected -0.328079\n",
      "E0126 00:40:57.049274 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861816, expected -0.251582\n",
      "E0126 00:40:57.049281 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21686, expected 0.980453\n",
      "E0126 00:40:57.049285 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598793, expected 0.847711\n",
      "E0126 00:40:57.049289 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00623322, expected -0.148094\n",
      "E0126 00:40:57.049296 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391197, expected 0.24692\n",
      "E0126 00:40:57.049305 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0126 00:40:57.049312 2050018 buffer_comparator.cc:157] Difference at 20861: 0.0473442, expected -0.0853729\n",
      "E0126 00:40:57.049322 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671219, expected 0.481705\n",
      "2025-01-26 00:40:57.049327: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.049998 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.050014 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.050019 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.050026 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.050030 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.050034 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.050042 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.050050 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.050057 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.050065 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.050068: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.050798 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.050814 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.050820 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.050827 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.050831 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.050835 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.050842 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.050850 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.050857 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.050866 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.050869: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.051814 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.051830 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.051836 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.051843 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.051847 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.051851 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.051858 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.051866 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.051874 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.051882 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.051885: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.052971 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.052988 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.052994 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.053001 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.053005 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.053009 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.053016 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.053026 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.053033 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.053042 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.053046: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.053982 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.053998 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.054004 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.054011 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.054015 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.054018 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.054026 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.054034 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.054043 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.054051 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.054054: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.055011 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.055027 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.055033 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.055040 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.055044 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.055048 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.055055 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.055063 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.055070 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.055079 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.055082: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.055985 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854904, expected 0.651917\n",
      "E0126 00:40:57.056004 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490173, expected -0.328079\n",
      "E0126 00:40:57.056009 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861664, expected -0.251582\n",
      "E0126 00:40:57.056016 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21642, expected 0.980453\n",
      "E0126 00:40:57.056020 2050018 buffer_comparator.cc:157] Difference at 11249: 0.59843, expected 0.847711\n",
      "E0126 00:40:57.056024 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00585938, expected -0.148094\n",
      "E0126 00:40:57.056032 2050018 buffer_comparator.cc:157] Difference at 15248: 0.39116, expected 0.24692\n",
      "E0126 00:40:57.056040 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632782, expected 0.211475\n",
      "E0126 00:40:57.056048 2050018 buffer_comparator.cc:157] Difference at 20861: 0.0469704, expected -0.0853729\n",
      "E0126 00:40:57.056056 2050018 buffer_comparator.cc:157] Difference at 24193: 0.670959, expected 0.481705\n",
      "2025-01-26 00:40:57.056059: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0126 00:40:57.057498 2050018 buffer_comparator.cc:157] Difference at 2774: 0.854811, expected 0.651917\n",
      "E0126 00:40:57.057516 2050018 buffer_comparator.cc:157] Difference at 5692: -0.490153, expected -0.328079\n",
      "E0126 00:40:57.057522 2050018 buffer_comparator.cc:157] Difference at 7625: -0.0861647, expected -0.251582\n",
      "E0126 00:40:57.057529 2050018 buffer_comparator.cc:157] Difference at 10116: 1.21569, expected 0.980453\n",
      "E0126 00:40:57.057533 2050018 buffer_comparator.cc:157] Difference at 11249: 0.598515, expected 0.847711\n",
      "E0126 00:40:57.057537 2050018 buffer_comparator.cc:157] Difference at 12328: 0.00544977, expected -0.148094\n",
      "E0126 00:40:57.057545 2050018 buffer_comparator.cc:157] Difference at 15248: 0.391176, expected 0.24692\n",
      "E0126 00:40:57.057555 2050018 buffer_comparator.cc:157] Difference at 18309: 0.0632911, expected 0.211475\n",
      "E0126 00:40:57.057562 2050018 buffer_comparator.cc:157] Difference at 20861: 0.046452, expected -0.0853729\n",
      "E0126 00:40:57.057571 2050018 buffer_comparator.cc:157] Difference at 24193: 0.671127, expected 0.481705\n",
      "2025-01-26 00:40:57.057574: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=<function real_part>\n",
      "), (ScaleByAdamState(count=Array(40000, dtype=int32), mu=CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=None\n",
      "), nu=CombinedINR(\n",
      "  terms=(\n",
      "    MLPINR(\n",
      "      layers=(\n",
      "        CountingIdentity(\n",
      "          _embedding_matrix=f32[3],\n",
      "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,2],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        SirenLayer(\n",
      "          weights=f32[256,256],\n",
      "          biases=f32[256],\n",
      "          activation_kwargs={'w0': 12.0}\n",
      "        ),\n",
      "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
      "      )\n",
      "    ),\n",
      "  ),\n",
      "  post_processor=None\n",
      ")), EmptyState()), State(0x7b5f5c8400d0=i32[]), Array([ 7.288151  , 13.96123   ,  3.5011785 , ...,  0.10847425,\n",
      "        0.10036564,  0.09392834], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# run it\n",
    "try:\n",
    "    results = experiment.initialize()\n",
    "    print(results)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print()\n",
    "    traceback.print_exc()\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
