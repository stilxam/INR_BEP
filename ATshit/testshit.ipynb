{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import jax\n",
    "# import jax.numpy as jnp\n",
    "# import equinox as eqx\n",
    "\n",
    "# def generate_alpha(x: jax.Array):\n",
    "#     \"\"\"\n",
    "#     Generate alpha used in the FINER activation.\n",
    "#     FINER generates alpha as |x| + 1.\n",
    "#     :param x: input array for alpha generation.\n",
    "#     :return: alpha array.\n",
    "#     \"\"\"\n",
    "#     return jnp.abs(x) + 1\n",
    "\n",
    "# def finer_activation(x: jax.Array, omega: float):\n",
    "#     \"\"\"\n",
    "#     FINER activation function: sin(omega * alpha * x)\n",
    "#     :param x: input array for activation.\n",
    "#     :param omega: frequency scaling factor (omega).\n",
    "#     :return: output array after applying variable-periodic activation function.\n",
    "#     \"\"\"\n",
    "#     alpha = generate_alpha(x)\n",
    "#     return jnp.sin(omega * alpha * x)\n",
    "\n",
    "# def init_weights(key, fan_in: int, omega: float, is_first: bool):\n",
    "#     \"\"\"\n",
    "#     Initializes weights based on the SIREN/FINER initialization scheme.\n",
    "#     :param key: random key for JAX.\n",
    "#     :param fan_in: input dimension size.\n",
    "#     :param omega: frequency scaling factor (omega).\n",
    "#     :param is_first: boolean indicating if it's the first layer.\n",
    "#     :return: initialized weight array.\n",
    "#     \"\"\"\n",
    "#     if is_first:\n",
    "#         bound = 1.0 / fan_in\n",
    "#     else:\n",
    "#         bound = jnp.sqrt(6.0 / fan_in) / omega\n",
    "#     return jax.random.uniform(key, shape=(fan_in,), minval=-bound, maxval=bound)\n",
    "\n",
    "# def init_bias(key, out_size: int, k: float = 20):\n",
    "#     \"\"\"\n",
    "#     Initializes bias based on a uniform distribution with a larger range.\n",
    "#     :param key: random key for JAX.\n",
    "#     :param out_size: output size (number of neurons in the layer).\n",
    "#     :param k: scaling factor for bias initialization.\n",
    "#     :return: initialized bias array.\n",
    "#     \"\"\"\n",
    "#     return jax.random.uniform(key, shape=(out_size,), minval=-k, maxval=k)\n",
    "\n",
    "# class FinerLayer(eqx.Module):\n",
    "#     \"\"\"\n",
    "#     FINER Layer using variable-periodic activation functions.\n",
    "#     This layer applies a linear transformation followed by a FINER sine activation.\n",
    "#     \"\"\"\n",
    "#     weights: jax.Array\n",
    "#     biases: jax.Array\n",
    "#     omega: float\n",
    "\n",
    "#     def __init__(self, in_size: int, out_size: int, key: jax.Array, omega: float = 30.0, is_first: bool = False):\n",
    "#         \"\"\"\n",
    "#         Initializes the FINER layer with the given parameters.\n",
    "#         :param in_size: input size (number of input features).\n",
    "#         :param out_size: output size (number of neurons).\n",
    "#         :param key: JAX random key for initialization.\n",
    "#         :param omega: frequency scaling factor (omega).\n",
    "#         :param is_first: boolean indicating if this is the first layer.\n",
    "#         \"\"\"\n",
    "#         key_w, key_b = jax.random.split(key)\n",
    "#         self.weights = init_weights(key_w, in_size, omega, is_first)\n",
    "#         self.biases = init_bias(key_b, out_size)\n",
    "#         self.omega = omega\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass: applies the linear transformation and the FINER activation.\n",
    "#         :param x: input array.\n",
    "#         :return: output after applying the FINER activation.\n",
    "#         \"\"\"\n",
    "#         wx_b = jnp.dot(x, self.weights) + self.biases\n",
    "#         return finer_activation(wx_b, self.omega)\n",
    "\n",
    "# class Finer(eqx.Module):\n",
    "#     \"\"\"\n",
    "#     Full FINER model with multiple hidden layers.\n",
    "#     \"\"\"\n",
    "#     layers: list\n",
    "\n",
    "#     def __init__(self, in_size: int, out_size: int, hidden_layers: int = 3, hidden_size: int = 256, key: jax.Array, omega: float = 30.0):\n",
    "#         \"\"\"\n",
    "#         Initialize the FINER network.\n",
    "#         :param in_size: input size (number of input features).\n",
    "#         :param out_size: output size (number of output features).\n",
    "#         :param hidden_layers: number of hidden layers.\n",
    "#         :param hidden_size: number of neurons in each hidden layer.\n",
    "#         :param key: JAX random key for initialization.\n",
    "#         :param omega: frequency scaling factor for the FINER activation.\n",
    "#         \"\"\"\n",
    "#         layers = []\n",
    "#         keys = jax.random.split(key, num=hidden_layers + 2)\n",
    "\n",
    "#         # First layer\n",
    "#         layers.append(FinerLayer(in_size, hidden_size, key=keys[0], omega=omega, is_first=True))\n",
    "\n",
    "#         # Hidden layers\n",
    "#         for i in range(hidden_layers):\n",
    "#             layers.append(FinerLayer(hidden_size, hidden_size, key=keys[i+1], omega=omega))\n",
    "\n",
    "#         # Output layer (no activation)\n",
    "#         layers.append(FinerLayer(hidden_size, out_size, key=keys[-1], omega=omega, is_first=False))\n",
    "\n",
    "#         self.layers = layers\n",
    "\n",
    "#     def __call__(self, x):\n",
    "#         \"\"\"\n",
    "#         Forward pass through the entire FINER network.\n",
    "#         :param x: input array.\n",
    "#         :return: output of the model.\n",
    "#         \"\"\"\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x)\n",
    "#         return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def init_bias(shape, k, key):\n",
    "    \"\"\"\n",
    "    Initializes bias values for a layer uniformly in the range [-k, k].\n",
    "\n",
    "    :param shape: Shape of the bias vector (e.g., (out_features,)).\n",
    "    :param k: Bound for uniform initialization, i.e., bias values are drawn from [-k, k].\n",
    "    :param key: JAX random key for generating random values.\n",
    "    :return: A JAX array with initialized biases.\n",
    "    \"\"\"\n",
    "    return jax.random.uniform(key, shape, minval=-k, maxval=k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias_cond(is_first, shape, fbs=None, key=None):\n",
    "    \"\"\"\n",
    "    Conditionally initializes the bias based on whether it is the first layer and if an fbs value is provided.\n",
    "\n",
    "    :param is_first: Boolean indicating if this is the first layer in the network.\n",
    "    :param shape: Shape of the bias vector.\n",
    "    :param fbs: Bound for uniform initialization, used only if `is_first` is True.\n",
    "    :param key: JAX random key for generating random values.\n",
    "    :return: Initialized bias, or None if not the first layer and no fbs is provided.\n",
    "    \"\"\"\n",
    "    if is_first and fbs is not None and key is not None:\n",
    "        return init_bias(shape, fbs, key)\n",
    "    return None  # Return None if conditions are not met\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alpha(x):\n",
    "    \"\"\"\n",
    "    Generates a scaling factor (alpha) for input x based on its magnitude.\n",
    "    \n",
    "    :param x: Input tensor.\n",
    "    :return: Scaling factor based on |x| + 1.\n",
    "    \"\"\"\n",
    "    return jnp.abs(x) + 1\n",
    "\n",
    "def finer_activation(x, omega=1):\n",
    "    \"\"\"\n",
    "    Variable-periodic activation function for FINER.\n",
    "    \n",
    "    :param x: Input tensor.\n",
    "    :param omega: Frequency control parameter.\n",
    "    :return: Activated tensor.\n",
    "    \"\"\"\n",
    "    alpha = generate_alpha(x)\n",
    "    return jnp.sin(omega * alpha * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "from jax import random\n",
    "\n",
    "class FINERLayer(eqx.Module):\n",
    "    \"\"\"\n",
    "    A single FINER layer with a configurable sine activation.\n",
    "    \"\"\"\n",
    "    weight: jnp.ndarray\n",
    "    bias: jnp.ndarray\n",
    "    omega: float\n",
    "    is_last: bool = False\n",
    "\n",
    "    def __init__(self, in_features, out_features, key, omega=30, is_last=False):\n",
    "        self.omega = omega\n",
    "        self.is_last = is_last\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        w_key, b_key = random.split(key)\n",
    "        fan_in = in_features\n",
    "        bound = 1.0 / fan_in if is_last else jnp.sqrt(6.0 / fan_in) / omega\n",
    "        self.weight = random.uniform(w_key, (out_features, in_features), minval=-bound, maxval=bound)\n",
    "        self.bias = random.uniform(b_key, (out_features,), minval=-1.0, maxval=1.0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Linear transformation: xW + b\n",
    "        wx_b = jnp.dot(x, self.weight.T) + self.bias\n",
    "        # Apply the finer_activation unless this is the last layer\n",
    "        return wx_b if self.is_last else finer_activation(wx_b, omega=self.omega)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FINERModel(eqx.Module):\n",
    "    \"\"\"\n",
    "    FINER model composed of multiple FINER layers.\n",
    "    \"\"\"\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, in_features=2, out_features=3, hidden_layers=3, hidden_features=256, \n",
    "                 first_omega=30, hidden_omega=30, key=None):\n",
    "        keys = random.split(key, hidden_layers + 2)\n",
    "        \n",
    "        # Define layers\n",
    "        self.layers = []\n",
    "        \n",
    "        # Input layer with first_omega\n",
    "        self.layers.append(FINERLayer(in_features, hidden_features, keys[0], omega=first_omega, is_last=False))\n",
    "        \n",
    "        # Hidden layers with hidden_omega\n",
    "        for i in range(1, hidden_layers + 1):\n",
    "            self.layers.append(FINERLayer(hidden_features, hidden_features, keys[i], omega=hidden_omega, is_last=False))\n",
    "        \n",
    "        # Output layer (no activation)\n",
    "        self.layers.append(FINERLayer(hidden_features, out_features, keys[-1], omega=hidden_omega, is_last=True))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Pass the input through each layer in sequence\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4858135  0.4693304  0.6433001 ]\n",
      " [0.49075848 0.45279476 0.62308896]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "key = random.PRNGKey(0)\n",
    "finer_model = FINERModel(in_features=2, out_features=3, hidden_layers=3, hidden_features=256,\n",
    "                         first_omega=30, hidden_omega=30, key=key)\n",
    "\n",
    "# Example input\n",
    "x = jnp.array([[0.5, -1.0], [1.0, 0.5]])  # Batch of inputs\n",
    "\n",
    "# Forward pass through the model\n",
    "output = finer_model(x)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def load_image(image_path):\n",
    "    # Load image and convert to RGB\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return img\n",
    "\n",
    "def prepare_data(img):\n",
    "    # Get image dimensions\n",
    "    height, width, _ = img.shape\n",
    "\n",
    "    # Create normalized coordinates and RGB values\n",
    "    x_coords = np.linspace(-1, 1, width)\n",
    "    y_coords = np.linspace(-1, 1, height)\n",
    "    coords = np.array(np.meshgrid(x_coords, y_coords)).reshape(2, -1).T  # Shape: (width*height, 2)\n",
    "    rgb_values = img.reshape(-1, 3)  # Flatten RGB values to match coords\n",
    "\n",
    "    return coords, rgb_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got int32. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m finer_model, opt_state, loss\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_steps):\n\u001b[0;32m---> 39\u001b[0m     finer_model, opt_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiner_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgb_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m1000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     41\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 33\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(finer_model, opt_state, coords, rgb_values)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_step\u001b[39m(finer_model, opt_state, coords, rgb_values):\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;66;03m# Compute loss and gradients\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiner_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrgb_values\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state)\n\u001b[1;32m     35\u001b[0m     finer_model \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mapply_updates(finer_model, updates)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api.py:501\u001b[0m, in \u001b[0;36m_check_input_dtype_revderiv\u001b[0;34m(name, holomorphic, allow_int, x)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, dtypes\u001b[38;5;241m.\u001b[39mextended) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    498\u001b[0m     dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minteger) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mbool_)):\n\u001b[1;32m    500\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_int:\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires real- or complex-valued inputs (input dtype \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    502\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthat is a sub-dtype of np.inexact), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you want to use Boolean- or integer-valued inputs, use vjp \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor set allow_int to True.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    505\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m dtypes\u001b[38;5;241m.\u001b[39missubdtype(aval\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39minexact):\n\u001b[1;32m    506\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires numerical-valued inputs (input dtype that is a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    507\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msub-dtype of np.bool_ or np.number), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maval\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: grad requires real- or complex-valued inputs (input dtype that is a sub-dtype of np.inexact), but got int32. If you want to use Boolean- or integer-valued inputs, use vjp or set allow_int to True."
     ]
    }
   ],
   "source": [
    "\n",
    "import optax\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "\n",
    "# Load and prepare image\n",
    "image_path = \"example_data/parrot.png\"  # Replace with your image path\n",
    "img = load_image(image_path)\n",
    "coords, rgb_values = prepare_data(img)\n",
    "\n",
    "# Convert data to JAX arrays\n",
    "coords = jnp.array(coords)\n",
    "rgb_values = jnp.array(rgb_values)\n",
    "\n",
    "# Initialize model\n",
    "key = jax.random.PRNGKey(0)\n",
    "finer_model = FINERModel(in_features=2, out_features=3, hidden_layers=3, hidden_features=256, key=key)\n",
    "\n",
    "# Define loss function\n",
    "def loss_fn(params, coords, rgb_values):\n",
    "    pred_rgb = eqx.filter_jit(finer_model)(coords)  # Predict RGB values\n",
    "    return jnp.mean((pred_rgb - rgb_values) ** 2)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-4)\n",
    "opt_state = optimizer.init(finer_model)\n",
    "\n",
    "# Training loop\n",
    "num_steps = 10000\n",
    "\n",
    "@jax.jit\n",
    "def train_step(finer_model, opt_state, coords, rgb_values):\n",
    "    # Compute loss and gradients\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(finer_model, coords, rgb_values)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    finer_model = eqx.apply_updates(finer_model, updates)\n",
    "    return finer_model, opt_state, loss\n",
    "\n",
    "for step in range(num_steps):\n",
    "    finer_model, opt_state, loss = train_step(finer_model, opt_state, coords, rgb_values)\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random chunks of cod ethat i have been messing with to try and make it work \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "def generate_alpha(x):\n",
    "    \"\"\"\n",
    "    Generates a scaling factor (alpha) for input x based on its magnitude.\n",
    "    \"\"\"\n",
    "    return jnp.abs(x) + 1\n",
    "\n",
    "def finer_activation(x, omega=1):\n",
    "    \"\"\"\n",
    "    Variable-periodic activation function for FINER.\n",
    "    \"\"\"\n",
    "    alpha = generate_alpha(x)\n",
    "    return jnp.sin(omega * alpha * x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import activation_functions as act  # Import finer_activation here if needed\n",
    "from .inr_layers import INRLayer\n",
    "\n",
    "class FinerLayer(INRLayer):\n",
    "    \"\"\"\n",
    "    A single FINER layer with a configurable sine activation.\n",
    "    \"\"\"\n",
    "    allowed_keys = frozenset({'omega'})  # omega for frequency control\n",
    "    allows_multiple_weights_and_biases = False\n",
    "\n",
    "    def __init__(self, in_features, out_features, omega=30, key=None, is_last=False):\n",
    "        self.omega = omega\n",
    "        self.is_last = is_last\n",
    "        \n",
    "        # Initialize weights and biases with special conditions\n",
    "        w_key, b_key = jax.random.split(key)\n",
    "        fan_in = in_features\n",
    "        bound = 1.0 / fan_in if is_last else jnp.sqrt(6.0 / fan_in) / omega\n",
    "        self.weight = jax.random.uniform(w_key, (out_features, in_features), minval=-bound, maxval=bound)\n",
    "        self.bias = jax.random.uniform(b_key, (out_features,), minval=-1.0, maxval=1.0)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Linear transformation: xW + b\n",
    "        wx_b = jnp.dot(x, self.weight.T) + self.bias\n",
    "        # Apply finer_activation unless this is the last layer\n",
    "        return wx_b if self.is_last else act.finer_activation(wx_b, omega=self.omega)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from .inr_layers import INRLayer  # Make sure this path aligns with your file structure\n",
    "\n",
    "class FinerLayer(INRLayer):\n",
    "    \"\"\"\n",
    "    FINER layer with custom initialization and activation function.\n",
    "    \"\"\"\n",
    "    allowed_keys = frozenset({'omega'})  # omega for frequency control\n",
    "    allows_multiple_weights_and_biases = False\n",
    "\n",
    "    def __init__(self, weight, bias, omega=30, is_last=False):\n",
    "        \"\"\"\n",
    "        Initialize FinerLayer with specified weight and bias.\n",
    "        \n",
    "        :param weight: Weight matrix.\n",
    "        :param bias: Bias vector.\n",
    "        :param omega: Frequency control parameter.\n",
    "        :param is_last: Whether this is the last layer (no activation if True).\n",
    "        \"\"\"\n",
    "        self.weight = weight\n",
    "        self.bias = bias\n",
    "        self.omega = omega\n",
    "        self.is_last = is_last\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, in_size, out_size, key, is_first=False, omega=30, fbs=None):\n",
    "        \"\"\"\n",
    "        Custom from_config for FinerLayer to handle unique initialization requirements.\n",
    "\n",
    "        :param in_size: Input size.\n",
    "        :param out_size: Output size.\n",
    "        :param key: Random key for initializing weights and biases.\n",
    "        :param is_first: Flag to indicate if this is the first layer.\n",
    "        :param omega: Frequency control parameter.\n",
    "        :param fbs: Bound for initializing the bias in the first layer.\n",
    "        :return: Initialized FinerLayer instance with weights and biases.\n",
    "        \"\"\"\n",
    "        w_key, b_key = jax.random.split(key)\n",
    "\n",
    "        # Custom weight initialization\n",
    "        fan_in = in_size\n",
    "        bound = 1.0 / fan_in if is_first else jnp.sqrt(6.0 / fan_in) / omega\n",
    "        weight = jax.random.uniform(w_key, shape=(out_size, in_size), minval=-bound, maxval=bound)\n",
    "\n",
    "        # Custom bias initialization for the first layer\n",
    "        if is_first and fbs is not None:\n",
    "            bias = jax.random.uniform(b_key, shape=(out_size,), minval=-fbs, maxval=fbs)\n",
    "        else:\n",
    "            bias = jax.random.uniform(b_key, shape=(out_size,), minval=-1.0, maxval=1.0)\n",
    "\n",
    "        # Return an instance of FinerLayer with initialized weights and biases\n",
    "        return cls(weight=weight, bias=bias, omega=omega, is_last=not is_first)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (inr_edu_24)",
   "language": "python",
   "name": "inr_edu_24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
