{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdtab\u001b[0m (\u001b[33mabdtab-tue\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Data Preparation for Audio Modality\n",
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from scipy.io import wavfile\n",
    "\n",
    "file_path = \"C:/Users/abdel/Downloads/Salem_Arabiano.wav\"\n",
    "\n",
    "# Function to load and preprocess audio data\n",
    "def load_audio(file_path, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Load audio file and resample to the desired sample rate.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the audio file.\n",
    "        sample_rate (int): Desired sample rate.\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: Normalized audio signal.\n",
    "        float: Duration of the audio in seconds.\n",
    "    \"\"\"\n",
    "    original_rate, audio = wavfile.read(file_path)\n",
    "\n",
    "    if len(audio.shape) > 1:\n",
    "        # Convert to mono if stereo\n",
    "        audio = np.mean(audio, axis=1)\n",
    "\n",
    "    # Resample if necessary\n",
    "    if original_rate != sample_rate:\n",
    "        duration = len(audio) / original_rate\n",
    "        time_old = np.linspace(0, duration, len(audio))\n",
    "        time_new = np.linspace(0, duration, int(sample_rate * duration))\n",
    "        audio = np.interp(time_new, time_old, audio)\n",
    "\n",
    "    # Normalize audio\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "    return jnp.array(audio), duration\n",
    "\n",
    "# Function to sample audio data at specific time points\n",
    "def sample_audio(audio, duration, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Sample the audio signal at specific time points.\n",
    "\n",
    "    Args:\n",
    "        audio (jnp.ndarray): Normalized audio signal.\n",
    "        duration (float): Duration of the audio in seconds.\n",
    "        num_samples (int): Number of samples.\n",
    "\n",
    "    Returns:\n",
    "        jnp.ndarray: Time points.\n",
    "        jnp.ndarray: Corresponding audio values.\n",
    "    \"\"\"\n",
    "    time_points = jnp.linspace(0, duration, num_samples)\n",
    "    indices = (time_points * len(audio) / duration).astype(int)\n",
    "    indices = jnp.clip(indices, 0, len(audio) - 1)  # Ensure indices are within bounds\n",
    "\n",
    "    return time_points, audio[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the audio file\n",
    "audio_data, fragment_length = load_audio_file(config.target_function_config['audio_file'])\n",
    "\n",
    "# Create the sampler\n",
    "window_size = 1024  # Adjust this based on your needs\n",
    "batch_size = 32     # Adjust based on your memory constraints\n",
    "sampler = SoundSampler(\n",
    "    sound_fragment=audio_data,\n",
    "    fragment_length=fragment_length,\n",
    "    window_size=window_size,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# Create the loss evaluator\n",
    "loss_evaluator = SoundLossEvaluator(\n",
    "    time_domain_weight=1.0,\n",
    "    frequency_domain_weight=0.1  # Adjust these weights as needed\n",
    ")\n",
    "\n",
    "# Initialize the model (using your existing config)\n",
    "model = cju.construct_model(config)\n",
    "\n",
    "# Create optimizer\n",
    "learning_rate = 1e-4\n",
    "optimizer = optax.adam(learning_rate)\n",
    "opt_state = optimizer.init(model)\n",
    "\n",
    "# Training step function\n",
    "@jax.jit\n",
    "def train_step(model, opt_state, key):\n",
    "    # Get batch of samples\n",
    "    time_points, pressure_values = sampler(key)\n",
    "    \n",
    "    # Calculate loss and gradients\n",
    "    def loss_fn(model):\n",
    "        loss, _ = loss_evaluator(model, (time_points, pressure_values))\n",
    "        return loss\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(model)\n",
    "    \n",
    "    # Update model\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    model = optax.apply_updates(model, updates)\n",
    "    \n",
    "    return model, opt_state, loss\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"inr-audio\",\n",
    "    config={\n",
    "        \"window_size\": window_size,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"num_epochs\": num_epochs\n",
    "    }\n",
    ")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    model, opt_state, loss = train_step(model, opt_state, subkey)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "        wandb.log({\"loss\": loss})\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'audio_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m\n\u001b[1;32m     30\u001b[0m config\u001b[38;5;241m.\u001b[39mtarget_function_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maudio_file\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/abdel/Downloads/Salem_Arabiano.wav\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Set the path to the audio file you want to use\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscale_to_01\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     33\u001b[0m }\n\u001b[1;32m     35\u001b[0m config\u001b[38;5;241m.\u001b[39mloss_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlosses.scaled_mse_loss\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     36\u001b[0m config\u001b[38;5;241m.\u001b[39msampler \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msampling.GridSubsetSampler\u001b[39m\u001b[38;5;124m'\u001b[39m,{\n\u001b[0;32m---> 37\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[43maudio_length\u001b[49m],  \u001b[38;5;66;03m# Define the length of the audio signal\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2000\u001b[39m,\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mallow_duplicates\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m })\n\u001b[1;32m     42\u001b[0m config\u001b[38;5;241m.\u001b[39moptimizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     43\u001b[0m config\u001b[38;5;241m.\u001b[39moptimizer_config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1.5e-4\u001b[39m\n\u001b[1;32m     45\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'audio_length' is not defined"
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "\n",
    "# Define the audio input (1D) modality\n",
    "config.architecture = './model_components'  \n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 1  # Since audio is 1D\n",
    "config.model_config.out_size = 1  # For output signals\n",
    "\n",
    "config.model_config.terms = [\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.FinerLayer',  \n",
    "        'num_splits': 1,\n",
    "        'use_complex': False,\n",
    "        'activation_kwargs': {'w0': 30}, \n",
    "        'initialization_scheme':'initialization_schemes.finer_scheme',\n",
    "        'initialization_scheme_kwargs':{'bias_k' : 10} \n",
    "    })\n",
    "]\n",
    "\n",
    "# Training setup for audio modality\n",
    "config.trainer_module = './inr_utils/' \n",
    "config.trainer_type = 'training.train_inr'\n",
    "\n",
    "# Set up for audio data input\n",
    "config.target_function = 'audio.AudioSignal'  # Assuming you have an audio data handler\n",
    "config.target_function_config = {\n",
    "    'audio_file': 'C:/Users/abdel/Downloads/Salem_Arabiano.wav',  # Set the path to the audio file you want to use\n",
    "    'scale_to_01': True,\n",
    "}\n",
    "\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.sampler = ('sampling.GridSubsetSampler',{\n",
    "    'size': [audio_length],  # Define the length of the audio signal\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "config.steps = 40000\n",
    "config.use_wandb = False  # You can enable this if you're logging with Weights & Biases\n",
    "\n",
    "config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "config.after_step_callback_config = {\n",
    "    'callbacks': [\n",
    "        ('callbacks.print_loss', {'after_every':400}),\n",
    "        'callbacks.report_loss',\n",
    "        ('callbacks.MetricCollectingCallback', {'metric_collector':'metrics.MetricCollector'}),\n",
    "        'callbacks.raise_error_on_nan'\n",
    "    ],\n",
    "    'show_logs': False\n",
    "}\n",
    "\n",
    "config.metric_collector_config = {\n",
    "    'metrics': [\n",
    "        ('metrics.MSEOnFixedGrid', {'grid': [audio_length], 'batch_size': audio_length, 'frequency': 'every_n_batches'})\n",
    "    ],\n",
    "    'batch_frequency': 400,\n",
    "    'epoch_frequency': 1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inr(jnp.zeros(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we run the experiment while logging things to wandb\n",
    "with wandb.init(\n",
    "    project='inr_edu_24',\n",
    "    notes='test',\n",
    "    tags=['test']\n",
    ") as run:\n",
    "    results = experiment.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a configuration object\n",
    "config = Config()\n",
    "config.architecture = './model_components'\n",
    "\n",
    "# We'll still use the CombinedINR (summing outputs of multiple sub-MLPs)\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "# Now define how many input/output dimensions your INR expects\n",
    "# For 1D audio (mono), in_size=1 and out_size=1\n",
    "# (If stereo, you might do in_size=1, out_size=2, etc.)\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 1\n",
    "config.model_config.out_size = 1\n",
    "\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    # ('inr_modules.MLPINR.new_from_config',{\n",
    "    #     'hidden_size': 256,\n",
    "    #     'num_layers': 5,\n",
    "    #     'layer_type': 'inr_layers.SirenLayer',\n",
    "    #     'num_splits': 1,\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'w0': 30.},\n",
    "    #     'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "    #     'positional_encoding_layer': ('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies': 10}),\n",
    "    # }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "    ('inr_modules.MLPINR.new_from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.FinerLayer',\n",
    "        'num_splits': 1,\n",
    "        'use_complex': False,\n",
    "        'activation_kwargs': {'w0': 30},\n",
    "        'initialization_scheme':'initialization_schemes.finer_scheme',\n",
    "        'initialization_scheme_kwargs':{'bias_k' : 10}\n",
    "        # 'initialization_scheme_k' : {'k': 20}\n",
    "        #'positional_encoding_layer': ('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies': 10}),\n",
    "    })\n",
    "]\n",
    "\n",
    "# Tell the system where to look for trainer objects\n",
    "config.trainer_module = './inr_utils/'\n",
    "config.trainer_type = 'training.train_inr'\n",
    "\n",
    "\n",
    "config.target_function = 'audio.ContinuousAudio'\n",
    "config.target_function_config = {\n",
    "    'audio_file': './example_data/example.wav',\n",
    "    # If your audio class has a built-in normalization, you can set it here\n",
    "    'scale_to_01': True,\n",
    "    # If you have a custom interpolation method or want nearest-sample access,\n",
    "    # define it or remove if not needed\n",
    "    'interpolation_method': 'audio.make_piecewise_constant_interpolation',\n",
    "    # Additional parameters your audio class might need\n",
    "    'sample_rate': 16000,\n",
    "}\n",
    "\n",
    "\n",
    "config.loss_function = 'losses.scaled_mse_loss_with_scale_factor'\n",
    "config.loss_function_config = {\n",
    "    'scale_factor': 10\n",
    "}     \n",
    "                  \n",
    "\n",
    "                \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (inr_edu_24)",
   "language": "python",
   "name": "inr_edu_24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
