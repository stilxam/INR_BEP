{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mabdtab\u001b[0m (\u001b[33mabdtab-tue\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pdb\n",
    "import os\n",
    "import traceback\n",
    "import numpy as np\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "import equinox as eqx\n",
    "from typing import Optional, Callable\n",
    "import librosa\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_file(file_path, sr=16000, save_npy=True):\n",
    "    \"\"\"\n",
    "    Load an audio file and return it as a normalized numpy array.\n",
    "    Optionally save as .npy file.\n",
    "    \n",
    "    Args:\n",
    "        file_path: Path to the audio file\n",
    "        sr: Target sampling rate (default: 16000)\n",
    "        save_npy: Whether to save the audio as .npy file (default: True)\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (audio_array, fragment_length, npy_path)\n",
    "    \"\"\"\n",
    "    file_path = './data_gt_bach.wav'\n",
    "    # Load the audio file\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    # Convert to numpy array and normalize to [-1, 1] range\n",
    "    audio = np.array(audio, dtype=np.float32)\n",
    "    audio = audio / np.max(np.abs(audio))\n",
    "\n",
    "    if save_npy:\n",
    "        # Create npy filename from original audio filename\n",
    "        npy_path = os.path.splitext(file_path)[0] + '.npy'\n",
    "        np.save(npy_path, audio)\n",
    "        return audio, len(audio), npy_path\n",
    "    \n",
    "    return audio, len(audio), None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell [3] - Configuration setup\n",
    "config = Config()\n",
    "# audio_path = './data_gt_bach.wav'\n",
    "# audio_data, fragment_length = load_audio_file(audio_path)\n",
    "\n",
    "# Model architecture configuration\n",
    "config.architecture = './model_components'\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "# Model configuration\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 1  # Time dimension input\n",
    "config.model_config.out_size = 1  # Audio amplitude output\n",
    "config.model_config.terms = [\n",
    "    # ('inr_modules.MLPINR.new_from_config',{\n",
    "    #     'hidden_size': 256,\n",
    "    #     'num_layers': 5,\n",
    "    #     'layer_type': 'inr_layers.SirenLayer',\n",
    "    #     'num_splits': 1,\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'w0': 30.},\n",
    "    #     'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "    #     'positional_encoding_layer': ('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies': 10}),\n",
    "    # }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.FinerLayer',\n",
    "        'num_splits': 1,\n",
    "        'use_complex': False,\n",
    "        'activation_kwargs': {'w0': 10},\n",
    "        'initialization_scheme':'initialization_schemes.finer_scheme',\n",
    "        'initialization_scheme_kwargs':{'bias_k' : 5}\n",
    "        # 'initialization_scheme_k' : {'k': 20}\n",
    "        #'positional_encoding_layer': ('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies': 10}),\n",
    "    })\n",
    "]\n",
    "\n",
    "# Training configuration\n",
    "config.trainer_module = './inr_utils/'\n",
    "config.trainer_type = 'training.train_inr'\n",
    "\n",
    "# # Target function configuration  # don't need this because neither sampler nor loss evaluator uses it\n",
    "# config.target_function = 'audio.ContinuousAudio'\n",
    "# config.target_function_config = {\n",
    "#     'audio_file': './example_data/example.wav',\n",
    "#     'scale_to_01': True,\n",
    "#     'interpolation_method': 'audio.make_piecewise_constant_interpolation',\n",
    "#     'sample_rate': 16000,\n",
    "# }\n",
    "\n",
    "# Loss function configuration\n",
    "config.loss_function = 'losses.SoundLossEvaluator'\n",
    "config.loss_function_config = {\n",
    "    'time_domain_weight': 1.0,\n",
    "    'frequency_domain_weight': 0.1\n",
    "}\n",
    "\n",
    "config.optimizer = 'optax.adam'\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1e-3\n",
    "}\n",
    "\n",
    "# Sampler configuration\n",
    "config.sampler = ('sampling.SoundSampler', {\n",
    "    'window_size': 1024,\n",
    "    'batch_size': 32,\n",
    "    'fragment_length': None, # Will be set after loading audio\n",
    "    'sound_fragment': './data_gt_bach.npy',  # TODO store audio file as npy and put path here\n",
    "    #'sound_fragment': \"path_to_audio_file.npy\",  # TODO store audio file as npy and put path here\n",
    "})\n",
    "config.optimizer_state = None \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the two code blocks below are where the erros are coming from, it is in the training loop the code below is way more than it has to be and ivejust been trying to fix the errors, thank you agian for the help :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abdtab/INR_BEP/inr_audio/wandb/run-20250121_141253-5146l816</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdtab-tue/inr-audio/runs/5146l816' target=\"_blank\">autumn-dawn-24</a></strong> to <a href='https://wandb.ai/abdtab-tue/inr-audio' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdtab-tue/inr-audio' target=\"_blank\">https://wandb.ai/abdtab-tue/inr-audio</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdtab-tue/inr-audio/runs/5146l816' target=\"_blank\">https://wandb.ai/abdtab-tue/inr-audio/runs/5146l816</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2514/742912583.py\", line 40, in <module>\n",
      "    model, opt_state, loss, state = train_step(config.model_config, config.optimizer_state, subkey)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_jit.py\", line 242, in __call__\n",
      "    return self._call(False, args, kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_module.py\", line 1078, in __call__\n",
      "    return self.__func__(self.__self__, *args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_jit.py\", line 215, in _call\n",
      "    out = self._cached(dynamic_donate, dynamic_nodonate, static)\n",
      "ValueError: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'tuple'>, (((<function make_inr_train_step_function.<locals>.train_step at 0x7f93d310e3b0>,), PyTreeDef(*)), (({'in_size': 1, 'out_size': 1, 'terms': [('inr_modules.MLPINR.from_config', {'hidden_size': 256, 'num_layers': 5, 'layer_type': 'inr_layers.FinerLayer', 'num_splits': 1, 'use_complex': False, 'activation_kwargs': {'w0': 10}, 'initialization_scheme': 'initialization_schemes.finer_scheme', 'initialization_scheme_kwargs': {'bias_k': 5}})]},), PyTreeDef(*)), ((None,), PyTreeDef(((None, *, None), {})))). The error was:\n",
      "TypeError: unhashable type: 'Config'\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">autumn-dawn-24</strong> at: <a href='https://wandb.ai/abdtab-tue/inr-audio/runs/5146l816' target=\"_blank\">https://wandb.ai/abdtab-tue/inr-audio/runs/5146l816</a><br/> View project at: <a href='https://wandb.ai/abdtab-tue/inr-audio' target=\"_blank\">https://wandb.ai/abdtab-tue/inr-audio</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250121_141253-5146l816/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'tuple'>, (((<function make_inr_train_step_function.<locals>.train_step at 0x7f93d310e3b0>,), PyTreeDef(*)), (({'in_size': 1, 'out_size': 1, 'terms': [('inr_modules.MLPINR.from_config', {'hidden_size': 256, 'num_layers': 5, 'layer_type': 'inr_layers.FinerLayer', 'num_splits': 1, 'use_complex': False, 'activation_kwargs': {'w0': 10}, 'initialization_scheme': 'initialization_schemes.finer_scheme', 'initialization_scheme_kwargs': {'bias_k': 5}})]},), PyTreeDef(*)), ((None,), PyTreeDef(((None, *, None), {})))). The error was:\nTypeError: unhashable type: 'Config'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# The train_step function handles sampling internally, so we don't need to call sampler\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m model, opt_state, loss, state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_jit.py:215\u001b[0m, in \u001b[0;36m_JitWrapper._call\u001b[0;34m(self, is_lower, args, kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m             out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cached(dynamic_donate, dynamic_nodonate, static)\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdynamic_donate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdynamic_nodonate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m XlaRuntimeError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# Catch Equinox's runtime errors, and re-raise them with actually useful\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# information. (By default XlaRuntimeError produces a lot of terrifying\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     \u001b[38;5;66;03m# but useless information.)\u001b[39;00m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    221\u001b[0m         last_msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m last_stack \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[38;5;66;03m# callback necessarily executed in the same interpreter as we are in\u001b[39;00m\n\u001b[1;32m    228\u001b[0m         \u001b[38;5;66;03m# here?\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Non-hashable static arguments are not supported. An error occurred while trying to hash an object of type <class 'tuple'>, (((<function make_inr_train_step_function.<locals>.train_step at 0x7f93d310e3b0>,), PyTreeDef(*)), (({'in_size': 1, 'out_size': 1, 'terms': [('inr_modules.MLPINR.from_config', {'hidden_size': 256, 'num_layers': 5, 'layer_type': 'inr_layers.FinerLayer', 'num_splits': 1, 'use_complex': False, 'activation_kwargs': {'w0': 10}, 'initialization_scheme': 'initialization_schemes.finer_scheme', 'initialization_scheme_kwargs': {'bias_k': 5}})]},), PyTreeDef(*)), ((None,), PyTreeDef(((None, *, None), {})))). The error was:\nTypeError: unhashable type: 'Config'\n"
     ]
    }
   ],
   "source": [
    "# Cell [7] - Training loop\n",
    "import sys\n",
    "sys.path.append('/home/abdtab/INR_BEP')  # Add the parent directory containing inr_utils\n",
    "\n",
    "import inr_utils.sampling as sampling\n",
    "\n",
    "from inr_utils.training import make_inr_train_step_function\n",
    "\n",
    "train_step = make_inr_train_step_function(\n",
    "    loss_evaluator=config.loss_function,\n",
    "    sampler=config.sampler,\n",
    "    optimizer=config.optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 1000\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "\n",
    "# First, instantiate the sampler\n",
    "sampler_class_name, sampler_config = config.sampler\n",
    "sampler = sampling.SoundSampler(**sampler_config)\n",
    "\n",
    "# First, instantiate the sampler\n",
    "sampler_class_name, sampler_config = config.sampler\n",
    "sampler = sampling.SoundSampler(**sampler_config)\n",
    "\n",
    "with wandb.init(\n",
    "    project=\"inr-audio\",\n",
    "    config={\n",
    "        \"window_size\": config.sampler[1]['window_size'],\n",
    "        \"batch_size\": config.sampler[1]['batch_size'],\n",
    "        \"learning_rate\": config.optimizer_config['learning_rate'],\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"audio_path\": './data_gt_bach.npy'\n",
    "    }\n",
    ") as run:\n",
    "    for epoch in range(num_epochs):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        # The train_step function handles sampling internally, so we don't need to call sampler\n",
    "        model, opt_state, loss, state = train_step(config.model_config, config.optimizer_state, subkey)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "            wandb.log({\n",
    "                \"loss\": loss,\n",
    "                \"epoch\": epoch,\n",
    "            })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel_components\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minr_modules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CombinedINR  \u001b[38;5;66;03m# Import your model class\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Create train_step with config components\u001b[39;00m\n\u001b[1;32m     11\u001b[0m train_step \u001b[38;5;241m=\u001b[39m make_inr_train_step_function(\n\u001b[0;32m---> 12\u001b[0m     loss_evaluator\u001b[38;5;241m=\u001b[39m\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39mloss_function,\n\u001b[1;32m     13\u001b[0m     sampler\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39msampler,\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39moptimizer\n\u001b[1;32m     15\u001b[0m )\n\u001b[1;32m     17\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     18\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "# this is the code i wrote to try and get the model working and solving the errors \n",
    "# and i know for a fact that this is way more complicated than it needs to be\n",
    "# but i was solving the errors with gpt and cursor at that point cuz i gave up on manually fixing it \n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/abdtab/INR_BEP')  # Add the parent directory containing inr_utils\n",
    "\n",
    "import inr_utils.sampling as sampling\n",
    "from inr_utils.training import make_inr_train_step_function, initialize_state\n",
    "from model_components.inr_modules import CombinedINR  # Import your model class\n",
    "\n",
    "# Create train_step with config components\n",
    "train_step = make_inr_train_step_function(\n",
    "    loss_evaluator=config.loss_function,\n",
    "    sampler=config.sampler,\n",
    "    optimizer=config.optimizer\n",
    ")\n",
    "\n",
    "num_epochs = 1000\n",
    "key = jax.random.PRNGKey(0)\n",
    "\n",
    "# First, instantiate the sampler\n",
    "sampler_class_name, sampler_config = config.sampler\n",
    "sampler = sampling.SoundSampler(**sampler_config)\n",
    "\n",
    "# Create the model instance using CombinedINR and your config\n",
    "model = CombinedINR.from_config(config.model_config)\n",
    "model, state = initialize_state(model)\n",
    "\n",
    "# Initialize optimizer state\n",
    "opt_state = config.optimizer.init(model.parameters())\n",
    "\n",
    "with wandb.init(\n",
    "    project=\"inr-audio\",\n",
    "    config={\n",
    "        \"window_size\": config.sampler[1]['window_size'],\n",
    "        \"batch_size\": config.sampler[1]['batch_size'],\n",
    "        \"learning_rate\": config.optimizer_config['learning_rate'],\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"audio_path\": './data_gt_bach.npy'\n",
    "    }\n",
    ") as run:\n",
    "    for epoch in range(num_epochs):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        model, opt_state, loss, state = train_step(model, opt_state, subkey, state)\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "            wandb.log({\n",
    "                \"loss\": loss,\n",
    "                \"epoch\": epoch,\n",
    "            })\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_57731/1907487116.py\", line 3, in <module>\n",
      "    inr = cju.run_utils.get_model_from_config_and_key(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_jax_utils/run_utils.py\", line 95, in get_model_from_config_and_key\n",
      "    un_initialized_model = get_model_from_config(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/config_realization.py\", line 1185, in get_model_from_config\n",
      "    default_module = load_from_path(name=\"architecture\", path=config[default_module_key]) if default_module_key is not None else None\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/module_loading.py\", line 48, in load_from_path\n",
      "    raise ModuleNotFoundError(f\"Could not find {path=}\")\n",
      "ModuleNotFoundError: Could not find path='./model_components'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find path='./model_components'\n",
      "\n",
      "\n",
      "> \u001b[0;32m/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/module_loading.py\u001b[0m(48)\u001b[0;36mload_from_path\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     46 \u001b[0;31m    \u001b[0;31m# first check if path exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     47 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 48 \u001b[0;31m        \u001b[0;32mraise\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not find {path=}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     50 \u001b[0;31m    \u001b[0;31m# if path exists but is not a .py or .pyc file, see if it is a directory with a __init__.py file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/config_realization.py\u001b[0m(1185)\u001b[0;36mget_model_from_config\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m   1183 \u001b[0;31m    \u001b[0mmissing_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissing_kwargs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1184 \u001b[0;31m    \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_prompt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m-> 1185 \u001b[0;31m    \u001b[0mdefault_module\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"architecture\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdefault_module_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdefault_module_key\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1186 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0madd_model_module_to_architecture_default_module\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m   1187 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# this i just took from inr_example.ipynb to try and get the model working\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False,\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (inr_edu_24)",
   "language": "python",
   "name": "inr_edu_24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
