{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common_dl_utils as cdu\n",
    "from common_dl_utils.config_creation import Config, VariableCollector\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_path = './DIV2K'\n",
    "\n",
    "# for entropy_group in os.listdir(dataset_path):\n",
    "#     print(entropy_group, len(os.listdir(f\"{dataset_path}/{entropy_group}\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = Config()\n",
    "# variable = VariableCollector()\n",
    "\n",
    "# dataset_path = './DIV2K'\n",
    "# start_data_index = 0\n",
    "# end_data_index = 1\n",
    "\n",
    "# def put_all_data_in(dataset_path, start_data_index, end_data_index, top_level_name=\"entropy_class\", low_level_name=\"datapoint\"):\n",
    "#     entropy_groups = []\n",
    "#     for entropy_group in os.listdir(dataset_path):\n",
    "#         group_files = os.listdir(f\"{dataset_path}/{entropy_group}\")[start_data_index:end_data_index]\n",
    "#         entropy_groups.append(variable(*[  # this assumes all entropy groups have the same number of files\n",
    "#             f\"{dataset_path}/{entropy_group}/{file_name}\"\n",
    "#             for file_name in group_files\n",
    "#         ], group=low_level_name))\n",
    "#     return variable(*entropy_groups, group=top_level_name)\n",
    "\n",
    "\n",
    "# put_all_data_in(dataset_path, start_data_index, end_data_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "variable = VariableCollector()\n",
    "\n",
    "dataset_path = './DIV2K'\n",
    "start_data_index = 0\n",
    "end_data_index = 1\n",
    "\n",
    "def put_all_data_in(dataset_path, start_data_index, end_data_index, top_level_name=\"entropy_class\", low_level_name=\"datapoint\"):\n",
    "    entropy_groups = []\n",
    "    for entropy_group in os.listdir(dataset_path):\n",
    "        group_files = os.listdir(f\"{dataset_path}/{entropy_group}\")[start_data_index:end_data_index]\n",
    "        entropy_groups.append(variable(*[  # this assumes all entropy groups have the same number of files\n",
    "            f\"{dataset_path}/{entropy_group}/{file_name}\"\n",
    "            for file_name in group_files\n",
    "        ], group=low_level_name))\n",
    "    return variable(*entropy_groups, group=top_level_name)\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 3\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': variable('inr_layers.SirenLayer', 'inr_layers.SinCardLayer', 'inr_layers.HoscLayer', 'inr_layers.AdaHoscLayer', 'inr_layers.RealWIRE', 'inr_layers.GaussianINRLayer', 'inr_layers.QuadraticLayer', 'inr_layers.MultiQuadraticLayer', 'inr_layers.LaplacianLayer', 'inr_layers.ExpSinLayer', 'inr_layers.FinerLayer', group='method'),\n",
    "        'num_splits': 3,\n",
    "        'activation_kwargs': variable(\n",
    "            {'w0':variable(*tuple(np.linspace(18.33, 31.67, 4)), group=\"hyperparam\")}, #SIREN #  you can nest variables and put complex datastructures in their slots\n",
    "            {'w0':variable(*tuple(np.linspace(18.33, 31.67, 4)), group=\"hyperparam\")}, #SinCard\n",
    "            {'w0': variable(*tuple(np.linspace(3.67, 10.33, 4)), group=\"hyperparam\")}, #HOSC\n",
    "            {'w0': variable(*tuple(np.linspace(3.33, 12.67, 4)), group=\"hyperparam\")}, #AdaHOSC\n",
    "            {'w0': variable(*tuple(np.linspace(8.33, 21.67, 4)), group=\"hyperparam\"), 's0': variable(*tuple(np.linspace(18.33, 31.67, 4)), group=\"hyperparam\")}, #WIRE\n",
    "            {'inverse_scale':variable(*tuple(np.linspace(9.17, 13.83, 4)), group=\"hyperparam\")}, #Gaussian\n",
    "            {'a': variable(*tuple(np.linspace(29.17, 45.83, 4)), group=\"hyperparam\")}, #Quadratic\n",
    "            {'a': variable(*tuple(np.linspace(33.33, 46.67, 4)), group=\"hyperparam\")}, #MultiQuadratic\n",
    "            {'a': variable(*tuple(np.linspace(0.292, 1.258, 4)), group=\"hyperparam\")}, #Laplacian\n",
    "            # {'a': variable(*tuple(np.linspace(0.03, 1., 6)), group=\"hyperparam\"), 'b': variable(*tuple(np.linspace(1.0, 2., 6)), group=\"hyperparam\")}, #SuperGaussian\n",
    "            {'a': variable(*tuple(np.linspace(2.5, 8.5, 4)), group=\"hyperparam\")}, #ExpSin\n",
    "            {'w0': variable(*tuple(np.linspace(17.5, 27.5, 4)), group=\"hyperparam\")}, #Finer\n",
    "            group='method'),  # by specifying a group, you make sure that the values in the group are linked, so SirenLayer wil always go with w0 and GaussianINRLayer with inverse_scale\n",
    "    }),\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  \n",
    "config.trainer_type = 'training.train_inr_scan'#'training.train_inr'  # NB you can use a different training loop, e.g. training.train_inr_scan instead to make it train much faster\n",
    "config.loss_evaluator = 'losses.PointWiseLossEvaluator'\n",
    "config.target_function = 'images.ContinuousImage'\n",
    "config.target_function_config = {\n",
    "    'image': put_all_data_in(dataset_path, start_data_index, end_data_index),\n",
    "    'scale_to_01': False,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation',\n",
    "    'minimal_coordinate': -1.,\n",
    "    'maximal_coordinate':1.,\n",
    "}   \n",
    "config.data_index = None\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.take_grad_of_target_function = False\n",
    "#config.state_update_function = ('auxiliary.ilm_updater', {num_steps = 10000})\n",
    "# config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size':[1356, 2040],# [240, 320], group=\"datapoint\"),#[2040, 1356],\n",
    "    'batch_size': 27120,# 32*240, group=\"datapoint\"),#2000,\n",
    "    'allow_duplicates': False,\n",
    "    'min':-1.\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "# config.optimizer = 'sgd'\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.e-4\n",
    "}\n",
    "config.steps = 100000 #changed from 40000\n",
    "# config.use_wandb = True\n",
    "\n",
    "# # now we want some extra things, like logging, to happen during training\n",
    "# # the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# # The callbacks we want to use can be found in inr_utils.callbacks\n",
    "# config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "# config.after_step_callback_config = {\n",
    "#     'callbacks':[\n",
    "#         ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "#         'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "#         ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "#              {'metric_collector':'metrics.MetricCollector'}\n",
    "#         ),\n",
    "#         'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "#     ],\n",
    "#     'show_logs': False\n",
    "# }\n",
    "\n",
    "# config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\n",
    "\n",
    "# config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "#     'metrics':[\n",
    "#         # ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "#         # # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "#         # ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "#         # ^ compute the MSE with the actual image pixels\n",
    "#         ('metrics.ImageGradMetrics', {\n",
    "#             'grid':variable([2040, 1356],[240, 320], group=\"datapoint\"), \n",
    "#             'batch_size': variable(2040, 2400, group=\"datapoint\"), \n",
    "#             'frequency': 'every_n_batches'\n",
    "#             }),\n",
    "#     ],\n",
    "#     'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "#     'epoch_frequency': 1  # not actually used\n",
    "# }\n",
    "\n",
    "#config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "config.optimizer_state = None  # we're starting from scratch\n",
    "\n",
    "\n",
    "config.components_module = \"./inr_utils/\"\n",
    "config.post_processor_type = \"post_processing.PostProcessor\"\n",
    "config.storage_directory = variable(\"factory_results/Siren_image\", \"factory_results/SinCard_image\", \"factory_results/Hosc_image\", \"factory_results/AdaHosc_image\", \"factory_results/WIRE_image\", \"factory_results/Gaussian_image\", \"factory_results/Quadratic_image\", \"factory_results/MultiQuadratic_image\", \"factory_results/Laplacian_image\", \"factory_results/ExpSin_image\", \"factory_results/Finer_image\", group=\"method\")\n",
    "config.wandb_kwargs = {}\n",
    "config.metrics = [\n",
    "        ('metrics.MSEOnFixedGrid', {\n",
    "            'grid':[1356, 2040],#[240, 320], group=\"datapoint\"), \n",
    "            'batch_size': 8160,# 2400, group=\"datapoint\"), \n",
    "            'frequency': 'every_n_batches'\n",
    "            }),\n",
    "    ]\n",
    "\n",
    "config.entropy_class = variable(1, 2, 3, 4, 5, group=\"entropy_class\")\n",
    "config.method_name = variable(\"Siren_image\", 'SinCard_image', 'Hosc_image', 'AdaHosc_image', 'WIRE_image', 'Gaussian_image', 'Quadratic_image', 'MultiQuadratic_image', 'Laplacian_image', 'ExpSin_image', 'Finer_image', group=\"method\")\n",
    "\n",
    "# config.wandb_group = variable(\"Siren_image\", 'SinCard_image', 'Hosc_image', 'AdaHosc_image', 'WIRE_image', 'Gaussian_image', 'Quadratic_image', 'MultiQuadratic_image', 'Laplacian_image', 'ExpSin_image', 'Finer_image', group=\"method\")\n",
    "config.wandb_entity = \"glukanov2000-eindhoven-university-of-technology\"\n",
    "config.wandb_project = \"images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"./factory_configs/images\"\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "config_files = []\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, Config):\n",
    "            return o.data\n",
    "        return super().default(o)\n",
    "\n",
    "for config_index, config_realization in enumerate(variable.realizations(config)):\n",
    "    group = f\"{config_realization['entropy_class']}-{config_realization['method_name']}\"\n",
    "    config_realization[\"wandb_group\"] = group\n",
    "    target_path = f\"{target_dir}/{group}-{config_index}.yaml\"\n",
    "    with open(target_path, \"w\") as yaml_file:\n",
    "        json.dump(config_realization, yaml_file, cls=MyEncoder)\n",
    "    config_files.append(target_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 11, 'hyperparam': 4, 'datapoint': 1, 'entropy_class': 5}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable._group_to_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "220"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(variable.realizations(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a slurm file that does what we want  NB you'll need to modify th account probably\n",
    "# and the time\n",
    "slurm_directory = \"./factory_slurm/images\"\n",
    "if not os.path.exists(slurm_directory):\n",
    "    os.makedirs(slurm_directory)\n",
    "#chnage account and output directory and maybe conda env name\n",
    "slurm_base = \"\"\"#!/bin/bash\n",
    "#SBATCH --account=tesr82933  \n",
    "#SBATCH --time=16:00:00\n",
    "#SBATCH -p gpu\n",
    "#SBATCH -N 1\n",
    "#SBATCH --tasks-per-node 1\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --output=./factory_output/R-%x.%j.out\n",
    "module load 2023\n",
    "module load Miniconda3/23.5.2-0\n",
    "\n",
    "# >>> conda initialize >>>\n",
    "# !! Contents within this block are managed by 'conda init' !!\n",
    "__conda_setup=\"$('/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\n",
    "if [ $? -eq 0 ]; then\n",
    "    eval \"$__conda_setup\"\n",
    "else\n",
    "    if [ -f \"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/etc/profile.d/conda.sh\" ]; then\n",
    "        . \"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/etc/profile.d/conda.sh\"\n",
    "    else\n",
    "        export PATH=\"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin:$PATH\"\n",
    "    fi\n",
    "fi\n",
    "unset __conda_setup\n",
    "# <<< conda initialize <<<\n",
    "\n",
    "conda init bash\n",
    "conda activate goshko_bep  # conda environment name\n",
    "\n",
    "wandblogin=\"$(< ./wandb.login)\"  # password stored in a file, don't add this file to your git repo!\n",
    "wandb login \"$wandblogin\"\n",
    "\n",
    "\n",
    "echo 'Starting new experiment!';\n",
    "\"\"\"\n",
    "\n",
    "for config_file in config_files:\n",
    "    slurm_script = slurm_base + f\"\\npython run_single.py --config={config_file}\"\n",
    "    slurm_file_name = (config_file.split(\"/\")[-1].split(\".\")[0])+\".bash\"\n",
    "    with open(f\"{slurm_directory}/{slurm_file_name}\", \"w\") as slurm_file:\n",
    "        slurm_file.write(slurm_script)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
