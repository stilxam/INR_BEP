{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:23:08.680233Z",
     "start_time": "2025-02-15T17:23:08.539206Z"
    }
   },
   "outputs": [],
   "source": [
    "import common_dl_utils as cdu\n",
    "from common_dl_utils.config_creation import Config, VariableCollector\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:23:09.119526Z",
     "start_time": "2025-02-15T17:23:09.101751Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "variable = VariableCollector()\n",
    "\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 1\n",
    "config.model_config.out_size = 1\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': variable('inr_layers.SirenLayer', 'inr_layers.SinCardLayer', 'inr_layers.HoscLayer', 'inr_layers.AdaHoscLayer', 'inr_layers.RealWIRE', 'inr_layers.GaussianINRLayer', 'inr_layers.QuadraticLayer', 'inr_layers.MultiQuadraticLayer', 'inr_layers.LaplacianLayer', 'inr_layers.ExpSinLayer', 'inr_layers.FinerLayer', group='method'),\n",
    "        'num_splits': 3,\n",
    "        'activation_kwargs': variable(\n",
    "            {'w0':variable(*tuple(np.linspace(15., 35., 6)), group=\"hyperparam\")}, #SIREN #  you can nest variables and put complex datastructures in their slots\n",
    "            {'w0':variable(*tuple(np.linspace(15., 35., 6)), group=\"hyperparam\")}, #SinCard\n",
    "            {'w0': variable(*tuple(np.linspace(2., 12., 6)), group=\"hyperparam\")}, #HOSC\n",
    "            {'w0': variable(*tuple(np.linspace(1., 15., 6)), group=\"hyperparam\")}, #AdaHOSC\n",
    "            {'w0': variable(*tuple(np.linspace(5., 25., 6)), group=\"hyperparam\"), 's0': variable(*tuple(np.linspace(15., 35., 6)), group=\"hyperparam\")}, #WIRE\n",
    "            {'inverse_scale':variable(*tuple(np.linspace(8., 15., 6)), group=\"hyperparam\")}, #Gaussian\n",
    "            {'a': variable(*tuple(np.linspace(25., 50., 6)), group=\"hyperparam\")}, #Quadratic\n",
    "            {'a': variable(*tuple(np.linspace(30., 50., 6)), group=\"hyperparam\")}, #MultiQuadratic\n",
    "            {'a': variable(*tuple(np.linspace(0.05, 1.5, 6)), group=\"hyperparam\")}, #Laplacian\n",
    "            {'a': variable(*tuple(np.linspace(1., 10., 6)), group=\"hyperparam\")}, #ExpSin\n",
    "            {'w0': variable(*tuple(np.linspace(15., 30., 6)), group=\"hyperparam\")}, #Finer\n",
    "            group='method'),  ## by specifying a group, you make sure that the values in the group are linked, so SirenLayer wil always go with w0 and GaussianINRLayer with inverse_scale\n",
    "        'positional_encoding_layer': ('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies': 10}),\n",
    "    }),\n",
    "\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  \n",
    "config.trainer_type = 'training.train_inr_scan'#'training.train_inr'  # NB you can use a different training loop, e.g. training.train_inr_scan instead to make it train much faster\n",
    "config.loss_evaluator = 'losses.SoundLossEvaluator'\n",
    "config.loss_function = 'losses.SoundLossEvaluator'\n",
    "config.loss_evaluator_config = {\n",
    "    'time_domain_weight': 1.0,\n",
    "    'frequency_domain_weight': 0.000001\n",
    "}\n",
    "\n",
    "# config.target_function = 'images.ContinuousImage'\n",
    "# config.target_function_config = {\n",
    "#     'image': variable('./example_data/Audio/Audio_entropy_5p_25p/hiphop.00080.wav', \"example_data/Audio/Audio_entropy_bottom_5p/rock.00086.wav\", group=\"datapoint\"),#'./example_data/gray_parrot_grads_scaled.npy',\n",
    "#     'scale_to_01': False,\n",
    "#     'interpolation_method': 'images.make_piece_wise_constant_interpolation',\n",
    "#     'minimal_coordinate': -1.,\n",
    "#     'maximal_coordinate':1.,\n",
    "# }   \n",
    "config.sampler = ('sampling.SoundSampler', {\n",
    "    'window_size': 256,\n",
    "    'batch_size': 32,\n",
    "    'allow_pickle': True,\n",
    "    'fragment_length': None,\n",
    "    'sound_fragment': \"./example_data/data_gt_bach.npy\",\n",
    "})\n",
    "\n",
    "config.data_index = None\n",
    "# config.loss_function = 'losses.scaled_mse_loss'\n",
    "# config.take_grad_of_target_function = False\n",
    "#config.state_update_function = ('auxiliary.ilm_updater', {num_steps = 10000})\n",
    "# config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "# config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "#     'size': variable([2040, 1356], [240, 320], group=\"datapoint\"),#[2040, 1356],\n",
    "#     'batch_size': variable(27120, 32*240, group=\"datapoint\"),#2000,\n",
    "#     'allow_duplicates': False,\n",
    "#     'min':-1.\n",
    "# })\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "# config.optimizer = 'sgd'\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.e-4,\n",
    "    'b1': 0.8,\n",
    "    'b2': 0.999999\n",
    "}\n",
    "config.steps = 20000 #changed from 40000\n",
    "# config.use_wandb = True\n",
    "\n",
    "\n",
    "#config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "config.optimizer_state = None  # we're starting from scratch\n",
    "\n",
    "\n",
    "config.components_module = \"./inr_utils/\"\n",
    "config.post_processor_type = \"post_processing.PostProcessor\"\n",
    "config.storage_directory = \"results/local_test\"\n",
    "# variable(\"factory_results/Siren_audio_grad\", \"factory_results/SinCard_audio_grad\", \"factory_results/Hosc_audio_grad\", \"factory_results/AdaHosc_audio_grad\", \"factory_results/WIRE_audio_grad\", \"factory_results/Gaussian_audio_grad\", \"factory_results/Quadratic_audio_grad\", \"factory_results/MultiQuadratic_audio_grad\", \"factory_results/Laplacian_audio_grad\", \"factory_results/ExpSin_audio_grad\", \"factory_results/Finer_audio_grad\", group=\"method\")\n",
    "config.wandb_kwargs = {}\n",
    "config.metrics = [\n",
    "    (\"metrics.AudioMetricsOnGrid\", {\n",
    "        'target_audio': \"./example_data/data_gt_bach.npy\",\n",
    "        'grid_size': None,\n",
    "        'batch_size': 1024,  # This will be automatically adjusted if needed\n",
    "        'sr': 16000,\n",
    "        'frequency': 'every_n_batches',\n",
    "        'save_path': './results/Audio/Audio_reconstructed_bach.wav'\n",
    "    }\n",
    "     ),\n",
    "]\n",
    "config.batch_frequency = 100\n",
    "\n",
    "\n",
    "    # 'metrics': [\n",
    "    #     ('metrics.AudioMetricsOnGrid', {\n",
    "    #         'target_audio': \"./example_data/data_gt_bach.npy\",\n",
    "    #         'grid_size': grsz,\n",
    "    #         'batch_size': 1024,  # This will be automatically adjusted if needed\n",
    "    #         'sr': 16000,\n",
    "    #         'frequency': 'every_n_batches',\n",
    "    #         'save_path': variable(\"factory_results/Siren_audio_grad\", \"factory_results/SinCard_audio_grad\", \"factory_results/Hosc_audio_grad\", \"factory_results/AdaHosc_audio_grad\", \"factory_results/WIRE_audio_grad\", \"factory_results/Gaussian_audio_grad\", \"factory_results/Quadratic_audio_grad\", \"factory_results/MultiQuadratic_audio_grad\", \"factory_results/Laplacian_audio_grad\", \"factory_results/ExpSin_audio_grad\", \"factory_results/Finer_audio_grad\", group=\"method\")\n",
    "    #     })\n",
    "#     ],\n",
    "#     'batch_frequency': 100,\n",
    "#     'epoch_frequency': 1\n",
    "# }\n",
    "# ]\n",
    "\n",
    "config.wandb_group = variable(\"Siren_audio_grad\", 'SinCard_audio_grad', 'Hosc_audio_grad', 'AdaHosc_audio_grad', 'WIRE_audio_grad', 'Gaussian_audio_grad', 'Quadratic_audio_grad', 'MultiQuadratic_audio_grad', 'Laplacian_audio_grad','ExpSin_audio_grad', 'Finer_audio_grad', group=\"method\")\n",
    "\n",
    "config.wandb_entity = \"abdtab-tue\"\n",
    "config.wandb_project = \"audi_reconstruction\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-15T17:23:10.270168Z",
     "start_time": "2025-02-15T17:23:10.261552Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 11, 'hyperparam': 6}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable._group_to_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(variable.realizations(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"./factory_configs/test\"\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "config_files = []\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, Config):\n",
    "            return o.data\n",
    "        return super().default(o)\n",
    "\n",
    "for config_index, config_realization in enumerate(variable.realizations(config)):\n",
    "    group = config_realization[\"wandb_group\"]\n",
    "    target_path = f\"{target_dir}/{group}-{config_index}.yaml\"\n",
    "    with open(target_path, \"w\") as yaml_file:\n",
    "        json.dump(config_realization, yaml_file, cls=MyEncoder)\n",
    "    config_files.append(target_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a slurm file that does what we want  NB you'll need to modify th account probably\n",
    "# and the time\n",
    "slurm_directory = \"./factory_slurm/test\"\n",
    "if not os.path.exists(slurm_directory):\n",
    "    os.makedirs(slurm_directory)\n",
    "#chnage account and output directory and maybe conda env name\n",
    "slurm_base = \"\"\"#!/bin/bash\n",
    "#SBATCH --account=tesr82932\n",
    "#SBATCH --time=0:15:00\n",
    "#SBATCH -p gpu\n",
    "#SBATCH -N 1\n",
    "#SBATCH --tasks-per-node 1\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --output=./factory_output/R-%x.%j.out\n",
    "module load 2023\n",
    "module load Miniconda3/23.5.2-0\n",
    "\n",
    "# >>> conda initialize >>>\n",
    "# !! Contents within this block are managed by 'conda init' !!\n",
    "__conda_setup=\"$('/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\n",
    "if [ $? -eq 0 ]; then\n",
    "    eval \"$__conda_setup\"\n",
    "else\n",
    "    if [ -f \"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/etc/profile.d/conda.sh\" ]; then\n",
    "        . \"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/etc/profile.d/conda.sh\"\n",
    "    else\n",
    "        export PATH=\"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin:$PATH\"\n",
    "    fi\n",
    "fi\n",
    "unset __conda_setup\n",
    "# <<< conda initialize <<<\n",
    "\n",
    "conda init bash\n",
    "conda activate snel_bep  # conda environment name\n",
    "\n",
    "wandblogin=\"$(< ./wandb.login)\"  # password stored in a file, don't add this file to your git repo!\n",
    "wandb login \"$wandblogin\"\n",
    "\n",
    "\n",
    "echo 'Starting new experiment!';\n",
    "\"\"\"\n",
    "\n",
    "for config_file in config_files:\n",
    "    slurm_script = slurm_base + f\"\\npython run_single.py --config={config_file}\"\n",
    "    slurm_file_name = (config_file.split(\"/\")[-1].split(\".\")[0])+\".bash\"\n",
    "    with open(f\"{slurm_directory}/{slurm_file_name}\", \"w\") as slurm_file:\n",
    "        slurm_file.write(slurm_script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
