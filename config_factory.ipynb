{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common_dl_utils as cdu\n",
    "from common_dl_utils.config_creation import Config, VariableCollector\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(15.0),\n",
       " np.float64(18.88888888888889),\n",
       " np.float64(22.77777777777778),\n",
       " np.float64(26.666666666666664),\n",
       " np.float64(30.555555555555557),\n",
       " np.float64(34.44444444444444),\n",
       " np.float64(38.33333333333333),\n",
       " np.float64(42.22222222222222),\n",
       " np.float64(46.111111111111114),\n",
       " np.float64(50.0))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(np.linspace(15., 50., 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "variable = VariableCollector()\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 1\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': variable('inr_layers.SirenLayer', 'inr_layers.GaussianINRLayer', 'inr_layers.QuadraticLayer', group='method'),\n",
    "        'num_splits': 3,\n",
    "        'activation_kwargs': variable(\n",
    "            {'w0':variable(15., 18., 20., 25., 30., 35., group=\"hyperparam\")},  #  you can nest variables and put complex datastructures in their slots\n",
    "            {'inverse_scale':variable(9., 10., 11., 12., 13., 14., group=\"hyperparam\")}, \n",
    "            {'a': variable(*tuple(np.linspace(20., 50., 6)), group=\"hyperparam\")},\n",
    "            group='method'),  # by specifying a group, you make sure that the values in the group are linked, so SirenLayer wil always go with w0 and GaussianINRLayer with inverse_scale\n",
    "    }),\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  \n",
    "config.trainer_type = 'training.train_inr_scan'#'training.train_inr'  # NB you can use a different training loop, e.g. training.train_inr_scan instead to make it train much faster\n",
    "config.loss_evaluator = 'losses.PointWiseGradLossEvaluator'\n",
    "config.target_function = 'images.ContinuousImage'\n",
    "config.target_function_config = {\n",
    "    'image': variable('./example_data/gray_parrot_grads_scaled.npy', \"example_data/gray_flower_grads_scaled.npy\", group=\"datapoint\"),#'./example_data/gray_parrot_grads_scaled.npy',\n",
    "    'scale_to_01': False,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation',\n",
    "    'minimal_coordinate': -1.,\n",
    "    'maximal_coordinate':1.,\n",
    "}   \n",
    "config.data_index = None\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.take_grad_of_target_function = False\n",
    "#config.state_update_function = ('auxiliary.ilm_updater', {num_steps = 10000})\n",
    "# config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': variable([2040, 1356], [240, 320], group=\"datapoint\"),#[2040, 1356],\n",
    "    'batch_size': variable(27120, 32*240, group=\"datapoint\"),#2000,\n",
    "    'allow_duplicates': False,\n",
    "    'min':-1.\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "# config.optimizer = 'sgd'\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.e-4\n",
    "}\n",
    "config.steps = 160000 #changed from 40000\n",
    "# config.use_wandb = True\n",
    "\n",
    "# # now we want some extra things, like logging, to happen during training\n",
    "# # the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# # The callbacks we want to use can be found in inr_utils.callbacks\n",
    "# config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "# config.after_step_callback_config = {\n",
    "#     'callbacks':[\n",
    "#         ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "#         'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "#         ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "#              {'metric_collector':'metrics.MetricCollector'}\n",
    "#         ),\n",
    "#         'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "#     ],\n",
    "#     'show_logs': False\n",
    "# }\n",
    "\n",
    "# config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\n",
    "\n",
    "# config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "#     'metrics':[\n",
    "#         # ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "#         # # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "#         # ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "#         # ^ compute the MSE with the actual image pixels\n",
    "#         ('metrics.ImageGradMetrics', {\n",
    "#             'grid':variable([2040, 1356],[240, 320], group=\"datapoint\"), \n",
    "#             'batch_size': variable(2040, 2400, group=\"datapoint\"), \n",
    "#             'frequency': 'every_n_batches'\n",
    "#             }),\n",
    "#     ],\n",
    "#     'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "#     'epoch_frequency': 1  # not actually used\n",
    "# }\n",
    "\n",
    "#config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "config.optimizer_state = None  # we're starting from scratch\n",
    "\n",
    "\n",
    "config.components_module = \"./inr_utils/\"\n",
    "config.post_processor_type = \"post_processing.PostProcessor\"\n",
    "config.storage_directory = variable(\"factory_results/Siren_example_grad\", \"factory_results/Gaussian_example_grad\", \"factory_results/Quadratic_example_grad\", group=\"method\")\n",
    "config.wandb_kwargs = {}\n",
    "config.metrics = [\n",
    "        ('metrics.ImageGradMetrics', {\n",
    "            'grid':variable([2040, 1356],[240, 320], group=\"datapoint\"), \n",
    "            'batch_size': variable(2040, 2400, group=\"datapoint\"), \n",
    "            'frequency': 'every_n_batches'\n",
    "            }),\n",
    "    ]\n",
    "\n",
    "config.wandb_group = variable(\"Siren_example_grad\", \"Gaussian_example_grad\", \"Quadratic_example_grad\", group=\"method\")\n",
    "config.wandb_entity = \"abdtab-tue\"\n",
    "config.wandb_project = \"inr_edu_24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 3, 'hyperparam': 6, 'datapoint': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable._group_to_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(variable.realizations(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"./factory_configs/test\"\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "config_files = []\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, Config):\n",
    "            return o.data\n",
    "        return super().default(o)\n",
    "\n",
    "for config_index, config_realization in enumerate(variable.realizations(config)):\n",
    "    group = config_realization[\"wandb_group\"]\n",
    "    target_path = f\"{target_dir}/{group}-{config_index}.yaml\"\n",
    "    with open(target_path, \"w\") as yaml_file:\n",
    "        json.dump(config_realization, yaml_file, cls=MyEncoder)\n",
    "    config_files.append(target_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a slurm file that does what we want  NB you'll need to modify th account probably\n",
    "# and the time\n",
    "slurm_directory = \"./factory_slurm/test\"\n",
    "if not os.path.exists(slurm_directory):\n",
    "    os.makedirs(slurm_directory)\n",
    "#chnage account and output directory and maybe conda env name\n",
    "slurm_base = \"\"\"#!/bin/bash\n",
    "#SBATCH --account=tesr82932\n",
    "#SBATCH --time=0:10:00\n",
    "#SBATCH -p gpu\n",
    "#SBATCH -N 1\n",
    "#SBATCH --tasks-per-node 1\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --output=./factory_output/R-%x.%j.out\n",
    "module load 2023\n",
    "module load Miniconda3/23.5.2-0\n",
    "\n",
    "# >>> conda initialize >>>\n",
    "# !! Contents within this block are managed by 'conda init' !!\n",
    "__conda_setup=\"$('/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\n",
    "if [ $? -eq 0 ]; then\n",
    "    eval \"$__conda_setup\"\n",
    "else\n",
    "    if [ -f \"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/etc/profile.d/conda.sh\" ]; then\n",
    "        . \"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/etc/profile.d/conda.sh\"\n",
    "    else\n",
    "        export PATH=\"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin:$PATH\"\n",
    "    fi\n",
    "fi\n",
    "unset __conda_setup\n",
    "# <<< conda initialize <<<\n",
    "\n",
    "conda init bash\n",
    "conda activate snel_bep  # conda environment name\n",
    "\n",
    "wandblogin=\"$(< ./wandb.login)\"  # password stored in a file, don't add this file to your git repo!\n",
    "wandb login \"$wandblogin\"\n",
    "\n",
    "\n",
    "echo 'Starting new experiment!';\n",
    "\"\"\"\n",
    "\n",
    "for config_file in config_files:\n",
    "    slurm_script = slurm_base + f\"\\npython run_single.py --config={config_file}\"\n",
    "    slurm_file_name = (config_file.split(\"/\")[-1].split(\".\")[0])+\".bash\"\n",
    "    with open(f\"{slurm_directory}/{slurm_file_name}\", \"w\") as slurm_file:\n",
    "        slurm_file.write(slurm_script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
