{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import common_dl_utils as cdu\n",
    "from common_dl_utils.config_creation import Config, VariableCollector\n",
    "import json\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "variable = VariableCollector()\n",
    "\n",
    "\n",
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 1\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': variable('inr_layers.SirenLayer', 'inr_layers.GaussianINRLayer', 'inr_layers.QuadraticLayer', group='method'),\n",
    "        'num_splits': 3,\n",
    "        'activation_kwargs': variable(\n",
    "            {'w0':variable(10., 15., 20., 25., 30., 35., group=\"hyperparam\")},  #  you can nest variables and put complex datastructures in their slots\n",
    "            {'inverse_scale':variable(10., 15., 20., 25., 30., 35., group=\"hyperparam\")},\n",
    "            {'a': variable(10., 15., 20., 25., 30., 35, group=\"hyperparam\")},\n",
    "            group='method'),  # by specifying a group, you make sure that the values in the group are linked, so SirenLayer wil always go with w0 and GaussianINRLayer with inverse_scale\n",
    "    }),\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "\n",
    "config.dataloader = 'sdf.SDFDataLoader'\n",
    "\n",
    "config.dataloader_config = {\n",
    "    \"path\": \"example_data/xyzrgb_statuette.ply\",\n",
    "    \"batch_size\": 200000,\n",
    "    \"keep_aspect_ratio\":True\n",
    "}\n",
    "\n",
    "config.num_cycles = 50000\n",
    "config.steps_per_cycle = 200\n",
    "\n",
    "\n",
    "config.loss_evaluator = \"losses.SDFLossEvaluator\"\n",
    "\n",
    "\n",
    "config.target_function = 'sdf.SDFDataLoader' #see when config. losseval\n",
    "config.target_function_config = {\n",
    "    \"path\": \"example_data/xyzrgb_statuette.ply\",\n",
    "    \"batch_size\": 200000,\n",
    "    \"keep_aspect_ratio\":True\n",
    "}\n",
    "\n",
    "\n",
    "config.optimizer = 'training.OptimizerFactory.single_optimizer'#'adamw'  # we'll have to add optax to the additional default modules later\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "# config.optimizer = 'sgd'\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.e-4\n",
    "}\n",
    "config.num_cycles = 50000\n",
    "config.steps_per_cycle = 200\n",
    "\n",
    "\n",
    "\n",
    "config.components_module = \"./inr_utils/\"\n",
    "config.post_processor_type = \"post_processing.PostProcessor\"\n",
    "config.storage_directory = variable(\"factory_results/Siren_sdf\", \"factory_results/Gaussian_sdf\", \"factory_results/Quadratic_sdf\", group=\"method\")\n",
    "\n",
    "config.after_cycle_callback = 'callbacks.ComposedCallback'\n",
    "config.after_cycle_callback_config = {\n",
    "    'callbacks':[\n",
    "        ('callbacks.print_loss', {'after_every':1}),  # only print the loss every 400th step\n",
    "        'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "        ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "             {'metric_collector':'metrics.MetricCollector'}\n",
    "        ),\n",
    "        'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "    ],\n",
    "    'show_logs': False\n",
    "}\n",
    "config.wandb_group = variable(\"Siren_example_grad\", \"Gaussian_example_grad\", \"Quadratic_example_grad\", group=\"method\")\n",
    "config.wandb_entity = \"abdtab-tue\"\n",
    "config.wandb_project = \"inr_edu_24\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'method': 3, 'hyperparam': 6, 'datapoint': 2}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable._group_to_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(variable.realizations(config)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = \"./factory_configs/test\"\n",
    "if not os.path.exists(target_dir):\n",
    "    os.makedirs(target_dir)\n",
    "\n",
    "config_files = []\n",
    "\n",
    "class MyEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, Config):\n",
    "            return o.data\n",
    "        return super().default(o)\n",
    "\n",
    "for config_index, config_realization in enumerate(variable.realizations(config)):\n",
    "    group = config_realization[\"wandb_group\"]\n",
    "    target_path = f\"{target_dir}/{group}-{config_index}.yaml\"\n",
    "    with open(target_path, \"w\") as yaml_file:\n",
    "        json.dump(config_realization, yaml_file, cls=MyEncoder)\n",
    "    config_files.append(target_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now create a slurm file that does what we want  NB you'll need to modify th account probably\n",
    "# and the time\n",
    "slurm_directory = \"./factory_slurm/test\"\n",
    "if not os.path.exists(slurm_directory):\n",
    "    os.makedirs(slurm_directory)\n",
    "#chnage account and output directory and maybe conda env name\n",
    "slurm_base = \"\"\"#!/bin/bash\n",
    "#SBATCH --account=tesr82932\n",
    "#SBATCH --time=0:10:00\n",
    "#SBATCH -p gpu\n",
    "#SBATCH -N 1\n",
    "#SBATCH --tasks-per-node 1\n",
    "#SBATCH --gpus=1\n",
    "#SBATCH --output=./factory_output/R-%x.%j.out\n",
    "module load 2023\n",
    "module load Miniconda3/23.5.2-0\n",
    "\n",
    "# >>> conda initialize >>>\n",
    "# !! Contents within this block are managed by 'conda init' !!\n",
    "__conda_setup=\"$('/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin/conda' 'shell.bash' 'hook' 2> /dev/null)\"\n",
    "if [ $? -eq 0 ]; then\n",
    "    eval \"$__conda_setup\"\n",
    "else\n",
    "    if [ -f \"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/etc/profile.d/conda.sh\" ]; then\n",
    "        . \"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/etc/profile.d/conda.sh\"\n",
    "    else\n",
    "        export PATH=\"/sw/arch/RHEL8/EB_production/2023/software/Miniconda3/23.5.2-0/bin:$PATH\"\n",
    "    fi\n",
    "fi\n",
    "unset __conda_setup\n",
    "# <<< conda initialize <<<\n",
    "\n",
    "conda init bash\n",
    "conda activate snel_bep  # conda environment name\n",
    "\n",
    "wandblogin=\"$(< ./wandb.login)\"  # password stored in a file, don't add this file to your git repo!\n",
    "wandb login \"$wandblogin\"\n",
    "\n",
    "\n",
    "echo 'Starting new experiment!';\n",
    "\"\"\"\n",
    "\n",
    "for config_file in config_files:\n",
    "    slurm_script = slurm_base + f\"\\npython run_single.py --config={config_file}\"\n",
    "    slurm_file_name = (config_file.split(\"/\")[-1].split(\".\")[0])+\".bash\"\n",
    "    with open(f\"{slurm_directory}/{slurm_file_name}\", \"w\") as slurm_file:\n",
    "        slurm_file.write(slurm_script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
