{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of training an INR locally\n",
    "This notebook provides an example of how to create an INR and train it locally using the tools in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon-martinus-koop\u001b[0m (\u001b[33mnld\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2024-11-07 19:03:13.390398: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\n",
    "\n",
    "To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \"CombinedINR\"` instead of `config[\"model_type\"] = \"CombinedINR\"`). You can also just use a dictionary instead.\n",
    "\n",
    "Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\n",
    "\n",
    "Doing this in a config instead of hard coded might seem like extra work, but consider this:\n",
    "1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \n",
    "   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\n",
    "2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\n",
    "   and then run an experiment specified by that config on any machine you want, e.g. on Snellius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 3\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.new_from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 1,\n",
    "        'use_complex': False,\n",
    "        'activation_kwargs': {'w0': 30.},\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        'positional_encoding_layer': ('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies': 10}),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "config.trainer_type = 'training.train_inr'\n",
    "config.target_function = 'images.ContinuousImage'\n",
    "config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation'\n",
    "}\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "config.steps = 40000\n",
    "config.use_wandb = True\n",
    "\n",
    "# now we want some extra things, like logging, to happen during training\n",
    "# the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# The callbacks we want to use can be found in inr_utils.callbacks\n",
    "config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "config.after_step_callback_config = {\n",
    "    'callbacks':[\n",
    "        ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "        'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "        ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "             {'metric_collector':'metrics.MetricCollector'}\n",
    "        ),\n",
    "        'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "    ],\n",
    "    'show_logs': False\n",
    "}\n",
    "\n",
    "config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "    'metrics':[\n",
    "        ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "        # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "        ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "        # ^ compute the MSE with the actual image pixels\n",
    "    ],\n",
    "    'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "    'epoch_frequency': 1  # not actually used\n",
    "}\n",
    "\n",
    "config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "config.optimizer_state = None  # we're starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      input_layer=ClassicalPositionalEncoding(_embedding_matrix=f32[10]),\n",
       "      hidden_layers=[\n",
       "        SirenLayer(\n",
       "          weights=f32[256,40],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 30.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 30.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 30.0}\n",
       "        )\n",
       "      ],\n",
       "      output_layer=Linear(\n",
       "        weights=f32[3,256],\n",
       "        biases=f32[3],\n",
       "        activation_kwargs={}\n",
       "      ),\n",
       "      post_processor=<function real_part>\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.6602258 ,  0.24299857, -0.19982576], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check that it works properly\n",
    "inr(jnp.zeros(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/simon/Documents/INR_BEP/wandb/run-20241107_190315-39cyc5a0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nld/inr_edu_24/runs/39cyc5a0' target=\"_blank\">jumping-dragon-22</a></strong> to <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">https://wandb.ai/nld/inr_edu_24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nld/inr_edu_24/runs/39cyc5a0' target=\"_blank\">https://wandb.ai/nld/inr_edu_24/runs/39cyc5a0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E1107 19:03:20.615464   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.615489   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.615497   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.615502   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.615508   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.615513   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.615518   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.615522   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.615524   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.615529   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.615533: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.616447   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.616454   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.616461   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.616464   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.616469   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.616473   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.616477   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.616480   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.616482   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.616485   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.616488: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.617433   19821 buffer_comparator.cc:157] Difference at 3759: -1.45641, expected -1.0845\n",
      "E1107 19:03:20.617443   19821 buffer_comparator.cc:157] Difference at 3828: 0.388306, expected 0.094574\n",
      "E1107 19:03:20.617449   19821 buffer_comparator.cc:157] Difference at 5869: -0.896627, expected -0.477135\n",
      "E1107 19:03:20.617453   19821 buffer_comparator.cc:157] Difference at 6830: -1.3205, expected -0.987137\n",
      "E1107 19:03:20.617457   19821 buffer_comparator.cc:157] Difference at 8402: -1.15347, expected -0.789398\n",
      "E1107 19:03:20.617461   19821 buffer_comparator.cc:157] Difference at 9749: 1.2948, expected 0.848717\n",
      "E1107 19:03:20.617464   19821 buffer_comparator.cc:157] Difference at 10740: 1.55066, expected 1.20514\n",
      "E1107 19:03:20.617467   19821 buffer_comparator.cc:157] Difference at 11469: -4.98654, expected -5.80765\n",
      "E1107 19:03:20.617469   19821 buffer_comparator.cc:157] Difference at 11640: 0.261467, expected 0.419174\n",
      "E1107 19:03:20.617472   19821 buffer_comparator.cc:157] Difference at 12797: 1.4749, expected 1.92444\n",
      "2024-11-07 19:03:20.617476: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.618329   19821 buffer_comparator.cc:157] Difference at 3759: -1.45673, expected -1.0845\n",
      "E1107 19:03:20.618338   19821 buffer_comparator.cc:157] Difference at 3828: 0.38826, expected 0.094574\n",
      "E1107 19:03:20.618344   19821 buffer_comparator.cc:157] Difference at 5869: -0.896759, expected -0.477135\n",
      "E1107 19:03:20.618348   19821 buffer_comparator.cc:157] Difference at 6830: -1.3206, expected -0.987137\n",
      "E1107 19:03:20.618353   19821 buffer_comparator.cc:157] Difference at 8402: -1.15332, expected -0.789398\n",
      "E1107 19:03:20.618357   19821 buffer_comparator.cc:157] Difference at 9749: 1.29502, expected 0.848717\n",
      "E1107 19:03:20.618360   19821 buffer_comparator.cc:157] Difference at 10740: 1.55032, expected 1.20514\n",
      "E1107 19:03:20.618363   19821 buffer_comparator.cc:157] Difference at 11469: -4.98656, expected -5.80765\n",
      "E1107 19:03:20.618365   19821 buffer_comparator.cc:157] Difference at 11640: 0.261398, expected 0.419174\n",
      "E1107 19:03:20.618368   19821 buffer_comparator.cc:157] Difference at 12797: 1.47525, expected 1.92444\n",
      "2024-11-07 19:03:20.618370: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.619528   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.619540   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.619546   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.619549   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.619554   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.619557   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.619561   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.619563   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.619565   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.619569   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.619571: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.620347   19821 buffer_comparator.cc:157] Difference at 3759: -1.45641, expected -1.0845\n",
      "E1107 19:03:20.620357   19821 buffer_comparator.cc:157] Difference at 3828: 0.388306, expected 0.094574\n",
      "E1107 19:03:20.620363   19821 buffer_comparator.cc:157] Difference at 5869: -0.896627, expected -0.477135\n",
      "E1107 19:03:20.620366   19821 buffer_comparator.cc:157] Difference at 6830: -1.3205, expected -0.987137\n",
      "E1107 19:03:20.620370   19821 buffer_comparator.cc:157] Difference at 8402: -1.15347, expected -0.789398\n",
      "E1107 19:03:20.620374   19821 buffer_comparator.cc:157] Difference at 9749: 1.2948, expected 0.848717\n",
      "E1107 19:03:20.620378   19821 buffer_comparator.cc:157] Difference at 10740: 1.55066, expected 1.20514\n",
      "E1107 19:03:20.620380   19821 buffer_comparator.cc:157] Difference at 11469: -4.98654, expected -5.80765\n",
      "E1107 19:03:20.620382   19821 buffer_comparator.cc:157] Difference at 11640: 0.261467, expected 0.419174\n",
      "E1107 19:03:20.620386   19821 buffer_comparator.cc:157] Difference at 12797: 1.4749, expected 1.92444\n",
      "2024-11-07 19:03:20.620388: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.621193   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.621202   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.621208   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.621212   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.621216   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.621220   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.621224   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.621226   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.621228   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.621232   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.621234: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.622100   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.622111   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.622117   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.622120   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.622125   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.622129   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.622132   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.622135   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.622136   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.622140   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.622142: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.623165   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.623175   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.623181   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.623184   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.623189   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.623193   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.623196   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.623199   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.623201   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.623204   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.623206: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.624455   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.624464   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.624470   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.624473   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.624477   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.624481   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.624485   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.624488   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.624489   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.624493   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.624495: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.625461   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.625471   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.625477   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.625480   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.625485   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.625489   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.625492   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.625495   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.625496   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.625500   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.625502: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.626608   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.626618   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.626624   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.626628   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.626633   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.626637   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.626640   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.626643   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.626644   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.626648   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.626650: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.627717   19821 buffer_comparator.cc:157] Difference at 3759: -1.45685, expected -1.0845\n",
      "E1107 19:03:20.627727   19821 buffer_comparator.cc:157] Difference at 3828: 0.388123, expected 0.094574\n",
      "E1107 19:03:20.627733   19821 buffer_comparator.cc:157] Difference at 5869: -0.897003, expected -0.477135\n",
      "E1107 19:03:20.627736   19821 buffer_comparator.cc:157] Difference at 6830: -1.32104, expected -0.987137\n",
      "E1107 19:03:20.627741   19821 buffer_comparator.cc:157] Difference at 8402: -1.15378, expected -0.789398\n",
      "E1107 19:03:20.627745   19821 buffer_comparator.cc:157] Difference at 9749: 1.29523, expected 0.848717\n",
      "E1107 19:03:20.627748   19821 buffer_comparator.cc:157] Difference at 10740: 1.54965, expected 1.20514\n",
      "E1107 19:03:20.627751   19821 buffer_comparator.cc:157] Difference at 11469: -4.98602, expected -5.80765\n",
      "E1107 19:03:20.627752   19821 buffer_comparator.cc:157] Difference at 11640: 0.261169, expected 0.419174\n",
      "E1107 19:03:20.627756   19821 buffer_comparator.cc:157] Difference at 12797: 1.47597, expected 1.92444\n",
      "2024-11-07 19:03:20.627758: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E1107 19:03:20.629370   19821 buffer_comparator.cc:157] Difference at 3759: -1.45812, expected -1.0845\n",
      "E1107 19:03:20.629382   19821 buffer_comparator.cc:157] Difference at 3828: 0.387787, expected 0.094574\n",
      "E1107 19:03:20.629388   19821 buffer_comparator.cc:157] Difference at 5869: -0.897485, expected -0.477135\n",
      "E1107 19:03:20.629392   19821 buffer_comparator.cc:157] Difference at 6830: -1.32157, expected -0.987137\n",
      "E1107 19:03:20.629396   19821 buffer_comparator.cc:157] Difference at 8402: -1.15402, expected -0.789398\n",
      "E1107 19:03:20.629400   19821 buffer_comparator.cc:157] Difference at 9749: 1.29544, expected 0.848717\n",
      "E1107 19:03:20.629403   19821 buffer_comparator.cc:157] Difference at 10740: 1.54895, expected 1.20514\n",
      "E1107 19:03:20.629406   19821 buffer_comparator.cc:157] Difference at 11469: -4.98534, expected -5.80765\n",
      "E1107 19:03:20.629408   19821 buffer_comparator.cc:157] Difference at 11640: 0.26088, expected 0.419174\n",
      "E1107 19:03:20.629411   19821 buffer_comparator.cc:157] Difference at 12797: 1.47738, expected 1.92444\n",
      "2024-11-07 19:03:20.629414: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 400 is 0.1342500001192093.\n",
      "Loss at step 800 is 0.07718414068222046.\n",
      "Loss at step 1200 is 0.07180865854024887.\n",
      "Loss at step 1600 is 0.05643702298402786.\n",
      "Loss at step 2000 is 0.051197513937950134.\n",
      "Loss at step 2400 is 0.05043477565050125.\n",
      "Loss at step 2800 is 0.04995693266391754.\n",
      "Loss at step 3200 is 0.0464225634932518.\n",
      "Loss at step 3600 is 0.0442780926823616.\n",
      "Loss at step 4000 is 0.05500848963856697.\n",
      "Loss at step 4400 is 0.042969051748514175.\n",
      "Loss at step 4800 is 0.045106660574674606.\n",
      "Loss at step 5200 is 0.0390787348151207.\n",
      "Loss at step 5600 is 0.034936655312776566.\n",
      "Loss at step 6000 is 0.03454167768359184.\n",
      "Loss at step 6400 is 0.035640861839056015.\n",
      "Loss at step 6800 is 0.03724970668554306.\n",
      "Loss at step 7200 is 0.03442167490720749.\n",
      "Loss at step 7600 is 0.032301291823387146.\n",
      "Loss at step 8000 is 0.03418886661529541.\n",
      "Loss at step 8400 is 0.03557194769382477.\n",
      "Loss at step 8800 is 0.02997479774057865.\n",
      "Loss at step 9200 is 0.034309711307287216.\n",
      "Loss at step 9600 is 0.03238853067159653.\n",
      "Loss at step 10000 is 0.031112292781472206.\n",
      "Loss at step 10400 is 0.031000329181551933.\n",
      "Loss at step 10800 is 0.033649202436208725.\n",
      "Loss at step 11200 is 0.030090929940342903.\n",
      "Loss at step 11600 is 0.02671491727232933.\n",
      "Loss at step 12000 is 0.03210251033306122.\n",
      "Loss at step 12400 is 0.03144845739006996.\n",
      "Loss at step 12800 is 0.027561649680137634.\n",
      "Loss at step 13200 is 0.03008284606039524.\n",
      "Loss at step 13600 is 0.03877859562635422.\n",
      "Loss at step 14000 is 0.029006900265812874.\n",
      "Loss at step 14400 is 0.03080359846353531.\n",
      "Loss at step 14800 is 0.027362123131752014.\n",
      "Loss at step 15200 is 0.030436133965849876.\n",
      "Loss at step 15600 is 0.024100717157125473.\n",
      "Loss at step 16000 is 0.029597530141472816.\n",
      "Loss at step 16400 is 0.030169256031513214.\n",
      "Loss at step 16800 is 0.027459649369120598.\n",
      "Loss at step 17200 is 0.02939317375421524.\n",
      "Loss at step 17600 is 0.024962924420833588.\n",
      "Loss at step 18000 is 0.023011596873402596.\n",
      "Loss at step 18400 is 0.0294911190867424.\n",
      "Loss at step 18800 is 0.02333643101155758.\n",
      "Loss at step 19200 is 0.025485847145318985.\n",
      "Loss at step 19600 is 0.027412744238972664.\n",
      "Loss at step 20000 is 0.02450065314769745.\n",
      "Loss at step 20400 is 0.022744933143258095.\n",
      "Loss at step 20800 is 0.023943757638335228.\n",
      "Loss at step 21200 is 0.02492828480899334.\n",
      "Loss at step 21600 is 0.025343608111143112.\n",
      "Loss at step 22000 is 0.026896998286247253.\n",
      "Loss at step 22400 is 0.0285204891115427.\n",
      "Loss at step 22800 is 0.024610713124275208.\n",
      "Loss at step 23200 is 0.02492326684296131.\n",
      "Loss at step 23600 is 0.024556118994951248.\n",
      "Loss at step 24000 is 0.031479720026254654.\n",
      "Loss at step 24400 is 0.026882605627179146.\n",
      "Loss at step 24800 is 0.026951797306537628.\n",
      "Loss at step 25200 is 0.023126397281885147.\n",
      "Loss at step 25600 is 0.022432509809732437.\n",
      "Loss at step 26000 is 0.02122844010591507.\n",
      "Loss at step 26400 is 0.02783934958279133.\n",
      "Loss at step 26800 is 0.02644423022866249.\n",
      "Loss at step 27200 is 0.020546656101942062.\n",
      "Loss at step 27600 is 0.02193501964211464.\n",
      "Loss at step 28000 is 0.02293797768652439.\n",
      "Loss at step 28400 is 0.02108951471745968.\n",
      "Loss at step 28800 is 0.024147827178239822.\n",
      "Loss at step 29200 is 0.02550804242491722.\n",
      "Loss at step 29600 is 0.025933273136615753.\n",
      "Loss at step 30000 is 0.025989005342125893.\n",
      "Loss at step 30400 is 0.023004649206995964.\n",
      "Loss at step 30800 is 0.02321445383131504.\n",
      "Loss at step 31200 is 0.021990710869431496.\n",
      "Loss at step 31600 is 0.02297286130487919.\n",
      "Loss at step 32000 is 0.024571184068918228.\n",
      "Loss at step 32400 is 0.023008298128843307.\n",
      "Loss at step 32800 is 0.02209194004535675.\n",
      "Loss at step 33200 is 0.02543245628476143.\n",
      "Loss at step 33600 is 0.022359130904078484.\n",
      "Loss at step 34000 is 0.020917009562253952.\n",
      "Loss at step 34400 is 0.02079252153635025.\n",
      "Loss at step 34800 is 0.022911744192242622.\n",
      "Loss at step 35200 is 0.021594036370515823.\n",
      "Loss at step 35600 is 0.02228187769651413.\n",
      "Loss at step 36000 is 0.021541984751820564.\n",
      "Loss at step 36400 is 0.023275794461369514.\n",
      "Loss at step 36800 is 0.023395907133817673.\n",
      "Loss at step 37200 is 0.02234872616827488.\n",
      "Loss at step 37600 is 0.020484348759055138.\n",
      "Loss at step 38000 is 0.02157561667263508.\n",
      "Loss at step 38400 is 0.02281867153942585.\n",
      "Loss at step 38800 is 0.02089514397084713.\n",
      "Loss at step 39200 is 0.022015782073140144.\n",
      "Loss at step 39600 is 0.02290898561477661.\n",
      "Loss at step 40000 is 0.022137820720672607.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb7e70ffc2e48f691d8892ef07d4a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='11.903 MB of 11.903 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MSE_on_fixed_grid</td><td></td></tr><tr><td>batch_within_epoch</td><td></td></tr><tr><td>epoch</td><td></td></tr><tr><td>loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MSE_on_fixed_grid</td><td>0.0033</td></tr><tr><td>batch_within_epoch</td><td>40000</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.02214</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">jumping-dragon-22</strong> at: <a href='https://wandb.ai/nld/inr_edu_24/runs/39cyc5a0' target=\"_blank\">https://wandb.ai/nld/inr_edu_24/runs/39cyc5a0</a><br/> View project at: <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">https://wandb.ai/nld/inr_edu_24</a><br/>Synced 5 W&B file(s), 0 media file(s), 1 artifact file(s) and 100 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241107_190315-39cyc5a0/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and we run the experiment while logging things to wandb\n",
    "with wandb.init(\n",
    "    project='inr_edu_24',\n",
    "    notes='test',\n",
    "    tags=['test']\n",
    ") as run:\n",
    "    results = experiment.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
