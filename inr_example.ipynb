{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of training an INR locally\n",
    "This notebook provides an example of how to create an INR and train it locally using the tools in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\n",
    "\n",
    "To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \"CombinedINR\"` instead of `config[\"model_type\"] = \"CombinedINR\"`). You can also just use a dictionary instead.\n",
    "\n",
    "Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\n",
    "\n",
    "Doing this in a config instead of hard coded might seem like extra work, but consider this:\n",
    "1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \n",
    "   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\n",
    "2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\n",
    "   and then run an experiment specified by that config on any machine you want, e.g. on Snellius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 3\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 256,\n",
    "    #     'num_layers': 5,\n",
    "    #     'layer_type': 'inr_layers.RealWire',\n",
    "    #     'num_splits': 1,\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'w0': 15., 's0':1.}\n",
    "    # }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 256,\n",
    "    #     'num_layers': 5,\n",
    "    #     'layer_type': 'inr_layers.FinerLayer',  # Include the FinerLayer here\n",
    "    #     'num_splits': 1,\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'w0': 20}  # Set the w0 hyperparameter for the FINER activation\n",
    "    # }),\n",
    "    ('inr_layers.FinerLayer', {\n",
    "        'in_features': config.model_config.in_size,\n",
    "        'out_features': config.model_config.out_size,\n",
    "        'hidden_layers': 3,  # Number of hidden layers for FINER\n",
    "        'hidden_size': 256,  # Size of each hidden layer\n",
    "        'omega': 30  # Frequency parameter for FINER activation\n",
    "    })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "config.trainer_type = 'training.train_inr'\n",
    "config.target_function = 'images.ContinuousImage'\n",
    "config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation'\n",
    "}\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "config.steps = 40000\n",
    "config.use_wandb = True\n",
    "\n",
    "# now we want some extra things, like logging, to happen during training\n",
    "# the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# The callbacks we want to use can be found in inr_utils.callbacks\n",
    "config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "config.after_step_callback_config = {\n",
    "    'callbacks':[\n",
    "        ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "        'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "        ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "            {'metric_collector':'metrics.MetricCollector'}\n",
    "        ),\n",
    "        'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "    ],\n",
    "    'show_logs': False\n",
    "}\n",
    "\n",
    "config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "    'metrics':[\n",
    "        ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "        # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "        ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "        # ^ compute the MSE with the actual image pixels\n",
    "    ],\n",
    "    'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "    'epoch_frequency': 1  # not actually used\n",
    "}\n",
    "\n",
    "config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "config.optimizer_state = None  # we're starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_17244/1141651026.py\", line 3, in <module>\n",
      "    inr = cju.run_utils.get_model_from_config_and_key(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_jax_utils/run_utils.py\", line 115, in get_model_from_config_and_key\n",
      "    return un_initialized_model.initialize()\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/config_realization.py\", line 182, in initialize\n",
      "    processed_self_kwargs = tree_map(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/trees.py\", line 126, in tree_map\n",
      "    {\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/trees.py\", line 127, in <dictcomp>\n",
      "    key: tree_map(sub_tree, func, is_leaf=is_leaf)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/trees.py\", line 132, in tree_map\n",
      "    return type(tree)(tree_map(sub_tree, func, is_leaf=is_leaf) for sub_tree in tree)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/trees.py\", line 132, in <genexpr>\n",
      "    return type(tree)(tree_map(sub_tree, func, is_leaf=is_leaf) for sub_tree in tree)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/trees.py\", line 116, in tree_map\n",
      "    return func(tree)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/config_realization.py\", line 184, in <lambda>\n",
      "    func = lambda x: x.initialize() if isinstance(x, PostponedInitialization) else x,\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/config_realization.py\", line 195, in initialize\n",
      "    return cls(**processed_self_kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_module.py\", line 572, in __call__\n",
      "    self = super(_ModuleMeta, initable_cls).__call__(*args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_better_abstract.py\", line 223, in __call__\n",
      "    raise TypeError(\n",
      "TypeError: Can't instantiate abstract class FinerLayer with abstract class attributes {'_activation_function'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't instantiate abstract class FinerLayer with abstract class attributes {'_activation_function'}\n",
      "\n",
      "\n",
      "> \u001b[0;32m/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_better_abstract.py\u001b[0m(223)\u001b[0;36m__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    221 \u001b[0;31m        \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__abstractclassvars__\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pyright: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    222 \u001b[0;31m            \u001b[0mabstract_class_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__abstractclassvars__\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pyright: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 223 \u001b[0;31m            raise TypeError(\n",
      "\u001b[0m\u001b[0;32m    224 \u001b[0;31m                \u001b[0;34mf\"Can't instantiate abstract class {cls.__name__} with abstract class \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    225 \u001b[0;31m                \u001b[0;34mf\"attributes {abstract_class_vars}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'inr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minr\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'inr' is not defined"
     ]
    }
   ],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "mul got incompatible shapes for broadcasting: (256,), (2,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# check that it works properly\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43minr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/INR_BEP/model_components/inr_modules.py:144\u001b[0m, in \u001b[0;36mCombinedINR.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processor(\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterms\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/INR_BEP/model_components/inr_modules.py:144\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processor(\u001b[38;5;28msum\u001b[39m(\u001b[43mterm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterms))\n",
      "File \u001b[0;32m~/INR_BEP/model_components/inr_modules.py:127\u001b[0m, in \u001b[0;36mMLPINR.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers:\n\u001b[1;32m    129\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/INR_BEP/model_components/inr_layers.py:387\u001b[0m, in \u001b[0;36mFinerLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    384\u001b[0m     wxb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m@\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Use the finer_activation function from activation_functions.py\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiner_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwxb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/INR_BEP/model_components/activation_functions.py:53\u001b[0m, in \u001b[0;36mfiner_activation\u001b[0;34m(x, wxb)\u001b[0m\n\u001b[1;32m     50\u001b[0m alpha_l \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mabs(wxb) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# As defined in the paper\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Apply the activation function sin(alpha^l * x)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msin(\u001b[43malpha_l\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:177\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:1143\u001b[0m, in \u001b[0;36m_multiply\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Multiply two arrays element-wise.\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03mJAX implementation of :obj:`numpy.multiply`. This is a universal function,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;124;03m  Array([ 0, 10, 20, 30], dtype=int32)\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_and(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/lax.py:1966\u001b[0m, in \u001b[0;36mbroadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   1964\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1966\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1967\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: mul got incompatible shapes for broadcasting: (256,), (2,)."
     ]
    }
   ],
   "source": [
    "# check that it works properly\n",
    "inr(jnp.zeros(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/abdtab/INR_BEP/wandb/run-20241024_002015-5vvi11vm</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/abdtab-tue/inr_edu_24/runs/5vvi11vm' target=\"_blank\">rural-cherry-12</a></strong> to <a href='https://wandb.ai/abdtab-tue/inr_edu_24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdtab-tue/inr_edu_24' target=\"_blank\">https://wandb.ai/abdtab-tue/inr_edu_24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/abdtab-tue/inr_edu_24/runs/5vvi11vm' target=\"_blank\">https://wandb.ai/abdtab-tue/inr_edu_24/runs/5vvi11vm</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_40872/4160894630.py\", line 7, in <module>\n",
      "    results = experiment.initialize()\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/config_realization.py\", line 195, in initialize\n",
      "    return cls(**processed_self_kwargs)\n",
      "  File \"/home/abdtab/INR_BEP/inr_utils/training.py\", line 152, in train_inr\n",
      "    inr, optimizer_state, loss = train_step(inr, optimizer_state, next(key_gen))\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_jit.py\", line 242, in __call__\n",
      "    return self._call(False, args, kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_module.py\", line 1078, in __call__\n",
      "    return self.__func__(self.__self__, *args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_jit.py\", line 215, in _call\n",
      "    out = self._cached(dynamic_donate, dynamic_nodonate, static)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 356, in cache_miss\n",
      "    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 179, in _python_pjit_helper\n",
      "    p, args_flat = _infer_params(fun, jit_info, args, kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 773, in _infer_params\n",
      "    p, args_flat = _infer_params_impl(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 670, in _infer_params_impl\n",
      "    jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 352, in memoized_fun\n",
      "    ans = call(fun, *args)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 1314, in _create_pjit_jaxpr\n",
      "    jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/profiler.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2278, in trace_to_jaxpr_dynamic\n",
      "    jaxpr, out_avals, consts, attrs_tracked = trace_to_subjaxpr_dynamic(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2301, in trace_to_subjaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers_)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 193, in call_wrapped\n",
      "    ans = self.f(*args, **dict(self.params, **kwargs))\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_jit.py\", line 51, in fun_wrapped\n",
      "    out = fun(*args, **kwargs)\n",
      "  File \"/home/abdtab/INR_BEP/inr_utils/training.py\", line 72, in train_step\n",
      "    loss, grad = eqx.filter_value_and_grad(evaluate_loss)(inr, locations)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_ad.py\", line 79, in __call__\n",
      "    return fun_value_and_grad(diff_x, nondiff_x, *args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api.py\", line 462, in value_and_grad_f\n",
      "    ans, vjp_py = _vjp(f_partial, *dyn_args)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api.py\", line 1962, in _vjp\n",
      "    out_primals, vjp = ad.vjp(flat_fun, primals_flat)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/ad.py\", line 142, in vjp\n",
      "    out_primals, pvals, jaxpr, consts = linearize(traceable, *primals)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/ad.py\", line 131, in linearize\n",
      "    jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/profiler.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 681, in trace_to_jaxpr_nounits\n",
      "    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 193, in call_wrapped\n",
      "    ans = self.f(*args, **dict(self.params, **kwargs))\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_ad.py\", line 62, in fun_value_and_grad\n",
      "    return self._fun(_x, *_args, **_kwargs)\n",
      "  File \"/home/abdtab/INR_BEP/inr_utils/training.py\", line 56, in evaluate_loss\n",
      "    pred_val = jax.vmap(inr)(locations)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api.py\", line 992, in vmap_f\n",
      "    out_flat = batching.batch(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 193, in call_wrapped\n",
      "    ans = self.f(*args, **dict(self.params, **kwargs))\n",
      "  File \"/home/abdtab/INR_BEP/model_components/inr_modules.py\", line 144, in __call__\n",
      "    return self.post_processor(sum(term(x) for term in self.terms))\n",
      "  File \"/home/abdtab/INR_BEP/model_components/inr_modules.py\", line 144, in <genexpr>\n",
      "    return self.post_processor(sum(term(x) for term in self.terms))\n",
      "  File \"/home/abdtab/INR_BEP/model_components/inr_modules.py\", line 127, in __call__\n",
      "    x = self.input_layer(x)\n",
      "  File \"/home/abdtab/INR_BEP/model_components/inr_layers.py\", line 387, in __call__\n",
      "    return act.finer_activation(x, wxb)\n",
      "  File \"/home/abdtab/INR_BEP/model_components/activation_functions.py\", line 53, in finer_activation\n",
      "    return jnp.sin(alpha_l * x)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py\", line 1036, in op\n",
      "    return getattr(self.aval, f\"_{name}\")(self, *args)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py\", line 573, in deferring_binary_op\n",
      "    return binary_op(*args)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py\", line 177, in __call__\n",
      "    return call(*args)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 356, in cache_miss\n",
      "    outs, out_flat, out_tree, args_flat, jaxpr, attrs_tracked = _python_pjit_helper(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 179, in _python_pjit_helper\n",
      "    p, args_flat = _infer_params(fun, jit_info, args, kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 773, in _infer_params\n",
      "    p, args_flat = _infer_params_impl(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 670, in _infer_params_impl\n",
      "    jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 352, in memoized_fun\n",
      "    ans = call(fun, *args)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 1314, in _create_pjit_jaxpr\n",
      "    jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/profiler.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2278, in trace_to_jaxpr_dynamic\n",
      "    jaxpr, out_avals, consts, attrs_tracked = trace_to_subjaxpr_dynamic(\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2301, in trace_to_subjaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers_)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 193, in call_wrapped\n",
      "    ans = self.f(*args, **dict(self.params, **kwargs))\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py\", line 1143, in _multiply\n",
      "    return lax.mul(x, y) if x.dtype != bool else lax.bitwise_and(x, y)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/lax.py\", line 433, in mul\n",
      "    return mul_p.bind(x, y)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/core.py\", line 438, in bind\n",
      "    return self.bind_with_trace(find_top_trace(args), args, params)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/core.py\", line 442, in bind_with_trace\n",
      "    out = trace.process_primitive(self, map(trace.full_raise, args), params)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 1969, in process_primitive\n",
      "    return self.default_process_primitive(primitive, tracers, params)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 1973, in default_process_primitive\n",
      "    out_avals, effects = primitive.abstract_eval(*avals, **params)\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/core.py\", line 475, in abstract_eval_\n",
      "    return abstract_eval(*args, **kwargs), no_effects\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/utils.py\", line 64, in standard_abstract_eval\n",
      "    return core.ShapedArray(shape_rule(*avals, **kwargs),\n",
      "  File \"/home/abdtab/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/lax.py\", line 1966, in broadcasting_shape_rule\n",
      "    raise TypeError(f'{name} got incompatible shapes for broadcasting: '\n",
      "TypeError: mul got incompatible shapes for broadcasting: (256,), (2,).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e686f8e9eb4d4d84c078e066575a44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.015 MB of 0.015 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rural-cherry-12</strong> at: <a href='https://wandb.ai/abdtab-tue/inr_edu_24/runs/5vvi11vm' target=\"_blank\">https://wandb.ai/abdtab-tue/inr_edu_24/runs/5vvi11vm</a><br/> View project at: <a href='https://wandb.ai/abdtab-tue/inr_edu_24' target=\"_blank\">https://wandb.ai/abdtab-tue/inr_edu_24</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241024_002015-5vvi11vm/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "mul got incompatible shapes for broadcasting: (256,), (2,).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# and we run the experiment while logging things to wandb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      3\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minr_edu_24\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     tags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[0;32m----> 7\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/config_realization.py:195\u001b[0m, in \u001b[0;36mPostponedInitialization.initialize\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m processed_self_kwargs\u001b[38;5;241m.\u001b[39mupdate(processed_kwargs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m _internal_utils\u001b[38;5;241m.\u001b[39msignature_to_var_keyword(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_wrapping \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprocessed_self_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/INR_BEP/inr_utils/training.py:152\u001b[0m, in \u001b[0;36mtrain_inr\u001b[0;34m(inr, target_function, loss_function, sampler, optimizer, steps, key, use_wandb, after_step_callback, after_training_callback, optimizer_state)\u001b[0m\n\u001b[1;32m    149\u001b[0m losses \u001b[38;5;241m=\u001b[39m steps\u001b[38;5;241m*\u001b[39m[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):  \u001b[38;5;66;03m# the actual training loop\u001b[39;00m\n\u001b[0;32m--> 152\u001b[0m     inr, optimizer_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey_gen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    153\u001b[0m     losses[step] \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    154\u001b[0m     after_step_callback(step, loss, inr, optimizer_state)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/INR_BEP/inr_utils/training.py:72\u001b[0m, in \u001b[0;36mmake_inr_train_step_function.<locals>.train_step\u001b[0;34m(inr, optimizer_state, key)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"take one train step with the inr model and optimizer\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m:param inr: inr model to be optimized\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03mNB this function is decorated with eqx.filter_jit\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m locations \u001b[38;5;241m=\u001b[39m sampler(key)\n\u001b[0;32m---> 72\u001b[0m loss, grad \u001b[38;5;241m=\u001b[39m \u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m updates, optimizer_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grad, optimizer_state, inr)\n\u001b[1;32m     74\u001b[0m inr \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mapply_updates(inr, updates)\n",
      "    \u001b[0;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "File \u001b[0;32m~/INR_BEP/inr_utils/training.py:56\u001b[0m, in \u001b[0;36mmake_inr_train_step_function.<locals>.evaluate_loss\u001b[0;34m(inr, locations)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_loss\u001b[39m(inr: eqx\u001b[38;5;241m.\u001b[39mModule, locations:jax\u001b[38;5;241m.\u001b[39mArray):\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m    Evaluate the loss of the inr at the locations\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03m    :param inr: eqx.Module to be evaluated at locations\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m    :param locations: (batch_size, n_dim) shaped jax.Array of locations at which to evaluate the inr and target_function\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;124;03m    :return: loss value (should be a scalar, assuming loss_function has the correct signature)\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     pred_val \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43minr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     true_val \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(target_function)(locations)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_function(pred_val, true_val)\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "File \u001b[0;32m~/INR_BEP/model_components/inr_modules.py:144\u001b[0m, in \u001b[0;36mCombinedINR.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processor(\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterms\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/INR_BEP/model_components/inr_modules.py:144\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_processor(\u001b[38;5;28msum\u001b[39m(\u001b[43mterm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mterms))\n",
      "File \u001b[0;32m~/INR_BEP/model_components/inr_modules.py:127\u001b[0m, in \u001b[0;36mMLPINR.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 127\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_layers:\n\u001b[1;32m    129\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/INR_BEP/model_components/inr_layers.py:387\u001b[0m, in \u001b[0;36mFinerLayer.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    384\u001b[0m     wxb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m@\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbiases\n\u001b[1;32m    386\u001b[0m \u001b[38;5;66;03m# Use the finer_activation function from activation_functions.py\u001b[39;00m\n\u001b[0;32m--> 387\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mact\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiner_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwxb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/INR_BEP/model_components/activation_functions.py:53\u001b[0m, in \u001b[0;36mfiner_activation\u001b[0;34m(x, wxb)\u001b[0m\n\u001b[1;32m     50\u001b[0m alpha_l \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mabs(wxb) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# As defined in the paper\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Apply the activation function sin(alpha^l * x)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39msin(\u001b[43malpha_l\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:1036\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m-> 1036\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/array_methods.py:573\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    571\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 573\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/ufunc_api.py:177\u001b[0m, in \u001b[0;36mufunc.__call__\u001b[0;34m(self, out, where, *args)\u001b[0m\n\u001b[1;32m    175\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhere argument of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    176\u001b[0m call \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__static_props[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_vectorized\n\u001b[0;32m--> 177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/numpy/ufuncs.py:1143\u001b[0m, in \u001b[0;36m_multiply\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Multiply two arrays element-wise.\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \n\u001b[1;32m   1119\u001b[0m \u001b[38;5;124;03mJAX implementation of :obj:`numpy.multiply`. This is a universal function,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;124;03m  Array([ 0, 10, 20, 30], dtype=int32)\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiply\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_and(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/lax.py:1966\u001b[0m, in \u001b[0;36mbroadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   1964\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1965\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1966\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1967\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1969\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: mul got incompatible shapes for broadcasting: (256,), (2,)."
     ]
    }
   ],
   "source": [
    "# and we run the experiment while logging things to wandb\n",
    "with wandb.init(\n",
    "    project='inr_edu_24',\n",
    "    notes='test',\n",
    "    tags=['test']\n",
    ") as run:\n",
    "    results = experiment.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class FinerLayer with abstract class attributes {'_activation_function'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 61\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[1;32m     60\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 61\u001b[0m finer_model \u001b[38;5;241m=\u001b[39m \u001b[43mFINERModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_layers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momega\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Define loss function\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(params, coords, rgb_values):\n",
      "    \u001b[0;31m[... skipping hidden 3 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[2], line 39\u001b[0m, in \u001b[0;36mFINERModel.__init__\u001b[0;34m(self, in_size, out_size, hidden_layers, hidden_features, omega, key)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Input layer with first_omega\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mappend(\u001b[43mFinerLayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43momega\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43momega\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Hidden layers with hidden_omega\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, hidden_layers \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/equinox/_better_abstract.py:223\u001b[0m, in \u001b[0;36mABCMeta.__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__abstractclassvars__) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     abstract_class_vars \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__abstractclassvars__)  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt instantiate abstract class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with abstract class \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattributes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mabstract_class_vars\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    226\u001b[0m     )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__abstractvars__) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# pyright: ignore\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class FinerLayer with abstract class attributes {'_activation_function'}"
     ]
    }
   ],
   "source": [
    "# Import the necessary modules and functions\n",
    "from model_components.inr_layers import FinerLayer\n",
    "import model_components.activation_functions as act\n",
    "import optax\n",
    "from PIL import Image\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import equinox as eqx\n",
    "\n",
    "from common_jax_utils import key_generator\n",
    "import model_components.auxiliary as aux\n",
    "import numpy as np\n",
    "\n",
    "# Utility functions to load and prepare the image\n",
    "def load_image(image_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img = np.array(img) / 255.0  # Normalize pixel values to [0, 1]\n",
    "    return img\n",
    "\n",
    "def prepare_data(img):\n",
    "    height, width, _ = img.shape\n",
    "    x_coords = np.linspace(-1, 1, width)\n",
    "    y_coords = np.linspace(-1, 1, height)\n",
    "    coords = np.array(np.meshgrid(x_coords, y_coords)).reshape(2, -1).T  # Shape: (width*height, 2)\n",
    "    rgb_values = img.reshape(-1, 3)  # Flatten RGB values to match coords\n",
    "    return jnp.array(coords), jnp.array(rgb_values)\n",
    "\n",
    "# Initialize the FINER model\n",
    "class FINERModel(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, in_size, out_size, hidden_layers, hidden_features, omega, key):\n",
    "        keys = jax.random.split(key, hidden_layers + 2)\n",
    "        \n",
    "        # Define layers\n",
    "        self.layers = []\n",
    "        \n",
    "        # Input layer with first_omega\n",
    "        self.layers.append(FinerLayer(in_size, hidden_features, omega=omega, key=keys[0], is_first=True, fbs=0.1))\n",
    "        \n",
    "        # Hidden layers with hidden_omega\n",
    "        for i in range(1, hidden_layers + 1):\n",
    "            self.layers.append(FinerLayer(hidden_features, hidden_features, omega=omega, key=keys[i]))\n",
    "        \n",
    "        # Output layer (no activation)\n",
    "        self.layers.append(FinerLayer(hidden_features, out_size, omega=omega, key=keys[-1], is_last=True))\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Pass input through each layer\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "# Image path and load image data\n",
    "image_path = \"./example_data/parrot.png\"  # Replace with your actual image path\n",
    "img = load_image(image_path)\n",
    "coords, rgb_values = prepare_data(img)\n",
    "\n",
    "# Initialize model\n",
    "key = jax.random.PRNGKey(0)\n",
    "finer_model = FINERModel(in_size=2, out_size=3, hidden_layers=3, hidden_features=256, omega=30, key=key)\n",
    "\n",
    "# Define loss function\n",
    "def loss_fn(params, coords, rgb_values):\n",
    "    pred_rgb = finer_model(coords)  # Predict RGB values\n",
    "    return jnp.mean((pred_rgb - rgb_values) ** 2)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = optax.adam(learning_rate=1e-4)\n",
    "opt_state = optimizer.init(finer_model)\n",
    "\n",
    "# Training loop\n",
    "num_steps = 5000\n",
    "\n",
    "@jax.jit\n",
    "def train_step(finer_model, opt_state, coords, rgb_values):\n",
    "    # Compute loss and gradients\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(finer_model, coords, rgb_values)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    finer_model = eqx.apply_updates(finer_model, updates)\n",
    "    return finer_model, opt_state, loss\n",
    "\n",
    "# Train the model\n",
    "for step in range(num_steps):\n",
    "    finer_model, opt_state, loss = train_step(finer_model, opt_state, coords, rgb_values)\n",
    "    if step % 1000 == 0:\n",
    "        print(f\"Step {step}, Loss: {loss}\")\n",
    "\n",
    "# Generate reconstructed image\n",
    "reconstructed_rgb = finer_model(coords).reshape(img.shape)\n",
    "reconstructed_img = (np.clip(reconstructed_rgb, 0, 1) * 255).astype(np.uint8)\n",
    "Image.fromarray(reconstructed_img).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (inr_edu_24)",
   "language": "python",
   "name": "inr_edu_24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
