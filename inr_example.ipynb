{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of training an INR locally\n",
    "This notebook provides an example of how to create an INR and train it locally using the tools in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:22.968569596Z",
     "start_time": "2025-01-06T14:55:22.925237450Z"
    }
   },
   "outputs": [],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:23.553170506Z",
     "start_time": "2025-01-06T14:55:23.447043302Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\n",
    "\n",
    "To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \"CombinedINR\"` instead of `config[\"model_type\"] = \"CombinedINR\"`). You can also just use a dictionary instead.\n",
    "\n",
    "Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\n",
    "\n",
    "Doing this in a config instead of hard coded might seem like extra work, but consider this:\n",
    "1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \n",
    "   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\n",
    "2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\n",
    "   and then run an experiment specified by that config on any machine you want, e.g. on Snellius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:24.410408058Z",
     "start_time": "2025-01-06T14:55:24.393999841Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 1\n",
    "config.model_config.out_size = 1\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 3,\n",
    "        'activation_kwargs': {'w0':12.},#{'inverse_scale': 5.},\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        'initialization_scheme_kwargs': {'w0': 12.},\n",
    "        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "    ('inr_modules.MLPINR.new_from_config',{\n",
    "        'hidden_size': 256,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.FinerLayer',\n",
    "        'num_splits': 1,\n",
    "        'use_complex': False,\n",
    "        'activation_kwargs': {'w0': 30},\n",
    "        'initialization_scheme':'initialization_schemes.finer_scheme',\n",
    "        'initialization_scheme_kwargs':{'bias_k' : 10}\n",
    "        # 'initialization_scheme_k' : {'k': 20}\n",
    "        #'positional_encoding_layer': ('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies': 10}),\n",
    "    })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "config.trainer_type = 'training.train_inr'\n",
    "config.loss_evaluator = 'losses.PointWiseLossEvaluator'\n",
    "config.target_function = 'images.ContinuousImage'\n",
    "config.target_function_config = {\n",
    "    'image': './example_data/parrot.png',\n",
    "    'scale_to_01': True,\n",
    "    'interpolation_method': 'images.make_piece_wise_constant_interpolation'\n",
    "}\n",
    "config.loss_function = 'losses.scaled_mse_loss'\n",
    "config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "config.sampler = ('sampling.GridSubsetSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    'size': [2040, 1356],\n",
    "    'batch_size': 2000,\n",
    "    'allow_duplicates': False,\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "config.steps = 20000 #changed from 40000\n",
    "config.use_wandb = True\n",
    "\n",
    "# now we want some extra things, like logging, to happen during training\n",
    "# the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# The callbacks we want to use can be found in inr_utils.callbacks\n",
    "config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "config.after_step_callback_config = {\n",
    "    'callbacks':[\n",
    "        ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "        'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "        ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "            {'metric_collector':'metrics.MetricCollector'}\n",
    "        ),\n",
    "        'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "    ],\n",
    "    'show_logs': False\n",
    "}\n",
    "\n",
    "config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\n",
    "\n",
    "config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "    'metrics':[\n",
    "        ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "        # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "        ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "        # ^ compute the MSE with the actual image pixels\n",
    "    ],\n",
    "    'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "    'epoch_frequency': 1  # not actually used\n",
    "}\n",
    "\n",
    "#config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "config.optimizer_state = None  # we're starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T13:11:48.290828912Z",
     "start_time": "2024-11-27T13:11:48.207796154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      layers=(\n",
       "        CountingIdentity(\n",
       "          _embedding_matrix=i32[],\n",
       "          state_index=StateIndex(\n",
       "            marker=<object object at 0x7aa0d7213240>,\n",
       "            init=i32[]\n",
       "          )\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,2],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        FinerLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        FinerLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "      )\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it works properly\n",
    "try:\n",
    "    inr(jnp.zeros(2))\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostponedInitialization(cls=train_inr, kwargs={'loss_evaluator': PostponedInitialization(cls=PointWiseLossEvaluator, kwargs={'target_function': PostponedInitialization(cls=ContinuousImage, kwargs={'image': './example_data/parrot.png', 'scale_to_01': True, 'interpolation_method': <function make_piece_wise_constant_interpolation at 0x7aa0b97130a0>}, missing_args=[]), 'loss_function': <function scaled_mse_loss at 0x7aa0b971c670>, 'state_update_function': <function counter_updater at 0x7aa0b971f520>}, missing_args=[]), 'sampler': PostponedInitialization(cls=GridSubsetSampler, kwargs={'size': [2040, 1356], 'batch_size': 2000, 'allow_duplicates': False, 'min': 0.0, 'max': 1.0, 'num_dimensions': None, 'indexing': 'ij'}, missing_args=[]), 'optimizer': PostponedInitialization(cls=adam, kwargs={'learning_rate': 0.00015, 'b1': 0.9, 'b2': 0.999, 'eps': 1e-08, 'eps_root': 0.0, 'mu_dtype': None, 'nesterov': False}, missing_args=[]), 'steps': 20000, 'use_wandb': True, 'after_step_callback': PostponedInitialization(cls=ComposedCallback, kwargs={'callbacks': [functools.partial(<function print_loss at 0x7aa0b971cca0>, after_every=400), <function report_loss at 0x7aa0b971d900>, PostponedInitialization(cls=MetricCollectingCallback, kwargs={'metric_collector': PostponedInitialization(cls=MetricCollector, kwargs={'metrics': [PostponedInitialization(cls=PlotOnGrid2D, kwargs={'grid': 256, 'batch_size': 2048, 'frequency': 'every_n_batches', 'use_wandb': True}, missing_args=[]), PostponedInitialization(cls=MSEOnFixedGrid, kwargs={'grid': [2040, 1356], 'batch_size': 2040, 'frequency': 'every_n_batches', 'num_dims': None, 'target_function': PostponedInitialization(cls=ContinuousImage, kwargs={'image': './example_data/parrot.png', 'scale_to_01': True, 'interpolation_method': <function make_piece_wise_constant_interpolation at 0x7aa0b97130a0>}, missing_args=[])}, missing_args=[])], 'batch_frequency': 400, 'epoch_frequency': 1}, missing_args=[])}, missing_args=[]), <function raise_error_on_nan at 0x7aa0b971dab0>], 'show_logs': False, 'use_wandb': True, 'display_func': <function pprint at 0x7aa11de09120>}, missing_args=[]), 'after_training_callback': <function after_training_callback at 0x7aa0b971f250>, 'optimizer_state': None, 'state_initialization_function': <function initialize_state at 0x7aa0b971de10>, 'state': None, 'inr': PostponedInitialization(cls=CombinedINR, kwargs={'terms': [PostponedInitialization(cls=from_config, kwargs={'hidden_size': 256, 'num_layers': 5, 'layer_type': <class 'model_components.inr_layers.SirenLayer'>, 'activation_kwargs': {'w0': 12.0}, 'initialization_scheme': <function siren_scheme at 0x7aa0d59d1b40>, 'initialization_scheme_kwargs': {'w0': 12.0}, 'positional_encoding_layer': PostponedInitialization(cls=CountingIdentity, kwargs={}, missing_args=[]), 'num_splits': 3, 'post_processor': None, 'in_size': 2, 'out_size': 3, 'key': Array([4177750840, 1613599438], dtype=uint32)}, missing_args=[])], 'post_processor': <function real_part at 0x7aa0d592b130>}, missing_args=[]), 'key': Array([ 793826064, 3381256178], dtype=uint32)}, missing_args=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/simon/Documents/INR_BEP/wandb/run-20250103_192213-fpkgzkuc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nld/inr_edu_24/runs/fpkgzkuc' target=\"_blank\">light-vortex-96</a></strong> to <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/abdtab-tue/inr_edu_24' target=\"_blank\">https://wandb.ai/abdtab-tue/inr_edu_24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nld/inr_edu_24/runs/fpkgzkuc' target=\"_blank\">https://wandb.ai/nld/inr_edu_24/runs/fpkgzkuc</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0103 19:22:19.402503  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.402526  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.402530  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.402537  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.402540  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.402544  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.402574  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.402582  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.402585  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.402592  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.402595: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.403460  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.403467  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.403471  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.403478  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.403481  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.403484  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.403516  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.403524  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.403531  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.403539  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.403541: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.404427  151466 buffer_comparator.cc:157] Difference at 1417: -0.0245552, expected -0.18261\n",
      "E0103 19:22:19.404437  151466 buffer_comparator.cc:157] Difference at 1508: 0.344917, expected 0.513586\n",
      "E0103 19:22:19.404440  151466 buffer_comparator.cc:157] Difference at 2581: -0.593309, expected -0.810047\n",
      "E0103 19:22:19.404447  151466 buffer_comparator.cc:157] Difference at 5564: -0.126198, expected -0.26074\n",
      "E0103 19:22:19.404450  151466 buffer_comparator.cc:157] Difference at 6406: 0.365433, expected 0.127136\n",
      "E0103 19:22:19.404454  151466 buffer_comparator.cc:157] Difference at 7411: 0.272489, expected 0.135361\n",
      "E0103 19:22:19.404484  151466 buffer_comparator.cc:157] Difference at 23116: 0.0235825, expected -0.128586\n",
      "E0103 19:22:19.404494  151466 buffer_comparator.cc:157] Difference at 26084: -0.164776, expected 0.0703011\n",
      "E0103 19:22:19.404501  151466 buffer_comparator.cc:157] Difference at 27192: -0.0297012, expected -0.151157\n",
      "E0103 19:22:19.404508  151466 buffer_comparator.cc:157] Difference at 30096: -0.210636, expected -0.355889\n",
      "2025-01-03 19:22:19.404510: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.405341  151466 buffer_comparator.cc:157] Difference at 1417: -0.0245972, expected -0.18261\n",
      "E0103 19:22:19.405350  151466 buffer_comparator.cc:157] Difference at 1508: 0.344833, expected 0.513586\n",
      "E0103 19:22:19.405354  151466 buffer_comparator.cc:157] Difference at 2581: -0.593285, expected -0.810047\n",
      "E0103 19:22:19.405361  151466 buffer_comparator.cc:157] Difference at 5564: -0.126305, expected -0.26074\n",
      "E0103 19:22:19.405364  151466 buffer_comparator.cc:157] Difference at 6406: 0.365238, expected 0.127136\n",
      "E0103 19:22:19.405367  151466 buffer_comparator.cc:157] Difference at 7411: 0.272558, expected 0.135361\n",
      "E0103 19:22:19.405398  151466 buffer_comparator.cc:157] Difference at 23116: 0.0235977, expected -0.128586\n",
      "E0103 19:22:19.405406  151466 buffer_comparator.cc:157] Difference at 26084: -0.164879, expected 0.0703011\n",
      "E0103 19:22:19.405412  151466 buffer_comparator.cc:157] Difference at 27192: -0.02948, expected -0.151157\n",
      "E0103 19:22:19.405421  151466 buffer_comparator.cc:157] Difference at 30096: -0.210617, expected -0.355889\n",
      "2025-01-03 19:22:19.405424: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.406590  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.406600  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.406604  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.406611  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.406614  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.406617  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.406648  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.406657  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.406664  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.406671  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.406674: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.407476  151466 buffer_comparator.cc:157] Difference at 1417: -0.0245552, expected -0.18261\n",
      "E0103 19:22:19.407489  151466 buffer_comparator.cc:157] Difference at 1508: 0.344917, expected 0.513586\n",
      "E0103 19:22:19.407495  151466 buffer_comparator.cc:157] Difference at 2581: -0.593309, expected -0.810047\n",
      "E0103 19:22:19.407502  151466 buffer_comparator.cc:157] Difference at 5564: -0.126198, expected -0.26074\n",
      "E0103 19:22:19.407505  151466 buffer_comparator.cc:157] Difference at 6406: 0.365433, expected 0.127136\n",
      "E0103 19:22:19.407509  151466 buffer_comparator.cc:157] Difference at 7411: 0.272489, expected 0.135361\n",
      "E0103 19:22:19.407540  151466 buffer_comparator.cc:157] Difference at 23116: 0.0235825, expected -0.128586\n",
      "E0103 19:22:19.407547  151466 buffer_comparator.cc:157] Difference at 26084: -0.164776, expected 0.0703011\n",
      "E0103 19:22:19.407550  151466 buffer_comparator.cc:157] Difference at 27192: -0.0297012, expected -0.151157\n",
      "E0103 19:22:19.407557  151466 buffer_comparator.cc:157] Difference at 30096: -0.210636, expected -0.355889\n",
      "2025-01-03 19:22:19.407559: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.408380  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.408391  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.408394  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.408402  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.408405  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.408408  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.408443  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.408452  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.408455  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.408462  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.408465: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.409338  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.409348  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.409352  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.409359  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.409362  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.409365  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.409402  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.409410  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.409413  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.409420  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.409422: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.410441  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.410451  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.410454  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.410462  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.410465  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.410468  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.410504  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.410512  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.410515  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.410522  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.410524: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.411762  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.411774  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.411777  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.411784  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.411787  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.411791  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.411827  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.411834  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.411838  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.411844  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.411847: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.412789  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.412800  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.412804  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.412811  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.412814  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.412817  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.412853  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.412860  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.412864  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.412870  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.412873: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.413966  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.413976  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.413979  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.413986  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.413989  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.413992  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.414027  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.414037  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.414040  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.414047  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.414049: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.415077  151466 buffer_comparator.cc:157] Difference at 1417: -0.0246429, expected -0.18261\n",
      "E0103 19:22:19.415087  151466 buffer_comparator.cc:157] Difference at 1508: 0.344866, expected 0.513586\n",
      "E0103 19:22:19.415091  151466 buffer_comparator.cc:157] Difference at 2581: -0.59333, expected -0.810047\n",
      "E0103 19:22:19.415098  151466 buffer_comparator.cc:157] Difference at 5564: -0.126446, expected -0.26074\n",
      "E0103 19:22:19.415101  151466 buffer_comparator.cc:157] Difference at 6406: 0.365242, expected 0.127136\n",
      "E0103 19:22:19.415104  151466 buffer_comparator.cc:157] Difference at 7411: 0.272873, expected 0.135361\n",
      "E0103 19:22:19.415141  151466 buffer_comparator.cc:157] Difference at 23116: 0.0237923, expected -0.128586\n",
      "E0103 19:22:19.415149  151466 buffer_comparator.cc:157] Difference at 26084: -0.164783, expected 0.0703011\n",
      "E0103 19:22:19.415153  151466 buffer_comparator.cc:157] Difference at 27192: -0.0294781, expected -0.151157\n",
      "E0103 19:22:19.415160  151466 buffer_comparator.cc:157] Difference at 30096: -0.210523, expected -0.355889\n",
      "2025-01-03 19:22:19.415162: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n",
      "E0103 19:22:19.416744  151466 buffer_comparator.cc:157] Difference at 1417: -0.0243073, expected -0.18261\n",
      "E0103 19:22:19.416756  151466 buffer_comparator.cc:157] Difference at 1508: 0.344694, expected 0.513586\n",
      "E0103 19:22:19.416759  151466 buffer_comparator.cc:157] Difference at 2581: -0.593427, expected -0.810047\n",
      "E0103 19:22:19.416767  151466 buffer_comparator.cc:157] Difference at 5564: -0.126866, expected -0.26074\n",
      "E0103 19:22:19.416770  151466 buffer_comparator.cc:157] Difference at 6406: 0.364695, expected 0.127136\n",
      "E0103 19:22:19.416773  151466 buffer_comparator.cc:157] Difference at 7411: 0.273155, expected 0.135361\n",
      "E0103 19:22:19.416806  151466 buffer_comparator.cc:157] Difference at 23116: 0.024056, expected -0.128586\n",
      "E0103 19:22:19.416814  151466 buffer_comparator.cc:157] Difference at 26084: -0.16444, expected 0.0703011\n",
      "E0103 19:22:19.416819  151466 buffer_comparator.cc:157] Difference at 27192: -0.0291781, expected -0.151157\n",
      "E0103 19:22:19.416831  151466 buffer_comparator.cc:157] Difference at 30096: -0.210361, expected -0.355889\n",
      "2025-01-03 19:22:19.416833: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 400 is 0.5147427916526794.\n",
      "Loss at step 800 is 0.35230839252471924.\n",
      "Loss at step 1200 is 0.3373890817165375.\n",
      "Loss at step 1600 is 0.28910431265830994.\n",
      "Loss at step 2000 is 0.2636910378932953.\n",
      "Loss at step 2400 is 0.22495049238204956.\n",
      "Loss at step 2800 is 0.2407882958650589.\n",
      "Loss at step 3200 is 0.22234590351581573.\n",
      "Loss at step 3600 is 0.20175431668758392.\n",
      "Loss at step 4000 is 0.22914260625839233.\n",
      "Loss at step 4400 is 0.20094959437847137.\n",
      "Loss at step 4800 is 0.21461221575737.\n",
      "Loss at step 5200 is 0.2006271332502365.\n",
      "Loss at step 5600 is 0.17052799463272095.\n",
      "Loss at step 6000 is 0.18510664999485016.\n",
      "Loss at step 6400 is 0.16419947147369385.\n",
      "Loss at step 6800 is 0.18627332150936127.\n",
      "Loss at step 7200 is 0.1719377040863037.\n",
      "Loss at step 7600 is 0.15777072310447693.\n",
      "Loss at step 8000 is 0.16198229789733887.\n",
      "Loss at step 8400 is 0.16666197776794434.\n",
      "Loss at step 8800 is 0.16124750673770905.\n",
      "Loss at step 9200 is 0.15172451734542847.\n",
      "Loss at step 9600 is 0.164412260055542.\n",
      "Loss at step 10000 is 0.1568562239408493.\n",
      "Loss at step 10400 is 0.16067136824131012.\n",
      "Loss at step 10800 is 0.15216176211833954.\n",
      "Loss at step 11200 is 0.14607229828834534.\n",
      "Loss at step 11600 is 0.12980681657791138.\n",
      "Loss at step 12000 is 0.1461191177368164.\n",
      "Loss at step 12400 is 0.15107852220535278.\n",
      "Loss at step 12800 is 0.12069764733314514.\n",
      "Loss at step 13200 is 0.143290176987648.\n",
      "Loss at step 13600 is 0.14631891250610352.\n",
      "Loss at step 14000 is 0.14653263986110687.\n",
      "Loss at step 14400 is 0.13925059139728546.\n",
      "Loss at step 14800 is 0.1312694549560547.\n",
      "Loss at step 15200 is 0.14084474742412567.\n",
      "Loss at step 15600 is 0.1399371325969696.\n",
      "Loss at step 16000 is 0.13037213683128357.\n",
      "Loss at step 16400 is 0.11590937525033951.\n",
      "Loss at step 16800 is 0.11307074129581451.\n",
      "Loss at step 17200 is 0.12772636115550995.\n",
      "Loss at step 17600 is 0.12145859748125076.\n",
      "Loss at step 18000 is 0.12297841906547546.\n",
      "Loss at step 18400 is 0.12507475912570953.\n",
      "Loss at step 18800 is 0.11139694601297379.\n",
      "Loss at step 19200 is 0.12887728214263916.\n",
      "Loss at step 19600 is 0.14156833291053772.\n",
      "Loss at step 20000 is 0.13330085575580597.\n",
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 20000 in final state after training.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5466e9158ae74c039bab6c88245d4ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='3.835 MB of 3.835 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MSE_on_fixed_grid</td><td>█▆▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>batch_within_epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▂▂▂▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▇█▇▇▆▄▅▅▄▄▃▄▄▄▄▄▃▃▃▃▂▂▃▁▁▂▂▃▂▂▁▂▂▁▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MSE_on_fixed_grid</td><td>0.01807</td></tr><tr><td>batch_within_epoch</td><td>20000</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>loss</td><td>0.1333</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">light-vortex-96</strong> at: <a href='https://wandb.ai/nld/inr_edu_24/runs/fpkgzkuc' target=\"_blank\">https://wandb.ai/nld/inr_edu_24/runs/fpkgzkuc</a><br/> View project at: <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">https://wandb.ai/nld/inr_edu_24</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 50 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250103_192213-fpkgzkuc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# and we run the experiment while logging things to wandb\n",
    "with wandb.init(\n",
    "    project='inr_edu_24',\n",
    "    notes='test',\n",
    "    tags=['test']\n",
    ") as run:\n",
    "    results = experiment.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      layers=(\n",
       "        CountingIdentity(\n",
       "          _embedding_matrix=i32[],\n",
       "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,2],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "      )\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr, losses, optimizer_state, state, loss_evaluator, additional_output = results\n",
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 20000 in final state after training.\n"
     ]
    }
   ],
   "source": [
    "from state_test_objects import after_training_callback, CountingIdentity\n",
    "after_training_callback(losses, inr, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountingIdentity(\n",
       "  _embedding_matrix=i32[],\n",
       "  state_index=StateIndex(marker=0, init=_Sentinel())\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr.terms[0].layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(inr.terms[0].layers[0], CountingIdentity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_temp_module.CountingIdentity"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inr.terms[0].layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (inr_edu_24)",
   "language": "python",
   "name": "inr_edu_24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
