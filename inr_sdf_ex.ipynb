{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of training an INR locally\n",
    "This notebook provides an example of how to create an INR and train it locally using the tools in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:22.968569596Z",
     "start_time": "2025-01-06T14:55:22.925237450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxwell_litsios\u001b[0m (\u001b[33mbep-circle\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:23.553170506Z",
     "start_time": "2025-01-06T14:55:23.447043302Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\n",
    "\n",
    "To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \"CombinedINR\"` instead of `config[\"model_type\"] = \"CombinedINR\"`). You can also just use a dictionary instead.\n",
    "\n",
    "Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\n",
    "\n",
    "Doing this in a config instead of hard coded might seem like extra work, but consider this:\n",
    "1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \n",
    "   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\n",
    "2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\n",
    "   and then run an experiment specified by that config on any machine you want, e.g. on Snellius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:24.410408058Z",
     "start_time": "2025-01-06T14:55:24.393999841Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 3\n",
    "config.model_config.out_size = 1\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 512,\n",
    "        'num_layers': 3,\n",
    "        'layer_type': 'inr_layers.SirenLayer',\n",
    "        'num_splits': 3,\n",
    "        'activation_kwargs': {'w0':12.},#{'inverse_scale': 5.},\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        'initialization_scheme_kwargs': {'w0': 12.},\n",
    "        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "config.trainer_type = 'training.train_inr'\n",
    "config.loss_evaluator = 'losses.SDFLossEvaluator' # see NeRFLossEvaluator in losses.py make that for SDF\n",
    "# config.loss_evaluator = ('losses.SDFLossEvaluator', {  # Change to use the class instead of direct function\n",
    "#     'target_function': 'sdf.occupancySDF',\n",
    "#     'target_function_config': {\n",
    "#         'mesh_path': './example_data/Armadillo.ply',\n",
    "#         'keep_aspect_ratio': False,\n",
    "#     },\n",
    "#     'loss_function': 'losses.sdf_loss'\n",
    "# })\n",
    "\n",
    "config.target_function = 'sdf.occupancySDF' #see when config. losseval\n",
    "config.target_function_config = {\n",
    "    'mesh_name': 'Armadillo',\n",
    "    'keep_aspect_ratio': False,\n",
    "} # there will be a similar one for sdf\n",
    "config.loss_function = 'losses.sdf_loss'\n",
    "config.state_update_function = ('state_test_objects.py', 'counter_updater')\n",
    "config.sampler = ('sampling.SDFSampler',{  # samples coordinates in a fixed grid, that should in this case coincide with the pixel locations in the image\n",
    "    # 'sdf_path': \"xyzrgb_statuette\",\n",
    "    'sdf_name': \"Armadillo\",\n",
    "    'batch_size': 256,\n",
    "    'num_dims': 3,\n",
    "    'keep_aspect_ratio': False,\n",
    "})\n",
    "\n",
    "config.optimizer = 'adam'  # we'll have to add optax to the additional default modules later\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 1.5e-4\n",
    "}\n",
    "config.steps = 20000 #changed from 40000\n",
    "config.use_wandb = True\n",
    "\n",
    "# now we want some extra things, like logging, to happen during training\n",
    "# the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# The callbacks we want to use can be found in inr_utils.callbacks\n",
    "config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "config.after_step_callback_config = {\n",
    "    'callbacks':[\n",
    "        ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\n",
    "        'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "        ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "             {'metric_collector':'metrics.MetricCollector'}\n",
    "        ),\n",
    "        'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "    ],\n",
    "    'show_logs': False\n",
    "}\n",
    "\n",
    "config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\n",
    "\n",
    "config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "    'metrics':[\n",
    "        ('metrics.JaccardIndexSDF', {\n",
    "            'frequency':'every_n_batches',\n",
    "            'grid': 100,\n",
    "            'batch_size': 100,\n",
    "            'num_dims': 3,\n",
    "        }),  \n",
    "\n",
    "    ],\n",
    "    'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "    'epoch_frequency': 1  # not actually used\n",
    "}\n",
    "\n",
    "#config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "config.optimizer_state = None  # we're starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T13:11:48.290828912Z",
     "start_time": "2024-11-27T13:11:48.207796154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      layers=(\n",
       "        CountingIdentity(\n",
       "          _embedding_matrix=f32[3],\n",
       "          state_index=StateIndex(\n",
       "            marker=<object object at 0x7f0d6ac67b70>,\n",
       "            init=i32[]\n",
       "          )\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[512,3],\n",
       "          biases=f32[512],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        Linear(weights=f32[1,512], biases=f32[1], activation_kwargs={})\n",
       "      )\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that it works properly\n",
    "try:\n",
    "    inr(jnp.zeros(3))\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostponedInitialization(cls=train_inr, kwargs={'loss_evaluator': PostponedInitialization(cls=SDFLossEvaluator, kwargs={'state_update_function': <function counter_updater at 0x7f0d68310670>}, missing_args=[]), 'sampler': PostponedInitialization(cls=SDFSampler, kwargs={'sdf_name': 'Armadillo', 'batch_size': 256, 'keep_aspect_ratio': False}, missing_args=[]), 'optimizer': PostponedInitialization(cls=adam, kwargs={'learning_rate': 0.00015, 'b1': 0.9, 'b2': 0.999, 'eps': 1e-08, 'eps_root': 0.0, 'mu_dtype': None, 'nesterov': False}, missing_args=[]), 'steps': 20000, 'use_wandb': True, 'after_step_callback': PostponedInitialization(cls=ComposedCallback, kwargs={'callbacks': [functools.partial(<function print_loss at 0x7f0d041211b0>, after_every=400), <function report_loss at 0x7f0d04122b90>, PostponedInitialization(cls=MetricCollectingCallback, kwargs={'metric_collector': PostponedInitialization(cls=MetricCollector, kwargs={'metrics': [PostponedInitialization(cls=JaccardIndexSDF, kwargs={'grid': 100, 'batch_size': 100, 'frequency': 'every_n_batches', 'num_dims': 3, 'target_function': PostponedInitialization(cls=occupancySDF, kwargs={'mesh_name': 'Armadillo', 'keep_aspect_ratio': False}, missing_args=[])}, missing_args=[])], 'batch_frequency': 400, 'epoch_frequency': 1}, missing_args=[])}, missing_args=[]), <function raise_error_on_nan at 0x7f0d04122d40>], 'show_logs': False, 'use_wandb': True, 'display_func': <function pprint at 0x7f0dc2d11900>}, missing_args=[]), 'after_training_callback': <function after_training_callback at 0x7f0d68310a60>, 'optimizer_state': None, 'state_initialization_function': <function initialize_state at 0x7f0d041232e0>, 'state': None, 'inr': PostponedInitialization(cls=CombinedINR, kwargs={'terms': [PostponedInitialization(cls=from_config, kwargs={'hidden_size': 512, 'num_layers': 3, 'layer_type': <class 'model_components.inr_layers.SirenLayer'>, 'activation_kwargs': {'w0': 12.0}, 'initialization_scheme': <function siren_scheme at 0x7f0d682a7b50>, 'initialization_scheme_kwargs': {'w0': 12.0}, 'positional_encoding_layer': PostponedInitialization(cls=CountingIdentity, kwargs={}, missing_args=[]), 'num_splits': 3, 'post_processor': None, 'in_size': 3, 'out_size': 1, 'key': Array([3941762975, 2946414623], dtype=uint32)}, missing_args=[])], 'post_processor': <function real_part at 0x7f0d6861bb50>}, missing_args=[]), 'key': Array([3194637447, 3504528721], dtype=uint32)}, missing_args=[])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ovindar/PycharmProjects/INR_BEP/wandb/run-20250129_124031-jwkvgbzd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/bep-circle/inr_edu_24/runs/jwkvgbzd' target=\"_blank\">rosy-hill-89</a></strong> to <a href='https://wandb.ai/bep-circle/inr_edu_24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/bep-circle/inr_edu_24' target=\"_blank\">https://wandb.ai/bep-circle/inr_edu_24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/bep-circle/inr_edu_24/runs/jwkvgbzd' target=\"_blank\">https://wandb.ai/bep-circle/inr_edu_24/runs/jwkvgbzd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_12081/4160894630.py\", line 7, in <module>\n",
      "    results = experiment.initialize()\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/common_dl_utils/config_realization.py\", line 195, in initialize\n",
      "    return cls(**processed_self_kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/inr_utils/training.py\", line 210, in train_inr\n",
      "    inr, optimizer_state, loss, state = train_step(inr, optimizer_state, next(key_gen), state)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/equinox/_jit.py\", line 275, in __call__\n",
      "    return self._call(False, args, kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/equinox/_module.py\", line 1096, in __call__\n",
      "    return self.__func__(self.__self__, *args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/equinox/_jit.py\", line 244, in _call\n",
      "    marker, _, _ = out = self._cached(\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/pjit.py\", line 340, in cache_miss\n",
      "    pgle_profiler) = _python_pjit_helper(fun, jit_info, *args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/pjit.py\", line 180, in _python_pjit_helper\n",
      "    p, args_flat = _infer_params(fun, jit_info, args, kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/pjit.py\", line 740, in _infer_params\n",
      "    p, args_flat = _infer_params_impl(\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/pjit.py\", line 629, in _infer_params_impl\n",
      "    jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 349, in memoized_fun\n",
      "    ans = call(fun, *args)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/pjit.py\", line 1310, in _create_pjit_jaxpr\n",
      "    jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/profiler.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2159, in trace_to_jaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 192, in call_wrapped\n",
      "    return self.f_transformed(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api_util.py\", line 292, in _argnums_partial\n",
      "    return _fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api_util.py\", line 72, in flatten_fun\n",
      "    ans = f(*py_args, **py_kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api_util.py\", line 318, in _argnames_partial\n",
      "    return _fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api_util.py\", line 292, in _argnums_partial\n",
      "    return _fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api_util.py\", line 652, in result_paths\n",
      "    ans = _fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/equinox/_jit.py\", line 55, in fun_wrapped\n",
      "    out = fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/inr_utils/training.py\", line 68, in train_step\n",
      "    (loss, state), grad = eqx.filter_value_and_grad(loss_evaluator, has_aux=True)(inr, locations, state)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/equinox/_ad.py\", line 81, in __call__\n",
      "    return fun_value_and_grad(diff_x, nondiff_x, *args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api.py\", line 470, in value_and_grad_f\n",
      "    ans, vjp_py, aux = _vjp(\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api.py\", line 1979, in _vjp\n",
      "    out_primals, vjp, aux = ad.vjp(flat_fun, primals_flat, has_aux=True)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/ad.py\", line 254, in vjp\n",
      "    out_primals, pvals, jaxpr, consts, aux = linearize(traceable, *primals, has_aux=True)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/ad.py\", line 237, in linearize\n",
      "    jaxpr, out_pvals, consts = pe.trace_to_jaxpr_nounits(jvpfun_flat, in_pvals)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/profiler.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 574, in trace_to_jaxpr_nounits\n",
      "    jaxpr, (out_pvals, consts, env) = fun.call_wrapped(pvals)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 192, in call_wrapped\n",
      "    return self.f_transformed(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 587, in trace_to_subjaxpr_nounits\n",
      "    out_tracers, jaxpr, out_consts, env = _trace_to_subjaxpr_nounits(\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 616, in _trace_to_subjaxpr_nounits\n",
      "    ans = f(*in_args)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api_util.py\", line 72, in flatten_fun\n",
      "    ans = f(*py_args, **py_kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/ad.py\", line 78, in jvpfun\n",
      "    out_primals, out_tangents = f(tag, primals, tangents)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/ad.py\", line 124, in jvp_subtrace_aux\n",
      "    ans, aux = f(*(map(partial(maybe_jvp_tracer, trace), primals, tangents)))\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api_util.py\", line 126, in flatten_fun_nokwargs2\n",
      "    pair = f(*py_args)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api_util.py\", line 292, in _argnums_partial\n",
      "    return _fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/equinox/_ad.py\", line 64, in fun_value_and_grad\n",
      "    return self._fun(_x, *_args, **_kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/inr_utils/losses.py\", line 218, in __call__\n",
      "    values_and_grads = jax.vmap(get_val_and_grad, in_axes=(None, 0))(inr, coords)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api.py\", line 999, in vmap_f\n",
      "    out_flat = batching.batch(\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 192, in call_wrapped\n",
      "    return self.f_transformed(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/batching.py\", line 589, in _batch_outer\n",
      "    outs, trace = f(tag, in_dims, *in_vals)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/batching.py\", line 604, in _batch_inner\n",
      "    outs = f(*in_tracers)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/interpreters/batching.py\", line 333, in flatten_fun_for_vmap\n",
      "    ans = f(*py_args, **py_kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api.py\", line 466, in value_and_grad_f\n",
      "    _check_input_dtype_grad(holomorphic, allow_int, leaf)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/api.py\", line 497, in _check_input_dtype_revderiv\n",
      "    dispatch.check_arg(x)\n",
      "  File \"/home/ovindar/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py\", line 285, in check_arg\n",
      "    raise TypeError(f\"Argument '{arg}' of type {type(arg)} is not a valid \"\n",
      "TypeError: Argument '<function real_part at 0x7f0d6861bb50>' of type <class 'function'> is not a valid JAX type.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rosy-hill-89</strong> at: <a href='https://wandb.ai/bep-circle/inr_edu_24/runs/jwkvgbzd' target=\"_blank\">https://wandb.ai/bep-circle/inr_edu_24/runs/jwkvgbzd</a><br> View project at: <a href='https://wandb.ai/bep-circle/inr_edu_24' target=\"_blank\">https://wandb.ai/bep-circle/inr_edu_24</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250129_124031-jwkvgbzd/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "Argument '<function real_part at 0x7f0d6861bb50>' of type <class 'function'> is not a valid JAX type.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# and we run the experiment while logging things to wandb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m      3\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minr_edu_24\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m     notes\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     tags\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      6\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m run:\n\u001b[0;32m----> 7\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/common_dl_utils/config_realization.py:195\u001b[0m, in \u001b[0;36mPostponedInitialization.initialize\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    193\u001b[0m processed_self_kwargs\u001b[38;5;241m.\u001b[39mupdate(processed_kwargs)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m _internal_utils\u001b[38;5;241m.\u001b[39msignature_to_var_keyword(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_wrapping \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls\n\u001b[0;32m--> 195\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprocessed_self_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/INR_BEP/inr_utils/training.py:210\u001b[0m, in \u001b[0;36mtrain_inr\u001b[0;34m(inr, loss_evaluator, sampler, optimizer, steps, key, use_wandb, after_step_callback, after_training_callback, optimizer_state, state_initialization_function, state)\u001b[0m\n\u001b[1;32m    207\u001b[0m losses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty(steps, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(steps):  \u001b[38;5;66;03m# the actual training loop\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     inr, optimizer_state, loss, state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mkey_gen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m     losses[step] \u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m    212\u001b[0m     after_step_callback(step, loss, inr, state, optimizer_state)\n",
      "    \u001b[0;31m[... skipping hidden 19 frame]\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/INR_BEP/inr_utils/training.py:68\u001b[0m, in \u001b[0;36mmake_inr_train_step_function.<locals>.train_step\u001b[0;34m(inr, optimizer_state, key, state)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"take one train step with the inr model and optimizer\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m:param inr: inr model to be optimized\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;124;03mNB this function is decorated with eqx.filter_jit\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m locations \u001b[38;5;241m=\u001b[39m sampler(key)  \u001b[38;5;66;03m# think of an array of coordinates, although this can in general be any PyTree containing the relevant information for the loss_evaluator\u001b[39;00m\n\u001b[0;32m---> 68\u001b[0m (loss, state), grad \u001b[38;5;241m=\u001b[39m \u001b[43meqx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfilter_value_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_evaluator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m grad \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mlambda\u001b[39;00m x: jnp\u001b[38;5;241m.\u001b[39mconjugate(x) \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39miscomplexobj(x) \u001b[38;5;28;01melse\u001b[39;00m x,\n\u001b[1;32m     71\u001b[0m     grad\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     73\u001b[0m updates, optimizer_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grad, optimizer_state, inr)\n",
      "    \u001b[0;31m[... skipping hidden 17 frame]\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/INR_BEP/inr_utils/losses.py:218\u001b[0m, in \u001b[0;36mSDFLossEvaluator.__call__\u001b[0;34m(self, inr, batch, state)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inr(coords)\n\u001b[1;32m    217\u001b[0m get_val_and_grad \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvalue_and_grad(get_values)\n\u001b[0;32m--> 218\u001b[0m values_and_grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_val_and_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m pred_val \u001b[38;5;241m=\u001b[39m values_and_grads[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    221\u001b[0m grad_val \u001b[38;5;241m=\u001b[39m values_and_grads[\u001b[38;5;241m1\u001b[39m]\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/PycharmProjects/INR_BEP/.venv/lib/python3.10/site-packages/jax/_src/dispatch.py:285\u001b[0m, in \u001b[0;36mcheck_arg\u001b[0;34m(arg)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcheck_arg\u001b[39m(arg: Any):\n\u001b[1;32m    284\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(arg, core\u001b[38;5;241m.\u001b[39mTracer) \u001b[38;5;129;01mor\u001b[39;00m core\u001b[38;5;241m.\u001b[39mvalid_jaxtype(arg)):\n\u001b[0;32m--> 285\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m of type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(arg)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJAX type.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument '<function real_part at 0x7f0d6861bb50>' of type <class 'function'> is not a valid JAX type."
     ]
    }
   ],
   "source": [
    "# and we run the experiment while logging things to wandb\n",
    "with wandb.init(\n",
    "    project='inr_edu_24',\n",
    "    notes='test',\n",
    "    tags=['test']\n",
    ") as run:\n",
    "    results = experiment.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      layers=(\n",
       "        CountingIdentity(\n",
       "          _embedding_matrix=i32[],\n",
       "          state_index=StateIndex(marker=0, init=_Sentinel())\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,2],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        SirenLayer(\n",
       "          weights=f32[256,256],\n",
       "          biases=f32[256],\n",
       "          activation_kwargs={'w0': 12.0}\n",
       "        ),\n",
       "        Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={})\n",
       "      )\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=<function real_part>\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr, losses, optimizer_state, state, loss_evaluator, additional_output = results\n",
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking model and state for CountingIdentity layers\n",
      "Found a CountingIdentity layer with counter value 20000 in final state after training.\n"
     ]
    }
   ],
   "source": [
    "from state_test_objects import after_training_callback, CountingIdentity\n",
    "after_training_callback(losses, inr, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountingIdentity(\n",
       "  _embedding_matrix=i32[],\n",
       "  state_index=StateIndex(marker=0, init=_Sentinel())\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr.terms[0].layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(inr.terms[0].layers[0], CountingIdentity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_temp_module.CountingIdentity"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(inr.terms[0].layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
