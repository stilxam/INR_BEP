{
 "cells": [
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Example of training an INR locally\\n\",\n",
    "    \"This notebook provides an example of how to create an INR and train it locally using the tools in this repo.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:10.214485557Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:07.978826788Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"\\u001b[34m\\u001b[1mwandb\\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\\n\",\n",
    "      \"\\u001b[34m\\u001b[1mwandb\\u001b[0m: Currently logged in as: \\u001b[33msimon-martinus-koop\\u001b[0m (\\u001b[33mnld\\u001b[0m). Use \\u001b[1m`wandb login --relogin`\\u001b[0m to force relogin\\n\",\n",
    "      \"2025-01-30 18:07:20.883818: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import pdb\\n\",\n",
    "    \"import traceback\\n\",\n",
    "    \"\\n\",\n",
    "    \"import jax\\n\",\n",
    "    \"from jax import numpy as jnp\\n\",\n",
    "    \"import optax\\n\",\n",
    "    \"import wandb\\n\",\n",
    "    \"\\n\",\n",
    "    \"from common_dl_utils.config_creation import Config\\n\",\n",
    "    \"import common_jax_utils as cju\\n\",\n",
    "    \"\\n\",\n",
    "    \"wandb.login()\\n\",\n",
    "    \"\\n\",\n",
    "    \"key = jax.random.PRNGKey(12398)\\n\",\n",
    "    \"key_gen = cju.key_generator(key)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 1,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:10.214753212Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:10.185174903Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\\n\",\n",
    "    \"\\n\",\n",
    "    \"To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \\\"CombinedINR\\\"` instead of `config[\\\"model_type\\\"] = \\\"CombinedINR\\\"`). You can also just use a dictionary instead.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\\n\",\n",
    "    \"\\n\",\n",
    "    \"Doing this in a config instead of hard coded might seem like extra work, but consider this:\\n\",\n",
    "    \"1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \\n\",\n",
    "    \"   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\\n\",\n",
    "    \"2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\\n\",\n",
    "    \"   and then run an experiment specified by that config on any machine you want, e.g. on Snellius\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 2,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:10.214921477Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:10.187940879Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"config = Config()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# first we specify what the model should look like\\n\",\n",
    "    \"config.architecture = './model_components'  # module containing all relevant classes for architectures\\n\",\n",
    "    \"# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\\n\",\n",
    "    \"# let config.architecture be the module that contains the \\\"main\\\" model class, and for all other components just specify the module\\n\",\n",
    "    \"# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\\n\",\n",
    "    \"config.model_type = 'inr_modules.CombinedINR'\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.model_config = Config()\\n\",\n",
    "    \"config.model_config.in_size = 3\\n\",\n",
    "    \"config.model_config.out_size = 1\\n\",\n",
    "    \"config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\\n\",\n",
    "    \"    ('inr_modules.MLPINR.from_config',{\\n\",\n",
    "    \"        'hidden_size': 256,\\n\",\n",
    "    \"        'num_layers': 5,\\n\",\n",
    "    \"        'layer_type': 'inr_layers.SirenLayer',\\n\",\n",
    "    \"        'num_splits': 3,\\n\",\n",
    "    \"        'activation_kwargs': {'w0':12.},#{'inverse_scale': 5.},\\n\",\n",
    "    \"        # 'initialization_scheme':'initialization_schemes.siren_scheme',\\n\",\n",
    "    \"        # 'initialization_scheme_kwargs': {'w0': 25.},\\n\",\n",
    "    \"        'positional_encoding_layer': ('state_test_objects.py', 'CountingIdentity'),\\n\",\n",
    "    \"        # 'post_processor': 'model_components.auxiliary.squeeze_array',\\n\",\n",
    "    \"    }),\\n\",\n",
    "    \"    # ('inr_modules.MLPINR.from_config',{\\n\",\n",
    "    \"    #     'hidden_size': 1024,\\n\",\n",
    "    \"    #     'num_layers': 2,\\n\",\n",
    "    \"    #     'num_splits': 1,\\n\",\n",
    "    \"    #     'layer_type': 'inr_layers.GaussianINRLayer',\\n\",\n",
    "    \"    #     'use_complex': False,\\n\",\n",
    "    \"    #     'activation_kwargs': {'inverse_scale': 1},\\n\",\n",
    "    \"    # })\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"# next, we set up the training loop, including the 'target_function' that we want to mimic\\n\",\n",
    "    \"config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\\n\",\n",
    "    \"config.trainer_type = 'training.train_with_dataloader_scan'\\n\",\n",
    "    \"# config.trainer_type = 'training.train_inr_with_dataloader'\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.dataloader = 'sdf.SDFDataLoader'\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.dataloader_config = {\\n\",\n",
    "    \"    \\\"sdf_name\\\": \\\"Armadillo\\\",\\n\",\n",
    "    \"    \\\"batch_size\\\": 20000,\\n\",\n",
    "    \"    \\\"keep_aspect_ratio\\\":True\\n\",\n",
    "    \"\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.num_cycles = 40\\n\",\n",
    "    \"config.steps_per_cycle = 200\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.loss_evaluator = \\\"losses.SDFLossEvaluator\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.target_function = 'sdf.SDFDataLoader' #see when config. losseval\\n\",\n",
    "    \"config.target_function_config = {\\n\",\n",
    "    \"    \\\"sdf_name\\\": \\\"Armadillo\\\",\\n\",\n",
    "    \"    \\\"batch_size\\\": 10000,\\n\",\n",
    "    \"    \\\"keep_aspect_ratio\\\":True,\\n\",\n",
    "    \"\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.state_update_function = ('state_test_objects.py', 'counter_updater')\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.optimizer = 'training.OptimizerFactory.single_optimizer'#'adamw'  # we'll have to add optax to the additional default modules later\\n\",\n",
    "    \"config.optimizer_type = 'adamw'\\n\",\n",
    "    \"config.optimizer_config = {\\n\",\n",
    "    \"    'learning_rate': 1e-5,#1.5e-4\\n\",\n",
    "    \"    'weight_decay': 1e-4,\\n\",\n",
    "    \"}\\n\",\n",
    "    \"config.optimizer_mask = 'masking.array_mask'\\n\",\n",
    "    \"config.steps = 1200 #changed from 40000\\n\",\n",
    "    \"config.use_wandb = True\\n\",\n",
    "    \"\\n\",\n",
    "    \"# now we want some extra things, like logging, to happen during training\\n\",\n",
    "    \"# the inr_utils.training.train_inr function allows for this through callbacks.\\n\",\n",
    "    \"# The callbacks we want to use can be found in inr_utils.callbacks\\n\",\n",
    "    \"config.after_cycle_callback = 'callbacks.ComposedCallback'\\n\",\n",
    "    \"config.after_cycle_callback_config = {\\n\",\n",
    "    \"    'callbacks':[\\n\",\n",
    "    \"        ('callbacks.print_loss', {'after_every':1}),  # only print the loss every 400th step\\n\",\n",
    "    \"        'callbacks.report_loss',  # but log the loss to wandb after every step\\n\",\n",
    "    \"        ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\\n\",\n",
    "    \"             {'metric_collector':'metrics.MetricCollector'}\\n\",\n",
    "    \"        ),\\n\",\n",
    "    \"        'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'show_logs': False\\n\",\n",
    "    \"}\\n\",\n",
    "    \"# config.after_step_callback = 'callbacks.ComposedCallback'\\n\",\n",
    "    \"# config.after_step_callback_config = {\\n\",\n",
    "    \"#     'callbacks':[\\n\",\n",
    "    \"#         ('callbacks.print_loss', {'after_every':400}),  # only print the loss every 400th step\\n\",\n",
    "    \"#         'callbacks.report_loss',  # but log the loss to wandb after every step\\n\",\n",
    "    \"#         ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\\n\",\n",
    "    \"#              {'metric_collector':'metrics.MetricCollector'}\\n\",\n",
    "    \"#         ),\\n\",\n",
    "    \"#         'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\\n\",\n",
    "    \"#     ],\\n\",\n",
    "    \"#     'show_logs': False\\n\",\n",
    "    \"# }\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\\n\",\n",
    "    \"\\n\",\n",
    "    \"config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\\n\",\n",
    "    \"    'metrics':[\\n\",\n",
    "    \"        ('metrics.JaccardIndexSDF', {\\n\",\n",
    "    \"            'frequency':'every_n_batches',\\n\",\n",
    "    \"            'grid_resolution': 100,\\n\",\n",
    "    \"            'num_dims': 3,\\n\",\n",
    "    \"            'batch_size': 10000\\n\",\n",
    "    \"        }),\\n\",\n",
    "    \"        (\\\"metrics.SDFReconstructor\\\",\\n\",\n",
    "    \"         {\\n\",\n",
    "    \"            'frequency':'every_n_batches',\\n\",\n",
    "    \"            'grid_resolution': 100,\\n\",\n",
    "    \"            'batch_size': 10000,\\n\",\n",
    "    \"        }),\\n\",\n",
    "    \"        # todo add view rendering here\\n\",\n",
    "    \"\\n\",\n",
    "    \"    ],\\n\",\n",
    "    \"    'batch_frequency': 4,  # compute all of these metrics every 400 batches\\n\",\n",
    "    \"    'epoch_frequency': 1  # not actually used\\n\",\n",
    "    \"}\\n\",\n",
    "    \"\\n\",\n",
    "    \"#config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \\n\",\n",
    "    \"config.optimizer_state = None  # we're starting from scratch\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 3,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:11.464582941Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:10.197258003Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# let's first see if we get the correct model\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    inr = cju.run_utils.get_model_from_config_and_key(\\n\",\n",
    "    \"        prng_key=next(key_gen),\\n\",\n",
    "    \"        config=config,\\n\",\n",
    "    \"        model_sub_config_name_base='model',\\n\",\n",
    "    \"        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    traceback.print_exc()\\n\",\n",
    "    \"    print(e)\\n\",\n",
    "    \"    print('\\\\n')\\n\",\n",
    "    \"    pdb.post_mortem()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 4,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:11.465269211Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:11.384587169Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"CombinedINR(\\n\",\n",
    "       \"  terms=(\\n\",\n",
    "       \"    MLPINR(\\n\",\n",
    "       \"      layers=(\\n\",\n",
    "       \"        CountingIdentity(\\n\",\n",
    "       \"          _embedding_matrix=f32[3],\\n\",\n",
    "       \"          state_index=StateIndex(\\n\",\n",
    "       \"            marker=<object object at 0x7d935391bdf0>,\\n\",\n",
    "       \"            init=i32[]\\n\",\n",
    "       \"          )\\n\",\n",
    "       \"        ),\\n\",\n",
    "       \"        SirenLayer(\\n\",\n",
    "       \"          weights=f32[256,3],\\n\",\n",
    "       \"          biases=f32[256],\\n\",\n",
    "       \"          activation_kwargs={'w0': 12.0}\\n\",\n",
    "       \"        ),\\n\",\n",
    "       \"        SirenLayer(\\n\",\n",
    "       \"          weights=f32[256,256],\\n\",\n",
    "       \"          biases=f32[256],\\n\",\n",
    "       \"          activation_kwargs={'w0': 12.0}\\n\",\n",
    "       \"        ),\\n\",\n",
    "       \"        SirenLayer(\\n\",\n",
    "       \"          weights=f32[256,256],\\n\",\n",
    "       \"          biases=f32[256],\\n\",\n",
    "       \"          activation_kwargs={'w0': 12.0}\\n\",\n",
    "       \"        ),\\n\",\n",
    "       \"        Linear(weights=f32[1,256], biases=f32[1], activation_kwargs={})\\n\",\n",
    "       \"      )\\n\",\n",
    "       \"    ),\\n\",\n",
    "       \"  ),\\n\",\n",
    "       \"  post_processor=<function real_part>\\n\",\n",
    "       \")\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 4,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"inr\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 5,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:11.741031476Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:11.389193643Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# check that it works properly\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    inr(jnp.zeros(3))\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    traceback.print_exc()\\n\",\n",
    "    \"    print(e)\\n\",\n",
    "    \"    print('\\\\n')\\n\",\n",
    "    \"    pdb.post_mortem()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:11.741653457Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:11.666015300Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"(Array([0.99375975], dtype=float32), None)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 6,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"inr(jnp.zeros(3))\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 6,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:11.741817267Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:11.708522769Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 7,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:12.580775980Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:11.713174021Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\\n\",\n",
    "    \"experiment = cju.run_utils.get_experiment_from_config_and_key(\\n\",\n",
    "    \"    prng_key=next(key_gen),\\n\",\n",
    "    \"    config=config,\\n\",\n",
    "    \"    model_kwarg_in_trainer='inr',\\n\",\n",
    "    \"    model_sub_config_name_base='model',  # so it looks for \\\"model_config\\\" in config\\n\",\n",
    "    \"    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\\n\",\n",
    "    \"    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \\n\",\n",
    "    \"    add_model_module_to_architecture_default_module=False,\\n\",\n",
    "    \"    initialize=False  # don't run the experiment yet, we want to use wandb\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 8,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"end_time\": \"2025-01-30T12:42:12.585634123Z\",\n",
    "     \"start_time\": \"2025-01-30T12:42:12.581143135Z\"\n",
    "    }\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"PostponedInitialization(cls=train_with_dataloader_scan, kwargs={'loss_evaluator': PostponedInitialization(cls=SDFLossEvaluator, kwargs={'state_update_function': <function counter_updater at 0x7d92f1761510>}, missing_args=[]), 'dataloader': PostponedInitialization(cls=SDFDataLoader, kwargs={'sdf_name': 'Armadillo', 'batch_size': 20000, 'keep_aspect_ratio': True, 'key': Array([1786414058, 1458264990], dtype=uint32)}, missing_args=[]), 'optimizer': PostponedInitialization(cls=single_optimizer, kwargs={'optimizer_type': <function adamw at 0x7d935d13e200>, 'optimizer_config': {'learning_rate': 1e-05, 'weight_decay': 0.0001}, 'optimizer_mask': <function array_mask at 0x7d9346b228c0>, 'learning_rate_schedule': None, 'schedule_boundaries': None}, missing_args=[]), 'steps_per_cycle': 200, 'num_cycles': 40, 'use_wandb': True, 'after_cycle_callback': PostponedInitialization(cls=ComposedCallback, kwargs={'callbacks': [functools.partial(<function print_loss at 0x7d92f171b1c0>, after_every=1), <function report_loss at 0x7d92f1736ef0>, PostponedInitialization(cls=MetricCollectingCallback, kwargs={'metric_collector': PostponedInitialization(cls=MetricCollector, kwargs={'metrics': [PostponedInitialization(cls=JaccardIndexSDF, kwargs={'grid_resolution': 100, 'batch_size': 10000, 'num_dims': 3, 'frequency': 'every_n_batches', 'target_function': PostponedInitialization(cls=SDFDataLoader, kwargs={'sdf_name': 'Armadillo', 'batch_size': 10000, 'keep_aspect_ratio': True, 'key': Array([3322333273, 3038886829], dtype=uint32)}, missing_args=[])}, missing_args=[]), PostponedInitialization(cls=SDFReconstructor, kwargs={'grid_resolution': 100, 'batch_size': 10000, 'frequency': 'every_n_batches'}, missing_args=[])], 'batch_frequency': 2, 'epoch_frequency': 1}, missing_args=[])}, missing_args=[]), <function raise_error_on_nan at 0x7d92f17370a0>], 'show_logs': False, 'use_wandb': True, 'display_func': <function pprint at 0x7d938ed0d120>}, missing_args=[]), 'after_training_callback': <function after_training_callback at 0x7d92f17617e0>, 'optimizer_state': None, 'state_initialization_function': <function initialize_state at 0x7d92f17376d0>, 'state': None, 'inr': PostponedInitialization(cls=CombinedINR, kwargs={'terms': [PostponedInitialization(cls=from_config, kwargs={'hidden_size': 256, 'num_layers': 5, 'layer_type': <class 'model_components.inr_layers.SirenLayer'>, 'activation_kwargs': {'w0': 12.0}, 'positional_encoding_layer': PostponedInitialization(cls=CountingIdentity, kwargs={}, missing_args=[]), 'num_splits': 3, 'initialization_scheme': None, 'initialization_scheme_kwargs': None, 'post_processor': None, 'in_size': 3, 'out_size': 1, 'key': Array([4177750840, 1613599438], dtype=uint32)}, missing_args=[])], 'post_processor': <function real_part at 0x7d93810e97e0>}, missing_args=[])}, missing_args=[])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 8,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"experiment\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 9,\n",
    "   \"metadata\": {\n",
    "    \"ExecuteTime\": {\n",
    "     \"start_time\": \"2025-01-30T12:42:12.587342722Z\"\n",
    "    },\n",
    "    \"is_executing\": true\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"Tracking run with wandb version 0.18.1\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"Run data is saved locally in <code>/home/simon/Documents/INR_BEP/wandb/run-20250130_180722-gw03sf06</code>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"Syncing run <strong><a href='https://wandb.ai/nld/inr_edu_24/runs/gw03sf06' target=\\\"_blank\\\">colorful-eon-229</a></strong> to <a href='https://wandb.ai/nld/inr_edu_24' target=\\\"_blank\\\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\\\"_blank\\\">docs</a>)<br/>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \" View project at <a href='https://wandb.ai/nld/inr_edu_24' target=\\\"_blank\\\">https://wandb.ai/nld/inr_edu_24</a>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \" View run at <a href='https://wandb.ai/nld/inr_edu_24/runs/gw03sf06' target=\\\"_blank\\\">https://wandb.ai/nld/inr_edu_24/runs/gw03sf06</a>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Loss at step 1 is 503.4261474609375.\\n\",\n",
    "      \"Loss at step 2 is 151.2399139404297.\\n\",\n",
    "      \"Loss at step 3 is 158.7724609375.\\n\",\n",
    "      \"Loss at step 4 is 153.7262725830078.\\n\",\n",
    "      \"Loss at step 5 is 148.79888916015625.\\n\",\n",
    "      \"Loss at step 6 is 154.47230529785156.\\n\",\n",
    "      \"Loss at step 7 is 140.12860107421875.\\n\",\n",
    "      \"Loss at step 8 is 144.83226013183594.\\n\",\n",
    "      \"Loss at step 9 is 142.53469848632812.\\n\",\n",
    "      \"Loss at step 10 is 142.410400390625.\\n\",\n",
    "      \"Loss at step 11 is 144.37510681152344.\\n\",\n",
    "      \"Loss at step 12 is 144.6142120361328.\\n\",\n",
    "      \"Loss at step 13 is 142.5659942626953.\\n\",\n",
    "      \"Loss at step 14 is 144.45936584472656.\\n\",\n",
    "      \"Loss at step 15 is 148.80271911621094.\\n\",\n",
    "      \"Loss at step 16 is 142.94775390625.\\n\",\n",
    "      \"Loss at step 17 is 141.3892364501953.\\n\",\n",
    "      \"Loss at step 18 is 142.63316345214844.\\n\",\n",
    "      \"Loss at step 19 is 142.49746704101562.\\n\",\n",
    "      \"Loss at step 20 is 139.62646484375.\\n\",\n",
    "      \"Loss at step 21 is 136.5791015625.\\n\",\n",
    "      \"Loss at step 22 is 142.61839294433594.\\n\",\n",
    "      \"Loss at step 23 is 144.36602783203125.\\n\",\n",
    "      \"Loss at step 24 is 147.67250061035156.\\n\",\n",
    "      \"Loss at step 25 is 139.03599548339844.\\n\",\n",
    "      \"Loss at step 26 is 142.9278106689453.\\n\",\n",
    "      \"Loss at step 27 is 141.47320556640625.\\n\",\n",
    "      \"Loss at step 28 is 142.55931091308594.\\n\",\n",
    "      \"Loss at step 29 is 137.64599609375.\\n\",\n",
    "      \"Loss at step 30 is 140.3170928955078.\\n\",\n",
    "      \"Loss at step 31 is 142.35797119140625.\\n\",\n",
    "      \"Loss at step 32 is 146.64427185058594.\\n\",\n",
    "      \"Loss at step 33 is 139.32582092285156.\\n\",\n",
    "      \"Loss at step 34 is 145.40850830078125.\\n\",\n",
    "      \"Loss at step 35 is 143.9984130859375.\\n\",\n",
    "      \"Loss at step 36 is 136.34359741210938.\\n\",\n",
    "      \"Loss at step 37 is 140.08192443847656.\\n\",\n",
    "      \"Loss at step 38 is 141.01693725585938.\\n\",\n",
    "      \"Loss at step 39 is 135.44921875.\\n\",\n",
    "      \"Loss at step 40 is 140.49887084960938.\\n\",\n",
    "      \"Checking model and state for CountingIdentity layers\\n\",\n",
    "      \"Found a CountingIdentity layer with counter value 8000 in final state after training.\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"application/vnd.jupyter.widget-view+json\": {\n",
    "       \"model_id\": \"8fec9c417e764f71869219638ee55264\",\n",
    "       \"version_major\": 2,\n",
    "       \"version_minor\": 0\n",
    "      },\n",
    "      \"text/plain\": [\n",
    "       \"VBox(children=(Label(value='1507.252 MB of 1576.874 MB uploaded\\\\r'), FloatProgress(value=0.9558483567113274, m…\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"<style>\\n\",\n",
    "       \"    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\\n\",\n",
    "       \"    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\\n\",\n",
    "       \"    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\\n\",\n",
    "       \"    </style>\\n\",\n",
    "       \"<div class=\\\"wandb-row\\\"><div class=\\\"wandb-col\\\"><h3>Run history:</h3><br/><table class=\\\"wandb\\\"><tr><td>batch_within_epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>jaccard_index</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\\\"wandb-col\\\"><h3>Run summary:</h3><br/><table class=\\\"wandb\\\"><tr><td>batch_within_epoch</td><td>40</td></tr><tr><td>epoch</td><td>1</td></tr><tr><td>jaccard_index</td><td>0</td></tr><tr><td>loss</td><td>140.49887</td></tr></table><br/></div></div>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \" View run <strong style=\\\"color:#cdcd00\\\">colorful-eon-229</strong> at: <a href='https://wandb.ai/nld/inr_edu_24/runs/gw03sf06' target=\\\"_blank\\\">https://wandb.ai/nld/inr_edu_24/runs/gw03sf06</a><br/> View project at: <a href='https://wandb.ai/nld/inr_edu_24' target=\\\"_blank\\\">https://wandb.ai/nld/inr_edu_24</a><br/>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 20 other file(s)\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/html\": [\n",
    "       \"Find logs at: <code>./wandb/run-20250130_180722-gw03sf06/logs</code>\"\n",
    "      ],\n",
    "      \"text/plain\": [\n",
    "       \"<IPython.core.display.HTML object>\"\n",
    "      ]\n",
    "     },\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"display_data\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"# and we run the experiment while logging things to wandb\\n\",\n",
    "    \"try:\\n\",\n",
    "    \"    with wandb.init(\\n\",\n",
    "    \"        project='inr_edu_24',\\n\",\n",
    "    \"        notes='test',\\n\",\n",
    "    \"        tags=['test']\\n\",\n",
    "    \"    ) as run:\\n\",\n",
    "    \"        results = experiment.initialize()\\n\",\n",
    "    \"except Exception as e:\\n\",\n",
    "    \"    traceback.print_exc()\\n\",\n",
    "    \"    print(e)\\n\",\n",
    "    \"    print('\\\\n')\\n\",\n",
    "    \"    pdb.post_mortem()\\n\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 10,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"CombinedINR(\\n\",\n",
    "       \"  terms=(\\n\",\n",
    "       \"    MLPINR(\\n\",\n",
    "       \"      layers=(\\n\",\n",
    "       \"        CountingIdentity(\\n\",\n",
    "       \"          _embedding_matrix=True,\\n\",\n",
    "       \"          state_index=StateIndex(\\n\",\n",
    "       \"            marker=<object object at 0x7d935391bdf0>,\\n\",\n",
    "       \"            init=True\\n\",\n",
    "       \"          )\\n\",\n",
    "       \"        ),\\n\",\n",
    "       \"        SirenLayer(weights=True, biases=True, activation_kwargs={'w0': False}),\\n\",\n",
    "       \"        SirenLayer(weights=True, biases=True, activation_kwargs={'w0': False}),\\n\",\n",
    "       \"        SirenLayer(weights=True, biases=True, activation_kwargs={'w0': False}),\\n\",\n",
    "       \"        Linear(weights=True, biases=True, activation_kwargs={})\\n\",\n",
    "       \"      )\\n\",\n",
    "       \"    ),\\n\",\n",
    "       \"  ),\\n\",\n",
    "       \"  post_processor=False\\n\",\n",
    "       \")\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 10,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import equinox as eqx\\n\",\n",
    "    \"from inr_utils.masking import array_mask\\n\",\n",
    "    \"array_mask(inr)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 11,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"leaf=None\\n\",\n",
    "      \"leaf=Array([0., 0., 0.], dtype=float32)\\n\",\n",
    "      \"leaf=<function <lambda> at 0x7d938d057130>\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"[False, True, False]\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 11,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"def is_array(leaf):\\n\",\n",
    "    \"    print(f\\\"{leaf=}\\\")\\n\",\n",
    "    \"    return eqx.is_array(leaf)\\n\",\n",
    "    \"jax.tree_util.tree_map(is_array, [None, jnp.zeros(3), lambda x: x], is_leaf=lambda x: x is None or eqx.is_array(x))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 12,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"/tmp/ipykernel_2601660/2208892696.py:1: DeprecationWarning:\\n\",\n",
    "      \"\\n\",\n",
    "      \"jax.tree_flatten is deprecated: use jax.tree.flatten (jax v0.4.25 or newer) or jax.tree_util.tree_flatten (any JAX version).\\n\",\n",
    "      \"\\n\"\n",
    "     ]\n",
    "    },\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"([Array([0., 0., 0.], dtype=float32)], PyTreeDef([None, *]))\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 12,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"jax.tree_flatten([None, jnp.zeros(3)])\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 13,\n",
    "   \"metadata\": {\n",
    "    \"is_executing\": true\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"CombinedINR(\\n\",\n",
    "       \"  terms=(\\n\",\n",
    "       \"    MLPINR(\\n\",\n",
    "       \"      layers=(\\n\",\n",
    "       \"        CountingIdentity(\\n\",\n",
    "       \"          _embedding_matrix=f32[3],\\n\",\n",
    "       \"          state_index=StateIndex(marker=0, init=_Sentinel())\\n\",\n",
    "       \"        ),\\n\",\n",
    "       \"        SirenLayer(\\n\",\n",
    "       \"          weights=f32[256,3],\\n\",\n",
    "       \"          biases=f32[256],\\n\",\n",
    "       \"          activation_kwargs={'w0': 12.0}\\n\",\n",
    "       \"        ),\\n\",\n",
    "       \"        SirenLayer(\\n\",\n",
    "       \"          weights=f32[256,256],\\n\",\n",
    "       \"          biases=f32[256],\\n\",\n",
    "       \"          activation_kwargs={'w0': 12.0}\\n\",\n",
    "       \"        ),\\n\",\n",
    "       \"        SirenLayer(\\n\",\n",
    "       \"          weights=f32[256,256],\\n\",\n",
    "       \"          biases=f32[256],\\n\",\n",
    "       \"          activation_kwargs={'w0': 12.0}\\n\",\n",
    "       \"        ),\\n\",\n",
    "       \"        Linear(weights=f32[1,256], biases=f32[1], activation_kwargs={})\\n\",\n",
    "       \"      )\\n\",\n",
    "       \"    ),\\n\",\n",
    "       \"  ),\\n\",\n",
    "       \"  post_processor=<function real_part>\\n\",\n",
    "       \")\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 13,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"inr, losses, optimizer_state, state, loss_evaluator, additional_output = results\\n\",\n",
    "    \"inr\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 14,\n",
    "   \"metadata\": {\n",
    "    \"is_executing\": true\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"Checking model and state for CountingIdentity layers\\n\",\n",
    "      \"Found a CountingIdentity layer with counter value 8000 in final state after training.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"from state_test_objects import after_training_callback, CountingIdentity\\n\",\n",
    "    \"after_training_callback(losses, inr, state)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 15,\n",
    "   \"metadata\": {\n",
    "    \"is_executing\": true\n",
    "   },\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"Array([[-0.9475113 ,  0.51643044, -0.732251  ],\\n\",\n",
    "       \"       [-0.03327261, -0.5643355 ,  0.07955802],\\n\",\n",
    "       \"       [-0.03956697,  0.89269453, -0.5453213 ],\\n\",\n",
    "       \"       ...,\\n\",\n",
    "       \"       [ 0.40899348, -0.77576375,  0.21623659],\\n\",\n",
    "       \"       [ 0.35770416,  0.64567757, -0.31344652],\\n\",\n",
    "       \"       [-0.03441858, -0.5913744 ,  0.3327012 ]], dtype=float32)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 15,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"from inr_utils.sdf import SDFDataLoader\\n\",\n",
    "    \"data_loader = SDFDataLoader(**config.dataloader_config, key=jax.random.PRNGKey(0))\\n\",\n",
    "    \"\\n\",\n",
    "    \"batch = next(iter(data_loader))\\n\",\n",
    "    \"batch[0]\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 16,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"Array(0.99999994, dtype=float32)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 16,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"jax.numpy.linalg.norm(batch[1], axis=-1).min()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 17,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"Array(1.7320508, dtype=float32)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 17,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"jax.numpy.linalg.norm(batch[1], axis=-1).max()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 18,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"(CudaDevice(id=0), CudaDevice(id=0), CudaDevice(id=0))\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 18,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"jax.tree.map(lambda x: x.device, batch)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 19,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"(20000, 3)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 19,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"batch[1].shape\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 20,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"CudaDevice(id=0)\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 20,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"def get_default_device():\\n\",\n",
    "    \"  return jax.config.jax_default_device or jax.local_devices()[0]\\n\",\n",
    "    \"get_default_device()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 21,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"PostponedInitialization(cls=train_with_dataloader_scan, kwargs={'loss_evaluator': PostponedInitialization(cls=SDFLossEvaluator, kwargs={'state_update_function': <function counter_updater at 0x7d92f1761510>}, missing_args=[]), 'dataloader': PostponedInitialization(cls=SDFDataLoader, kwargs={'sdf_name': 'Armadillo', 'batch_size': 20000, 'keep_aspect_ratio': True, 'key': Array([1786414058, 1458264990], dtype=uint32)}, missing_args=[]), 'optimizer': PostponedInitialization(cls=single_optimizer, kwargs={'optimizer_type': <function adamw at 0x7d935d13e200>, 'optimizer_config': {'learning_rate': 1e-05, 'weight_decay': 0.0001}, 'optimizer_mask': <function array_mask at 0x7d9346b228c0>, 'learning_rate_schedule': None, 'schedule_boundaries': None}, missing_args=[]), 'steps_per_cycle': 200, 'num_cycles': 40, 'use_wandb': True, 'after_cycle_callback': PostponedInitialization(cls=ComposedCallback, kwargs={'callbacks': [functools.partial(<function print_loss at 0x7d92f171b1c0>, after_every=1), <function report_loss at 0x7d92f1736ef0>, PostponedInitialization(cls=MetricCollectingCallback, kwargs={'metric_collector': PostponedInitialization(cls=MetricCollector, kwargs={'metrics': [PostponedInitialization(cls=JaccardIndexSDF, kwargs={'grid_resolution': 100, 'batch_size': 10000, 'num_dims': 3, 'frequency': 'every_n_batches', 'target_function': PostponedInitialization(cls=SDFDataLoader, kwargs={'sdf_name': 'Armadillo', 'batch_size': 10000, 'keep_aspect_ratio': True, 'key': Array([3322333273, 3038886829], dtype=uint32)}, missing_args=[])}, missing_args=[]), PostponedInitialization(cls=SDFReconstructor, kwargs={'grid_resolution': 100, 'batch_size': 10000, 'frequency': 'every_n_batches'}, missing_args=[])], 'batch_frequency': 2, 'epoch_frequency': 1}, missing_args=[])}, missing_args=[]), <function raise_error_on_nan at 0x7d92f17370a0>], 'show_logs': False, 'use_wandb': True, 'display_func': <function pprint at 0x7d938ed0d120>}, missing_args=[]), 'after_training_callback': <function after_training_callback at 0x7d92f17617e0>, 'optimizer_state': None, 'state_initialization_function': <function initialize_state at 0x7d92f17376d0>, 'state': None, 'inr': PostponedInitialization(cls=CombinedINR, kwargs={'terms': [PostponedInitialization(cls=from_config, kwargs={'hidden_size': 256, 'num_layers': 5, 'layer_type': <class 'model_components.inr_layers.SirenLayer'>, 'activation_kwargs': {'w0': 12.0}, 'positional_encoding_layer': PostponedInitialization(cls=CountingIdentity, kwargs={}, missing_args=[]), 'num_splits': 3, 'initialization_scheme': None, 'initialization_scheme_kwargs': None, 'post_processor': None, 'in_size': 3, 'out_size': 1, 'key': Array([4177750840, 1613599438], dtype=uint32)}, missing_args=[])], 'post_processor': <function real_part at 0x7d93810e97e0>}, missing_args=[])}, missing_args=[])\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 21,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"experiment\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 22,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"loss_evaluator = experiment.kwargs['loss_evaluator'].initialize()\\n\",\n",
    "    \"data_loader = experiment.kwargs['dataloader'].initialize()\\n\",\n",
    "    \"optimizer = experiment.kwargs['optimizer'].initialize()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 23,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"data\": {\n",
    "      \"text/plain\": [\n",
    "       \"{CudaDevice(id=0)}\"\n",
    "      ]\n",
    "     },\n",
    "     \"execution_count\": 23,\n",
    "     \"metadata\": {},\n",
    "     \"output_type\": \"execute_result\"\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"import inr_utils\\n\",\n",
    "    \"import equinox as eqx\\n\",\n",
    "    \"\\n\",\n",
    "    \"train_step = inr_utils.training.make_sampler_free_train_step(loss_evaluator=loss_evaluator, optimizer=optimizer)\\n\",\n",
    "    \"\\n\",\n",
    "    \"set(leaf.device for leaf in jax.tree.leaves(inr) if eqx.is_array(leaf))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 24,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{CudaDevice(id=0)}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"print(set(leaf.device for leaf in jax.tree.leaves(state) if eqx.is_array(leaf)))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 25,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{CudaDevice(id=0)}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"print(set(leaf.device for leaf in jax.tree.leaves(optimizer_state) if eqx.is_array(leaf)))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 26,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"batch = next(iter(data_loader))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 27,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{CudaDevice(id=0)}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"print(set(leaf.device for leaf in jax.tree.leaves(batch) if eqx.is_array(leaf)))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 28,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stderr\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"E0130 18:11:39.454519 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0535316, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.454545 2601660 buffer_comparator.cc:157] Difference at 3466: 3.75414, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.454552 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5179, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.454554 2601660 buffer_comparator.cc:157] Difference at 5875: 0.961105, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.454557 2601660 buffer_comparator.cc:157] Difference at 6446: 5.06743, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.454559 2601660 buffer_comparator.cc:157] Difference at 6663: -0.518723, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.454561 2601660 buffer_comparator.cc:157] Difference at 6794: 1.70668, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.454566 2601660 buffer_comparator.cc:157] Difference at 8168: 1.41833, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.454577 2601660 buffer_comparator.cc:157] Difference at 12705: 1.84048, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.454578 2601660 buffer_comparator.cc:157] Difference at 12720: -0.320306, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.454583: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.456878 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0535316, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.456892 2601660 buffer_comparator.cc:157] Difference at 3466: 3.75414, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.456899 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5179, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.456901 2601660 buffer_comparator.cc:157] Difference at 5875: 0.961105, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.456904 2601660 buffer_comparator.cc:157] Difference at 6446: 5.06743, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.456906 2601660 buffer_comparator.cc:157] Difference at 6663: -0.518723, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.456908 2601660 buffer_comparator.cc:157] Difference at 6794: 1.70668, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.456913 2601660 buffer_comparator.cc:157] Difference at 8168: 1.41833, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.456924 2601660 buffer_comparator.cc:157] Difference at 12705: 1.84048, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.456926 2601660 buffer_comparator.cc:157] Difference at 12720: -0.320306, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.456930: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.458763 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0522842, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.458777 2601660 buffer_comparator.cc:157] Difference at 3466: 3.76985, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.458784 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5155, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.458786 2601660 buffer_comparator.cc:157] Difference at 5875: 0.972187, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.458788 2601660 buffer_comparator.cc:157] Difference at 6446: 5.07782, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.458791 2601660 buffer_comparator.cc:157] Difference at 6663: -0.509399, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.458793 2601660 buffer_comparator.cc:157] Difference at 6794: 1.74046, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.458797 2601660 buffer_comparator.cc:157] Difference at 8168: 1.42235, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.458808 2601660 buffer_comparator.cc:157] Difference at 12705: 1.83153, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.458810 2601660 buffer_comparator.cc:157] Difference at 12720: -0.329475, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.458813: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.460278 2601660 buffer_comparator.cc:157] Difference at 2007: -0.052887, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.460291 2601660 buffer_comparator.cc:157] Difference at 3466: 3.76767, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.460298 2601660 buffer_comparator.cc:157] Difference at 5808: 1.51242, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.460300 2601660 buffer_comparator.cc:157] Difference at 5875: 0.969238, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.460303 2601660 buffer_comparator.cc:157] Difference at 6446: 5.07597, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.460305 2601660 buffer_comparator.cc:157] Difference at 6663: -0.511147, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.460307 2601660 buffer_comparator.cc:157] Difference at 6794: 1.73555, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.460311 2601660 buffer_comparator.cc:157] Difference at 8168: 1.42212, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.460324 2601660 buffer_comparator.cc:157] Difference at 12705: 1.83311, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.460326 2601660 buffer_comparator.cc:157] Difference at 12720: -0.330162, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.460329: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.464901 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0535316, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.464935 2601660 buffer_comparator.cc:157] Difference at 3466: 3.75414, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.464944 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5179, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.464946 2601660 buffer_comparator.cc:157] Difference at 5875: 0.961105, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.464948 2601660 buffer_comparator.cc:157] Difference at 6446: 5.06743, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.464951 2601660 buffer_comparator.cc:157] Difference at 6663: -0.518723, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.464953 2601660 buffer_comparator.cc:157] Difference at 6794: 1.70668, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.464957 2601660 buffer_comparator.cc:157] Difference at 8168: 1.41833, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.464968 2601660 buffer_comparator.cc:157] Difference at 12705: 1.84048, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.464970 2601660 buffer_comparator.cc:157] Difference at 12720: -0.320306, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.464973: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.466105 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0522842, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.466121 2601660 buffer_comparator.cc:157] Difference at 3466: 3.76985, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.466131 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5155, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.466134 2601660 buffer_comparator.cc:157] Difference at 5875: 0.972187, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.466138 2601660 buffer_comparator.cc:157] Difference at 6446: 5.07782, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.466141 2601660 buffer_comparator.cc:157] Difference at 6663: -0.509399, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.466144 2601660 buffer_comparator.cc:157] Difference at 6794: 1.74046, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.466150 2601660 buffer_comparator.cc:157] Difference at 8168: 1.42235, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.466167 2601660 buffer_comparator.cc:157] Difference at 12705: 1.83153, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.466170 2601660 buffer_comparator.cc:157] Difference at 12720: -0.329475, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.466174: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.468087 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0535316, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.468100 2601660 buffer_comparator.cc:157] Difference at 3466: 3.75414, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.468111 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5179, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.468113 2601660 buffer_comparator.cc:157] Difference at 5875: 0.961105, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.468118 2601660 buffer_comparator.cc:157] Difference at 6446: 5.06743, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.468121 2601660 buffer_comparator.cc:157] Difference at 6663: -0.518723, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.468123 2601660 buffer_comparator.cc:157] Difference at 6794: 1.70668, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.468130 2601660 buffer_comparator.cc:157] Difference at 8168: 1.41833, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.468148 2601660 buffer_comparator.cc:157] Difference at 12705: 1.84048, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.468151 2601660 buffer_comparator.cc:157] Difference at 12720: -0.320306, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.468155: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.470478 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0535316, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.470490 2601660 buffer_comparator.cc:157] Difference at 3466: 3.75414, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.470496 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5179, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.470498 2601660 buffer_comparator.cc:157] Difference at 5875: 0.961105, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.470501 2601660 buffer_comparator.cc:157] Difference at 6446: 5.06743, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.470503 2601660 buffer_comparator.cc:157] Difference at 6663: -0.518723, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.470505 2601660 buffer_comparator.cc:157] Difference at 6794: 1.70668, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.470510 2601660 buffer_comparator.cc:157] Difference at 8168: 1.41833, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.470521 2601660 buffer_comparator.cc:157] Difference at 12705: 1.84048, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.470523 2601660 buffer_comparator.cc:157] Difference at 12720: -0.320306, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.470526: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.473911 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0535316, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.473922 2601660 buffer_comparator.cc:157] Difference at 3466: 3.75414, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.473930 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5179, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.473932 2601660 buffer_comparator.cc:157] Difference at 5875: 0.961105, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.473934 2601660 buffer_comparator.cc:157] Difference at 6446: 5.06743, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.473936 2601660 buffer_comparator.cc:157] Difference at 6663: -0.518723, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.473938 2601660 buffer_comparator.cc:157] Difference at 6794: 1.70668, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.473943 2601660 buffer_comparator.cc:157] Difference at 8168: 1.41833, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.473954 2601660 buffer_comparator.cc:157] Difference at 12705: 1.84048, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.473956 2601660 buffer_comparator.cc:157] Difference at 12720: -0.320306, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.473959: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.479173 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0535316, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.479185 2601660 buffer_comparator.cc:157] Difference at 3466: 3.75414, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.479192 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5179, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.479194 2601660 buffer_comparator.cc:157] Difference at 5875: 0.961105, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.479197 2601660 buffer_comparator.cc:157] Difference at 6446: 5.06743, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.479199 2601660 buffer_comparator.cc:157] Difference at 6663: -0.518723, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.479201 2601660 buffer_comparator.cc:157] Difference at 6794: 1.70668, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.479205 2601660 buffer_comparator.cc:157] Difference at 8168: 1.41833, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.479216 2601660 buffer_comparator.cc:157] Difference at 12705: 1.84048, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.479218 2601660 buffer_comparator.cc:157] Difference at 12720: -0.320306, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.479221: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.482203 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0535316, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.482215 2601660 buffer_comparator.cc:157] Difference at 3466: 3.75414, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.482222 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5179, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.482224 2601660 buffer_comparator.cc:157] Difference at 5875: 0.961105, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.482227 2601660 buffer_comparator.cc:157] Difference at 6446: 5.06743, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.482229 2601660 buffer_comparator.cc:157] Difference at 6663: -0.518723, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.482231 2601660 buffer_comparator.cc:157] Difference at 6794: 1.70668, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.482235 2601660 buffer_comparator.cc:157] Difference at 8168: 1.41833, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.482246 2601660 buffer_comparator.cc:157] Difference at 12705: 1.84048, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.482249 2601660 buffer_comparator.cc:157] Difference at 12720: -0.320306, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.482251: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.485759 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0535316, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.485770 2601660 buffer_comparator.cc:157] Difference at 3466: 3.75414, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.485776 2601660 buffer_comparator.cc:157] Difference at 5808: 1.5179, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.485778 2601660 buffer_comparator.cc:157] Difference at 5875: 0.961105, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.485781 2601660 buffer_comparator.cc:157] Difference at 6446: 5.06743, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.485783 2601660 buffer_comparator.cc:157] Difference at 6663: -0.518723, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.485785 2601660 buffer_comparator.cc:157] Difference at 6794: 1.70668, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.485789 2601660 buffer_comparator.cc:157] Difference at 8168: 1.41833, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.485800 2601660 buffer_comparator.cc:157] Difference at 12705: 1.84048, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.485803 2601660 buffer_comparator.cc:157] Difference at 12720: -0.320306, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.485805: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\",\n",
    "      \"E0130 18:11:39.489212 2601660 buffer_comparator.cc:157] Difference at 2007: -0.0527763, expected -0.618668\\n\",\n",
    "      \"E0130 18:11:39.489223 2601660 buffer_comparator.cc:157] Difference at 3466: 3.76367, expected 4.34357\\n\",\n",
    "      \"E0130 18:11:39.489230 2601660 buffer_comparator.cc:157] Difference at 5808: 1.51389, expected 2.07677\\n\",\n",
    "      \"E0130 18:11:39.489231 2601660 buffer_comparator.cc:157] Difference at 5875: 0.964783, expected 1.66135\\n\",\n",
    "      \"E0130 18:11:39.489234 2601660 buffer_comparator.cc:157] Difference at 6446: 5.07288, expected 4.17685\\n\",\n",
    "      \"E0130 18:11:39.489236 2601660 buffer_comparator.cc:157] Difference at 6663: -0.513474, expected -0.749084\\n\",\n",
    "      \"E0130 18:11:39.489238 2601660 buffer_comparator.cc:157] Difference at 6794: 1.72568, expected 2.22054\\n\",\n",
    "      \"E0130 18:11:39.489243 2601660 buffer_comparator.cc:157] Difference at 8168: 1.42047, expected 0.894318\\n\",\n",
    "      \"E0130 18:11:39.489254 2601660 buffer_comparator.cc:157] Difference at 12705: 1.83643, expected 2.29286\\n\",\n",
    "      \"E0130 18:11:39.489256 2601660 buffer_comparator.cc:157] Difference at 12720: -0.327911, expected 0.221405\\n\",\n",
    "      \"2025-01-30 18:11:39.489259: E external/xla/xla/service/gpu/autotuning/gemm_fusion_autotuner.cc:982] Results do not match the reference. This is likely a bug/unexpected loss of precision.\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"out = train_step(inr, batch, optimizer_state, state)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 29,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"{CudaDevice(id=0)}\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"print(set(leaf.device for leaf in jax.tree.leaves(out) if eqx.is_array(leaf)))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 30,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"13.7 ms ± 115 μs per loop (mean ± std. dev. of 7 runs, 100 loops each)\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"%%timeit\\n\",\n",
    "    \"# this does run on the gpu\\n\",\n",
    "    \"out = train_step(inr, batch, optimizer_state, state)\\n\",\n",
    "    \"jax.block_until_ready(out)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 31,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"data_iter = iter(data_loader)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": 32,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [\n",
    "    {\n",
    "     \"name\": \"stdout\",\n",
    "     \"output_type\": \"stream\",\n",
    "     \"text\": [\n",
    "      \"750 μs ± 7.65 μs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\\n\"\n",
    "     ]\n",
    "    }\n",
    "   ],\n",
    "   \"source\": [\n",
    "    \"%%timeit\\n\",\n",
    "    \"next(data_iter)\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": []\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"inr_edu_24\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.10.14\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 2\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
