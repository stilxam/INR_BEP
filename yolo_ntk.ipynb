{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon-martinus-koop\u001b[0m (\u001b[33mnld\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2024-12-17 16:49:47.231826: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "import equinox as eqx\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.CombinedINR'\n",
    "\n",
    "config.model_config = Config()\n",
    "config.model_config.in_size = 2\n",
    "config.model_config.out_size = 1\n",
    "config.model_config.terms = [  # CombinedINR uses multiple MLPs and returns the sum of their outputs. These 'terms' are the MLPs\n",
    "    ('inr_modules.MLPINR.from_config',{\n",
    "        'hidden_size': 1028,\n",
    "        'num_layers': 5,\n",
    "        'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "        'num_splits': 3,\n",
    "        'activation_kwargs': {'inverse_scale': 5.},\n",
    "        'initialization_scheme':'initialization_schemes.siren_scheme',\n",
    "        'initialization_scheme_kwargs': {'w0': 12.}\n",
    "        #'positional_encoding_layer': ('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies': 10}),\n",
    "    }),\n",
    "    # ('inr_modules.MLPINR.from_config',{\n",
    "    #     'hidden_size': 1024,\n",
    "    #     'num_layers': 2,\n",
    "    #     'num_splits': 1,\n",
    "    #     'layer_type': 'inr_layers.GaussianINRLayer',\n",
    "    #     'use_complex': False,\n",
    "    #     'activation_kwargs': {'inverse_scale': 1},\n",
    "    # })\n",
    "]\n",
    "config.model_config.post_processor = lambda x: x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_inr(inr, location):\n",
    "    return inr(location)\n",
    "\n",
    "inr_grad = eqx.filter_grad(apply_inr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CombinedINR(\n",
       "  terms=(\n",
       "    MLPINR(\n",
       "      layers=(\n",
       "        GaussianINRLayer(\n",
       "          weights=f32[1028,2],\n",
       "          biases=f32[1028],\n",
       "          activation_kwargs={'inverse_scale': 5.0}\n",
       "        ),\n",
       "        GaussianINRLayer(\n",
       "          weights=f32[1028,1028],\n",
       "          biases=f32[1028],\n",
       "          activation_kwargs={'inverse_scale': 5.0}\n",
       "        ),\n",
       "        GaussianINRLayer(\n",
       "          weights=f32[1028,1028],\n",
       "          biases=f32[1028],\n",
       "          activation_kwargs={'inverse_scale': 5.0}\n",
       "        ),\n",
       "        GaussianINRLayer(\n",
       "          weights=f32[1028,1028],\n",
       "          biases=f32[1028],\n",
       "          activation_kwargs={'inverse_scale': 5.0}\n",
       "        ),\n",
       "        Linear(weights=f32[1,1028], biases=f32[1], activation_kwargs={})\n",
       "      )\n",
       "    ),\n",
       "  ),\n",
       "  post_processor=None\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr_grad(inr, jnp.array([0.1, 0.75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_inner_product(tree_1, tree_2):\n",
    "    component_wise = jax.tree.map(lambda x, y: jnp.sum(x*y), tree_1, tree_2)\n",
    "    return sum(jax.tree.leaves(component_wise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(5.973792, dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_inner_product(\n",
    "    inr_grad(inr, jnp.array([0.1, 0.75])),\n",
    "    inr_grad(inr, jnp.array([0.2, 0.65]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ntk_single(inr, loc_1, loc_2):\n",
    "    return tree_inner_product(\n",
    "    inr_grad(inr, loc_1),\n",
    "    inr_grad(inr, loc_2)\n",
    ")\n",
    "\n",
    "def _ntk_single(inr, loc1loc2):\n",
    "    channels = loc1loc2.shape[-1]//2\n",
    "    loc_1 = loc1loc2[:channels]\n",
    "    loc_2 = loc1loc2[channels:]\n",
    "    return ntk_single(inr, loc_1, loc_2)\n",
    "\n",
    "def ntk_array(inr, locations, batch_size):\n",
    "    channels = locations.shape[-1]\n",
    "    locations = locations.reshape(-1, channels)\n",
    "\n",
    "    #first on the lower triangle\n",
    "    loc_1_indices, loc_2_indices = jnp.tril_indices(locations.shape[0])\n",
    "    loc_1 = locations[loc_1_indices]\n",
    "    loc_2 = locations[loc_2_indices]\n",
    "    loc_1_loc_2 = jnp.concatenate([loc_1, loc_2], -1)\n",
    "    \n",
    "    apply_ntk_single_batch = lambda batch: jax.vmap(_ntk_single, (None, 0))(inr, batch)\n",
    "    batches = loc_1_loc_2.reshape((-1, batch_size, 2*channels))\n",
    "    print(f\"{batches.shape=}\")\n",
    "    num_batches = batches.shape[0]\n",
    "    resulting_batches = jax.lax.map(apply_ntk_single_batch, batches)\n",
    "    results_flat = resulting_batches.reshape(num_batches*batch_size)\n",
    "\n",
    "    return results_flat\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inr_utils.images import make_lin_grid\n",
    "locations = make_lin_grid(0., 1., (100, 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batches.shape=(10001, 5000, 4)\n"
     ]
    }
   ],
   "source": [
    "kernel = ntk_array(inr, locations, 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50005000,)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
