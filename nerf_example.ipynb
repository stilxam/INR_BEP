{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of training an INR locally\n",
    "This notebook provides an example of how to create an INR and train it locally using the tools in this repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:22.968569596Z",
     "start_time": "2025-01-06T14:55:22.925237450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msimon-martinus-koop\u001b[0m (\u001b[33mnld\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "2025-01-30 15:39:09.373390: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "import pdb\n",
    "import traceback\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import optax\n",
    "import wandb\n",
    "\n",
    "from common_dl_utils.config_creation import Config\n",
    "import common_jax_utils as cju\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "key = jax.random.PRNGKey(12398)\n",
    "key_gen = cju.key_generator(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "It seems to work, but we might want to implement gradient accumulation into the training loop so we can use larger batch sizes.\n",
    "\n",
    "Also, it would be good to implement metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a single INR on `example_data/parrot.png`. We'll use the `CombinedINR` clas from `model_components.inr_modules` together with the `SirenLayer` and `GaussianINRLayer` from `model_components.inr_layers` for the model, and we'll train it using the tools from `inr_utils`.\n",
    "\n",
    "To do all of this, basically we only need to create a config. We'll use the `common_dl_utils.config_creation.Config` class for this, but this is basically just a dictionary that allows for attribute access-like acces of its elements (so we can do `config.model_type = \"CombinedINR\"` instead of `config[\"model_type\"] = \"CombinedINR\"`). You can also just use a dictionary instead.\n",
    "\n",
    "Then we'll use the tools from `common_jax_utils` to first get a model from this config so we can inspect it, and then just run the experiment specified by the config.\n",
    "\n",
    "Doing this in a config instead of hard coded might seem like extra work, but consider this:\n",
    "1. you can serialize this config as a json file or a yaml file to later get the same model and experimental settings back \n",
    "   so when you are experimenting with different architectures, if you just store the configs you've used, you can easily recreate previous results\n",
    "2. when we get to running hyper parameter sweeps, you can easily get these configs (with a pick for the varying hyper parameters) from wandb\n",
    "   and then run an experiment specified by that config on any machine you want, e.g. on Snellius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-06T14:55:24.410408058Z",
     "start_time": "2025-01-06T14:55:24.393999841Z"
    }
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "# first we specify what the model should look like\n",
    "config.architecture = './model_components'  # module containing all relevant classes for architectures\n",
    "# NB if the classes relevant for creating the model are spread over multiple modules, this is no problem\n",
    "# let config.architecture be the module that contains the \"main\" model class, and for all other components just specify the module\n",
    "# or specify the other modules as default modules to the tools in common_jax_utils.run_utils\n",
    "config.model_type = 'inr_modules.NeRF.from_config'\n",
    "\n",
    "config.model_config = dict(\n",
    "    in_size=(3, 3),\n",
    "    out_size=(1, 3),\n",
    "    bottle_size=256,\n",
    "    block_length=4, \n",
    "    block_width=256,\n",
    "    num_blocks=2,\n",
    "    condition_length=1,\n",
    "    condition_width=256,\n",
    "    layer_type='inr_layers.SirenLayer',\n",
    "    activation_kwargs={'w0': 30.},\n",
    "    initialization_scheme=None,\n",
    "    initialization_scheme_kwargs=None,\n",
    "    positional_encoding_layer=('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies':6}),\n",
    "    direction_encoding_layer=('inr_layers.ClassicalPositionalEncoding.from_config', {'num_frequencies':6}),\n",
    ")\n",
    "\n",
    "\n",
    "# next, we set up the training loop, including the 'target_function' that we want to mimic\n",
    "config.trainer_module = './inr_utils/'  # similarly to config.architecture above, here we just specify in what module to look for objects by default\n",
    "# config.trainer_type = 'training.train_inr'\n",
    "# config.trainer_type = 'training.train_inr_with_dataloader'\n",
    "config.trainer_type = 'training.train_with_dataloader_scan'\n",
    "\n",
    "config.num_cycles = 50\n",
    "config.steps_per_cycle = 1000\n",
    "\n",
    "config.loss_evaluator = 'losses.NeRFLossEvaluator'\n",
    "config.loss_evaluator_config = dict(\n",
    "    #state_update_function = ('state_test_objects.py', 'counter_updater'),\n",
    "    num_coarse_samples=128,\n",
    "    num_fine_samples=512,\n",
    "    near=-1.,\n",
    "    far=1.,\n",
    "    noise_std=1.,  # this seems to be a reasonable value, based on https://github.com/bmild/nerf/blob/18b8aebda6700ed659cb27a0c348b737a5f6ab60/config_fern.txt#L14\n",
    "    white_bkgd=True,\n",
    "    lindisp=False,\n",
    "    randomized=True,\n",
    "    parallel_batch_size=500,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# config.sampler = ('sampling.NeRFSyntheticScenesSampler',{\n",
    "#     'split': 'train', \n",
    "#     'name': 'vase',\n",
    "#     'batch_size': 10,\n",
    "#     'poses_per_batch': 10,\n",
    "#     'base_path': 'synthetic_scenes',\n",
    "#     'size_limit': 100\n",
    "# })\n",
    "config.dataloader = 'nerf_utils.SyntheticScenesDataLoader'\n",
    "config.dataloader_config = {\n",
    "    'split': 'train', \n",
    "    'name': 'vase',\n",
    "    'batch_size': 500,\n",
    "    'poses_per_batch': 100,\n",
    "    'base_path': 'synthetic_scenes',\n",
    "    'size_limit': -1\n",
    "}\n",
    "\n",
    "config.optimizer = 'adamw'  # we'll have to add optax to the additional default modules later\n",
    "config.optimizer_config = {\n",
    "    'learning_rate': 5e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "}\n",
    "# config.steps = 20000 #changed from 40000\n",
    "config.use_wandb = True\n",
    "\n",
    "# now we want some extra things, like logging, to happen during training\n",
    "# the inr_utils.training.train_inr function allows for this through callbacks.\n",
    "# The callbacks we want to use can be found in inr_utils.callbacks\n",
    "# config.after_step_callback = 'callbacks.ComposedCallback'\n",
    "# config.after_step_callback_config = {\n",
    "#     'callbacks':[\n",
    "#         ('callbacks.print_loss', {'after_every':100}),  # only print the loss every 400th step\n",
    "#         'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "#         # ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "#         #      {'metric_collector':'metrics.MetricCollector'}\n",
    "#         # ),\n",
    "#         'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "#     ],\n",
    "#     'show_logs': False\n",
    "# }\n",
    "\n",
    "config.after_cycle_callback = 'callbacks.ComposedCallback'\n",
    "config.after_cycle_callback_config = {\n",
    "    'callbacks':[\n",
    "        ('callbacks.print_loss', {'after_every':1}),  # only print the loss every 400th step\n",
    "        'callbacks.report_loss',  # but log the loss to wandb after every step\n",
    "        # ('callbacks.MetricCollectingCallback', # this thing will help us collect metrics and log images to wandb\n",
    "        #      {'metric_collector':'metrics.MetricCollector'}\n",
    "        # ),\n",
    "        'callbacks.raise_error_on_nan'  # stop training if the loss becomes NaN\n",
    "    ],\n",
    "    'show_logs': False\n",
    "}\n",
    "\n",
    "#config.after_training_callback = ('state_test_objects.py', 'after_training_callback')\n",
    "\n",
    "# config.metric_collector_config = {  # the metrics for MetricCollectingCallback / metrics.MetricCollector\n",
    "#     'metrics':[\n",
    "#         ('metrics.PlotOnGrid2D', {'grid': 256, 'batch_size':8*256, 'frequency':'every_n_batches'}),  \n",
    "#         # ^ plots the image on this fixed grid so we can visually inspect the inr on wandb\n",
    "#         ('metrics.MSEOnFixedGrid', {'grid': [2040, 1356], 'batch_size':2040, 'frequency': 'every_n_batches'})\n",
    "#         # ^ compute the MSE with the actual image pixels\n",
    "#     ],\n",
    "#     'batch_frequency': 400,  # compute all of these metrics every 400 batches\n",
    "#     'epoch_frequency': 1  # not actually used\n",
    "# }\n",
    "\n",
    "#config.after_training_callback = None  # don't care for one now, but you could have this e.g. store some nice loss plots if you're not using wandb \n",
    "config.optimizer_state = None  # we're starting from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's first see if we get the correct model\n",
    "try:\n",
    "    inr = cju.run_utils.get_model_from_config_and_key(\n",
    "        prng_key=next(key_gen),\n",
    "        config=config,\n",
    "        model_sub_config_name_base='model',\n",
    "        add_model_module_to_architecture_default_module=False, # since the model is already in the default module specified by 'architecture',\n",
    "    )\n",
    "except Exception as e:\n",
    "    traceback.print_exc()\n",
    "    print(e)\n",
    "    print('\\n')\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T13:11:48.290828912Z",
     "start_time": "2024-11-27T13:11:48.207796154Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeRF(\n",
       "  coarse_model=NeRFComponent(\n",
       "    block_pos_enc=ClassicalPositionalEncoding(_embedding_matrix=f32[6]),\n",
       "    blocks=Sequential(\n",
       "      layers=(\n",
       "        NeRFBlock(\n",
       "          net=Sequential(\n",
       "            layers=(\n",
       "              SirenLayer(\n",
       "                weights=f32[256,39],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        ),\n",
       "        NeRFBlock(\n",
       "          net=Sequential(\n",
       "            layers=(\n",
       "              SirenLayer(\n",
       "                weights=f32[256,295],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ),\n",
       "    to_sigma=Linear(weights=f32[1,256], biases=f32[1], activation_kwargs={}),\n",
       "    to_rgb=Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={}),\n",
       "    conditional_pos_enc=ClassicalPositionalEncoding(_embedding_matrix=f32[6]),\n",
       "    condition=Sequential(\n",
       "      layers=(\n",
       "        MLPINR(\n",
       "          layers=(\n",
       "            SirenLayer(\n",
       "              weights=f32[256,295],\n",
       "              biases=f32[256],\n",
       "              activation_kwargs={'w0': 30.0}\n",
       "            ),\n",
       "            Linear(weights=f32[256,256], biases=f32[256], activation_kwargs={})\n",
       "          )\n",
       "        ),\n",
       "      )\n",
       "    )\n",
       "  ),\n",
       "  fine_model=NeRFComponent(\n",
       "    block_pos_enc=ClassicalPositionalEncoding(_embedding_matrix=f32[6]),\n",
       "    blocks=Sequential(\n",
       "      layers=(\n",
       "        NeRFBlock(\n",
       "          net=Sequential(\n",
       "            layers=(\n",
       "              SirenLayer(\n",
       "                weights=f32[256,39],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        ),\n",
       "        NeRFBlock(\n",
       "          net=Sequential(\n",
       "            layers=(\n",
       "              SirenLayer(\n",
       "                weights=f32[256,295],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              ),\n",
       "              SirenLayer(\n",
       "                weights=f32[256,256],\n",
       "                biases=f32[256],\n",
       "                activation_kwargs={'w0': 30.0}\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    ),\n",
       "    to_sigma=Linear(weights=f32[1,256], biases=f32[1], activation_kwargs={}),\n",
       "    to_rgb=Linear(weights=f32[3,256], biases=f32[3], activation_kwargs={}),\n",
       "    conditional_pos_enc=ClassicalPositionalEncoding(_embedding_matrix=f32[6]),\n",
       "    condition=Sequential(\n",
       "      layers=(\n",
       "        MLPINR(\n",
       "          layers=(\n",
       "            SirenLayer(\n",
       "              weights=f32[256,295],\n",
       "              biases=f32[256],\n",
       "              activation_kwargs={'w0': 30.0}\n",
       "            ),\n",
       "            Linear(weights=f32[256,256], biases=f32[256], activation_kwargs={})\n",
       "          )\n",
       "        ),\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we get the experiment from the config using common_jax_utils.run_utils.get_experiment_from_config_and_key\n",
    "experiment = cju.run_utils.get_experiment_from_config_and_key(\n",
    "    prng_key=next(key_gen),\n",
    "    config=config,\n",
    "    model_kwarg_in_trainer='inr',\n",
    "    model_sub_config_name_base='model',  # so it looks for \"model_config\" in config\n",
    "    trainer_default_module_key='trainer_module',  # so it knows to get the module specified by config.trainer_module\n",
    "    additional_trainer_default_modules=[optax],  # remember the don't forget to add optax to the default modules? This is that \n",
    "    add_model_module_to_architecture_default_module=False,\n",
    "    initialize=False  # don't run the experiment yet, we want to use wandb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PostponedInitialization(cls=train_with_dataloader_scan, kwargs={'loss_evaluator': PostponedInitialization(cls=NeRFLossEvaluator, kwargs={'num_coarse_samples': 128, 'num_fine_samples': 512, 'near': -1.0, 'far': 1.0, 'noise_std': 1.0, 'white_bkgd': True, 'lindisp': False, 'randomized': True, 'parallel_batch_size': 500, 'state_update_function': None}, missing_args=[]), 'dataloader': PostponedInitialization(cls=SyntheticScenesDataLoader, kwargs={'split': 'train', 'name': 'vase', 'batch_size': 500, 'poses_per_batch': 100, 'base_path': 'synthetic_scenes', 'size_limit': -1, 'key': Array([1786414058, 1458264990], dtype=uint32)}, missing_args=[]), 'optimizer': PostponedInitialization(cls=adamw, kwargs={'learning_rate': 0.0005, 'weight_decay': 0.0001, 'b1': 0.9, 'b2': 0.999, 'eps': 1e-08, 'eps_root': 0.0, 'mu_dtype': None, 'mask': None, 'nesterov': False}, missing_args=[]), 'steps_per_cycle': 1000, 'num_cycles': 50, 'use_wandb': True, 'after_cycle_callback': PostponedInitialization(cls=ComposedCallback, kwargs={'callbacks': [functools.partial(<function print_loss at 0x77e4cf5401f0>, after_every=1), <function report_loss at 0x77e4cf578040>, <function raise_error_on_nan at 0x77e4cf5781f0>], 'show_logs': False, 'use_wandb': True, 'display_func': <function pprint at 0x77e56fb0d120>}, missing_args=[]), 'optimizer_state': None, 'after_training_callback': None, 'state_initialization_function': <function initialize_state at 0x77e4cf578700>, 'state': None, 'inr': PostponedInitialization(cls=from_config, kwargs={'in_size': (3, 3), 'out_size': (1, 3), 'bottle_size': 256, 'block_length': 4, 'block_width': 256, 'num_blocks': 2, 'condition_length': 1, 'condition_width': 256, 'layer_type': <class 'model_components.inr_layers.SirenLayer'>, 'activation_kwargs': {'w0': 30.0}, 'initialization_scheme': None, 'initialization_scheme_kwargs': None, 'positional_encoding_layer': PostponedInitialization(cls=from_config, kwargs={'num_frequencies': 6, 'frequency_scaling': 2.0}, missing_args=[]), 'direction_encoding_layer': PostponedInitialization(cls=from_config, kwargs={'num_frequencies': 6, 'frequency_scaling': 2.0}, missing_args=[]), 'num_splits': 1, 'post_processor': None, 'shared_initialization': False, 'key': Array([4180435127, 2640552043], dtype=uint32)}, missing_args=[])}, missing_args=[])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/simon/Documents/INR_BEP/wandb/run-20250130_153911-s8s8dpxh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nld/inr_edu_24/runs/s8s8dpxh' target=\"_blank\">honest-galaxy-206</a></strong> to <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">https://wandb.ai/nld/inr_edu_24</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nld/inr_edu_24/runs/s8s8dpxh' target=\"_blank\">https://wandb.ai/nld/inr_edu_24/runs/s8s8dpxh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2472461/1995310846.py\", line 8, in <module>\n",
      "    results = experiment.initialize()\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/config_realization.py\", line 195, in initialize\n",
      "    return cls(**processed_self_kwargs)\n",
      "  File \"/home/simon/Documents/INR_BEP/inr_utils/training.py\", line 421, in train_with_dataloader_scan\n",
      "    carry, loss_array = train_cycle(carry, batches)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 337, in cache_miss\n",
      "    pgle_profiler) = _python_pjit_helper(fun, jit_info, *args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 177, in _python_pjit_helper\n",
      "    p, args_flat = _infer_params(fun, jit_info, args, kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 769, in _infer_params\n",
      "    p, args_flat = _infer_params_impl(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 651, in _infer_params_impl\n",
      "    jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 335, in memoized_fun\n",
      "    ans = call(fun, *args)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 1315, in _create_pjit_jaxpr\n",
      "    jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/profiler.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2189, in trace_to_jaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 187, in call_wrapped\n",
      "    return self.f_transformed(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api_util.py\", line 294, in _argnums_partial\n",
      "    return _fun(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api_util.py\", line 74, in flatten_fun\n",
      "    ans = f(*py_args, **py_kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api_util.py\", line 691, in result_paths\n",
      "    ans = _fun(*args, **kwargs)\n",
      "  File \"/home/simon/Documents/INR_BEP/inr_utils/training.py\", line 370, in train_cycle\n",
      "    carry, losses = jax.lax.scan(train_step, carry, batches)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py\", line 295, in scan\n",
      "    init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py\", line 279, in _create_jaxpr\n",
      "    jaxpr, consts, out_tree, attrs_tracked = _initial_style_jaxpr_attrs(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/control_flow/common.py\", line 71, in _initial_style_jaxpr_attrs\n",
      "    jaxpr, consts, out_tree, attrs_tracked = _initial_style_open_jaxpr(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/control_flow/common.py\", line 57, in _initial_style_open_jaxpr\n",
      "    jaxpr, _, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/profiler.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2189, in trace_to_jaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 187, in call_wrapped\n",
      "    return self.f_transformed(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api_util.py\", line 90, in flatten_fun_nokwargs\n",
      "    ans = f(*py_args)\n",
      "  File \"/home/simon/Documents/INR_BEP/inr_utils/training.py\", line 362, in train_step\n",
      "    updates, optimizer_state = optimizer.update(grad, optimizer_state, inr)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/optax/transforms/_combining.py\", line 73, in update_fn\n",
      "    updates, new_s = fn(updates, s, params, **extra_args)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/optax/_src/base.py\", line 330, in update\n",
      "    return tx.update(updates, state, params)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/optax/transforms/_adding.py\", line 50, in update_fn\n",
      "    updates = jtu.tree_map(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/tree_util.py\", line 358, in tree_map\n",
      "    all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest]\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/tree_util.py\", line 358, in <listcomp>\n",
      "    all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest]\n",
      "ValueError: Expected None, got 30.0.\n",
      "\n",
      "In previous releases of JAX, flatten-up-to used to consider None to be a tree-prefix of non-None values. To obtain the previous behavior, you can usually write:\n",
      "  jax.tree.map(lambda x, y: None if x is None else f(x, y), a, b, is_leaf=lambda x: x is None)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83d661506fc544cca8a7ad7bd7ff3b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.011 MB of 0.011 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">honest-galaxy-206</strong> at: <a href='https://wandb.ai/nld/inr_edu_24/runs/s8s8dpxh' target=\"_blank\">https://wandb.ai/nld/inr_edu_24/runs/s8s8dpxh</a><br/> View project at: <a href='https://wandb.ai/nld/inr_edu_24' target=\"_blank\">https://wandb.ai/nld/inr_edu_24</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250130_153911-s8s8dpxh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_2472461/1995310846.py\", line 8, in <module>\n",
      "    results = experiment.initialize()\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/common_dl_utils/config_realization.py\", line 195, in initialize\n",
      "    return cls(**processed_self_kwargs)\n",
      "  File \"/home/simon/Documents/INR_BEP/inr_utils/training.py\", line 421, in train_with_dataloader_scan\n",
      "    carry, loss_array = train_cycle(carry, batches)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 337, in cache_miss\n",
      "    pgle_profiler) = _python_pjit_helper(fun, jit_info, *args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 177, in _python_pjit_helper\n",
      "    p, args_flat = _infer_params(fun, jit_info, args, kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 769, in _infer_params\n",
      "    p, args_flat = _infer_params_impl(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 651, in _infer_params_impl\n",
      "    jaxpr, consts, out_avals, attrs_tracked = _create_pjit_jaxpr(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 335, in memoized_fun\n",
      "    ans = call(fun, *args)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/pjit.py\", line 1315, in _create_pjit_jaxpr\n",
      "    jaxpr, global_out_avals, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/profiler.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2189, in trace_to_jaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 187, in call_wrapped\n",
      "    return self.f_transformed(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api_util.py\", line 294, in _argnums_partial\n",
      "    return _fun(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api_util.py\", line 74, in flatten_fun\n",
      "    ans = f(*py_args, **py_kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api_util.py\", line 691, in result_paths\n",
      "    ans = _fun(*args, **kwargs)\n",
      "  File \"/home/simon/Documents/INR_BEP/inr_utils/training.py\", line 370, in train_cycle\n",
      "    carry, losses = jax.lax.scan(train_step, carry, batches)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/traceback_util.py\", line 180, in reraise_with_filtered_traceback\n",
      "    return fun(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py\", line 295, in scan\n",
      "    init_flat, carry_avals, carry_avals_out, init_tree, *rest = _create_jaxpr(init)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/control_flow/loops.py\", line 279, in _create_jaxpr\n",
      "    jaxpr, consts, out_tree, attrs_tracked = _initial_style_jaxpr_attrs(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/control_flow/common.py\", line 71, in _initial_style_jaxpr_attrs\n",
      "    jaxpr, consts, out_tree, attrs_tracked = _initial_style_open_jaxpr(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/lax/control_flow/common.py\", line 57, in _initial_style_open_jaxpr\n",
      "    jaxpr, _, consts, attrs_tracked = pe.trace_to_jaxpr_dynamic(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/profiler.py\", line 333, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/interpreters/partial_eval.py\", line 2189, in trace_to_jaxpr_dynamic\n",
      "    ans = fun.call_wrapped(*in_tracers)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/linear_util.py\", line 187, in call_wrapped\n",
      "    return self.f_transformed(*args, **kwargs)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/api_util.py\", line 90, in flatten_fun_nokwargs\n",
      "    ans = f(*py_args)\n",
      "  File \"/home/simon/Documents/INR_BEP/inr_utils/training.py\", line 362, in train_step\n",
      "    updates, optimizer_state = optimizer.update(grad, optimizer_state, inr)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/optax/transforms/_combining.py\", line 73, in update_fn\n",
      "    updates, new_s = fn(updates, s, params, **extra_args)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/optax/_src/base.py\", line 330, in update\n",
      "    return tx.update(updates, state, params)\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/optax/transforms/_adding.py\", line 50, in update_fn\n",
      "    updates = jtu.tree_map(\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/tree_util.py\", line 358, in tree_map\n",
      "    all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest]\n",
      "  File \"/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/tree_util.py\", line 358, in <listcomp>\n",
      "    all_leaves = [leaves] + [treedef.flatten_up_to(r) for r in rest]\n",
      "ValueError: Expected None, got 30.0.\n",
      "\n",
      "In previous releases of JAX, flatten-up-to used to consider None to be a tree-prefix of non-None values. To obtain the previous behavior, you can usually write:\n",
      "  jax.tree.map(lambda x, y: None if x is None else f(x, y), a, b, is_leaf=lambda x: x is None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected None, got 30.0.\n",
      "\n",
      "In previous releases of JAX, flatten-up-to used to consider None to be a tree-prefix of non-None values. To obtain the previous behavior, you can usually write:\n",
      "  jax.tree.map(lambda x, y: None if x is None else f(x, y), a, b, is_leaf=lambda x: x is None)\n",
      "\n",
      "> \u001b[0;32m/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/jax/_src/tree_util.py\u001b[0m(358)\u001b[0;36m<listcomp>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    356 \u001b[0;31m  \u001b[0;34m\"\"\"Alias of :func:`jax.tree.map`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    357 \u001b[0;31m  \u001b[0mleaves\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtreedef\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_flatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 358 \u001b[0;31m  \u001b[0mall_leaves\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mleaves\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_up_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    359 \u001b[0;31m  \u001b[0;32mreturn\u001b[0m \u001b[0mtreedef\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mxs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mall_leaves\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    360 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "PyTreeDef(CustomNode(NeRF[('coarse_model', 'fine_model'), (), ()], [CustomNode(NeRFComponent[('block_pos_enc', 'blocks', 'to_sigma', 'to_rgb', 'conditional_pos_enc', 'condition'), (), ()], [CustomNode(ClassicalPositionalEncoding[('_embedding_matrix',), (), ()], [*]), CustomNode(Sequential[('layers',), (), ()], [(CustomNode(NeRFBlock[('net',), (), ()], [CustomNode(Sequential[('layers',), (), ()], [(CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]))])]), CustomNode(NeRFBlock[('net',), (), ()], [CustomNode(Sequential[('layers',), (), ()], [(CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]))])]))]), CustomNode(Linear[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {}]), CustomNode(Linear[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {}]), CustomNode(ClassicalPositionalEncoding[('_embedding_matrix',), (), ()], [*]), CustomNode(Sequential[('layers',), (), ()], [(CustomNode(MLPINR[('layers',), (), ()], [(CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(Linear[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {}]))]),)])]), CustomNode(NeRFComponent[('block_pos_enc', 'blocks', 'to_sigma', 'to_rgb', 'conditional_pos_enc', 'condition'), (), ()], [CustomNode(ClassicalPositionalEncoding[('_embedding_matrix',), (), ()], [*]), CustomNode(Sequential[('layers',), (), ()], [(CustomNode(NeRFBlock[('net',), (), ()], [CustomNode(Sequential[('layers',), (), ()], [(CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]))])]), CustomNode(NeRFBlock[('net',), (), ()], [CustomNode(Sequential[('layers',), (), ()], [(CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]))])]))]), CustomNode(Linear[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {}]), CustomNode(Linear[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {}]), CustomNode(ClassicalPositionalEncoding[('_embedding_matrix',), (), ()], [*]), CustomNode(Sequential[('layers',), (), ()], [(CustomNode(MLPINR[('layers',), (), ()], [(CustomNode(SirenLayer[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {'w0': None}]), CustomNode(Linear[('weights', 'biases', 'activation_kwargs'), (), ()], [*, *, {}]))]),)])])]))\n",
      "> \u001b[0;32m/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/optax/transforms/_adding.py\u001b[0m(50)\u001b[0;36mupdate_fn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     48 \u001b[0;31m    \u001b[0;32mif\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     49 \u001b[0;31m      \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNO_PARAMS_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 50 \u001b[0;31m    updates = jtu.tree_map(\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m        lambda g, p: g + weight_decay * p, updates, params)\n",
      "\u001b[0m\u001b[0;32m     52 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31m    [... skipped 1 hidden frame(s)]\u001b[0m\n",
      "\n",
      "> \u001b[0;32m/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/optax/_src/base.py\u001b[0m(330)\u001b[0;36mupdate\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    328 \u001b[0;31m  \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    329 \u001b[0;31m    \u001b[0;32mdel\u001b[0m \u001b[0mextra_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 330 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    331 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    332 \u001b[0;31m  \u001b[0;32mreturn\u001b[0m \u001b[0mGradientTransformationExtraArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/simon/miniconda3/envs/inr_edu_24/lib/python3.10/site-packages/optax/transforms/_combining.py\u001b[0m(73)\u001b[0;36mupdate_fn\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     71 \u001b[0;31m    \u001b[0mnew_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     72 \u001b[0;31m    \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_fns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 73 \u001b[0;31m      \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     74 \u001b[0;31m      \u001b[0mnew_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     75 \u001b[0;31m    \u001b[0;32mreturn\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "> \u001b[0;32m/home/simon/Documents/INR_BEP/inr_utils/training.py\u001b[0m(362)\u001b[0;36mtrain_step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    360 \u001b[0;31m            \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    361 \u001b[0;31m        )\n",
      "\u001b[0m\u001b[0;32m--> 362 \u001b[0;31m        \u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    363 \u001b[0;31m        \u001b[0minr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_updates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    364 \u001b[0;31m        \u001b[0minr_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meqx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# and we run the experiment while logging things to wandb\n",
    "try:\n",
    "    with wandb.init(\n",
    "        project='inr_edu_24',\n",
    "        notes='test',\n",
    "        tags=['test']\n",
    "    ) as run:\n",
    "        results = experiment.initialize()\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print()\n",
    "    traceback.print_exc()\n",
    "    pdb.post_mortem()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m inr, losses, optimizer_state, state, loss_evaluator, additional_output \u001b[38;5;241m=\u001b[39m \u001b[43mresults\u001b[49m\n\u001b[1;32m      2\u001b[0m inr\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "inr, losses, optimizer_state, state, loss_evaluator, additional_output = results\n",
    "inr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from state_test_objects import after_training_callback, CountingIdentity\n",
    "after_training_callback(losses, inr, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inr.terms[0].layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(inr.terms[0].layers[0], CountingIdentity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(inr.terms[0].layers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inr_edu_24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
